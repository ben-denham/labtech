{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labtech Tutorial\n",
    "\n",
    "The following tutorial presents a full example of using labtech to\n",
    "easily add parallelism and caching to machine learning experiments.\n",
    "\n",
    "You can also run this tutorial as an ([interactive\n",
    "notebook](https://mybinder.org/v2/gh/ben-denham/labtech/main?filepath=examples/tutorial.ipynb)).\n",
    "\n",
    "Firstly, we will install and import `labtech` along with some other\n",
    "dependencies we will use in this tutorial:"
   ],
   "id": "e7568e78-03a2-4bef-8ef4-83a7b889bc7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install labtech mlflow scikit-learn"
   ],
   "id": "0fb14a7e-f60f-4f58-88dd-cddf9e3c247d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a single experiment as a labtech task\n",
    "\n",
    "Throughout this tutorial, we will use labtech to improve and extend the\n",
    "following machine learning experiment, where we:\n",
    "\n",
    "1.  Load and scale the `digits` dataset - a benchmark dataset where the\n",
    "    goal is to train a classifier that can correctly assign a number\n",
    "    between 0 and 9 to the image of a hand-written digit. We will want\n",
    "    to extend our experimentation to include other datasets.\n",
    "2.  Train a random forest classifier on the digits dataset. The\n",
    "    classifier is configured `n_estimators=5` (i.e. a forest of 5\n",
    "    trees). We will want to extend our experimentation to test other\n",
    "    `n_estimators` values and classifiers other than a random forest. We\n",
    "    also set a fixed `random_state` to ensure we get the same result\n",
    "    every time we run the code.\n",
    "3.  Evaluate the classifier by calculating the logistic loss of\n",
    "    probabilities predicted by the classifier for the dataset. Standard\n",
    "    evaluation practice would be to calculate loss for a separate test\n",
    "    dataset, but we will use a single dataset for both training and\n",
    "    testing to simplify this tutorial."
   ],
   "id": "9630bd95-80f5-4cca-8317-d5f2164acbb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "digits_X = StandardScaler().fit_transform(digits_X)\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=5,\n",
    "    random_state=42,\n",
    ")\n",
    "clf.fit(digits_X, digits_y)\n",
    "prob_y = clf.predict_proba(digits_X)\n",
    "\n",
    "print(f'{log_loss(digits_y, prob_y) = :.3}')"
   ],
   "id": "39938be1-40d5-4fe5-907c-335befb58ad1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set up this same experiment to be run with labtech, providing us\n",
    "with a foundation that we can extend throughout this tutorial.\n",
    "\n",
    "First, we’ll define a labtech *task type* that will load the dataset,\n",
    "train the classifier, and return the probabilities predicted for the\n",
    "dataset.\n",
    "\n",
    "Defining a task type for our experiment is as simple as defining a class\n",
    "decorated with `@labtech.task` that defines a `run()` method that\n",
    "performs the experiment and returns its result (the predicted\n",
    "probabilities):"
   ],
   "id": "0d86a736-3449-41e7-a5ec-c81c02471912"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labtech\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "\n",
    "    def run(self):\n",
    "        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "        digits_X = StandardScaler().fit_transform(digits_X)\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=5,\n",
    "            random_state=42,\n",
    "        )\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        prob_y = clf.predict_proba(digits_X)\n",
    "        return prob_y"
   ],
   "id": "4da1f81c-64cb-496c-abbb-bd2e8740376d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a labtech *lab* that can be used to execute the\n",
    "experiment. We’ll configure the lab to cache results in a folder called\n",
    "`'storage/classification_lab_1` and to use notebook-friendly progress\n",
    "bars:"
   ],
   "id": "096f72c7-e3c2-4d79-a016-a14f0fb957d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labtech.Lab(\n",
    "    storage='storage/classification_lab_1',\n",
    "    notebook=True,\n",
    ")"
   ],
   "id": "8b5386dc-6123-4d84-8161-6622d3162f9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a task instance of `ClassifierExperiment` and call\n",
    "`lab.run_task()` to run it. The output will be the predicted\n",
    "probabilities returned by the task’s `run()` method, so we can calculate\n",
    "the loss from them as before:"
   ],
   "id": "f0e6f4f1-5101-4515-bf72-b16039f62e12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_experiment = ClassifierExperiment()\n",
    "prob_y = lab.run_task(classifier_experiment)\n",
    "print(f'{log_loss(digits_y, prob_y) = :.3}')"
   ],
   "id": "de229ea9-5da7-4aa2-b933-2312080a6b73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An immediate benefit of running an experiment this way with labtech is\n",
    "that **the result will be cached to disk for future use**. Any future\n",
    "calls to run the same experiment (even after restarting Python) will\n",
    "load the result from the cache:"
   ],
   "id": "b12c1f86-1a5b-446c-821e-da59ce3598e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_y = lab.run_task(classifier_experiment)\n",
    "print(f'{log_loss(digits_y, prob_y) = :.3}')"
   ],
   "id": "dff1b3f7-41be-4734-965b-a7a3b45b056b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the task to just return the prediction probabilities instead of\n",
    "also performing the loss calculation would give us flexibility to change\n",
    "the evaluation (e.g. from `log_loss` to another metric) while still\n",
    "being able to re-use the same cached result.\n",
    "\n",
    "We can also ask our lab to construct `task` objects for all previously\n",
    "cached results for a given task type by calling `lab.cached_tasks()`. A\n",
    "given task could then be passed to `lab.run_task()` to load it’s result\n",
    "(or we could pass a list of tasks to `lab.run_tasks()`, as we will see\n",
    "in the next section of this tutorial)."
   ],
   "id": "1bd8810f-ea6a-4296-ab69-324ed42d2711"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.cached_tasks([\n",
    "    ClassifierExperiment,\n",
    "])"
   ],
   "id": "2abd3866-ec5a-45d2-b909-045a8e4c2d8f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important that you clear any cached results whenever you make\n",
    "a change that will impact the behaviour of a task - otherwise your\n",
    "cached results may no longer reflect the actual result of the current\n",
    "code.\n",
    "\n",
    "You can clear the cached results for a list of tasks with\n",
    "`lab.uncache_tasks()`:"
   ],
   "id": "722b0743-aaf1-4cf5-870c-ade5af326d14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.uncache_tasks([\n",
    "    classifier_experiment,\n",
    "])"
   ],
   "id": "fe82f1c6-2c28-400f-aac8-419e37328e3f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterising tasks, and running many tasks in parallel\n",
    "\n",
    "Let’s extend our experimentation to compare the results for classifiers\n",
    "configured with different `n_estimators` values.\n",
    "\n",
    "To do so, we’ll add a `n_estimators` parameter to our\n",
    "`ClassifierExperiment` task type and reference it within the `run()`\n",
    "method with `self.n_estimators`. Task parameters are declared in the\n",
    "exactly same way as\n",
    "[dataclass](https://docs.python.org/3/library/dataclasses.html) fields:"
   ],
   "id": "af8cb5e8-3500-49e2-b2c7-3ec64f26279c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labtech\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    n_estimators: int\n",
    "\n",
    "    def run(self):\n",
    "        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "        digits_X = StandardScaler().fit_transform(digits_X)\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            random_state=42,\n",
    "        )\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        prob_y = clf.predict_proba(digits_X)\n",
    "        return prob_y"
   ],
   "id": "66b91d21-1e80-43ae-9d36-752a92c1e5c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll use a list comprehension to construct a list of\n",
    "`ClassifierExperiment` tasks with different `n_estimators` values:"
   ],
   "id": "4911abb3-aca0-4336-a1bb-8f46fa5ceff4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_experiments = [\n",
    "    ClassifierExperiment(\n",
    "        n_estimators=n_estimators,\n",
    "    )\n",
    "    for n_estimators in range(1, 11)\n",
    "]"
   ],
   "id": "110ade65-a248-4ba2-b897-844e32ab8df3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a list of tasks with `lab.run_tasks()`, which has the added\n",
    "benefit of leverage Python’s multiprocessing capabilities to **run the\n",
    "tasks in parallel** - running as many tasks simultaneously as possible\n",
    "with the CPU of the machine running the tasks. Also, because we’ve\n",
    "changed the definition of our `ClassifierExperiment` class, we’ll keep\n",
    "caches for the new definition separate by constructing a new lab that\n",
    "uses a different storage directory:"
   ],
   "id": "3f37fe4a-190a-48a9-8599-cb27760b5d0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labtech.Lab(\n",
    "    storage='storage/classification_lab_2',\n",
    "    notebook=True,\n",
    ")\n",
    "results = lab.run_tasks(classifier_experiments)"
   ],
   "id": "ad9154fb-76ea-4c15-94b0-bdf888181ab3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lab.run_tasks()` returns a dictionary mapping each input task to the\n",
    "result it returned, which we can loop over to print loss metrics for\n",
    "each experiment:"
   ],
   "id": "15eb935f-1f5a-4683-9709-561fc0cc843b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment, prob_y in results.items():\n",
    "    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')"
   ],
   "id": "738f13da-31d9-4f7b-9412-d61ae7d71e16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximising concurrency and caching with dependent tasks\n",
    "\n",
    "Labtech’s true power lies in its ability to manage complex networks of\n",
    "dependent tasks - automatically running as many tasks as possible in\n",
    "parallel (even different types of tasks) and re-using cached results\n",
    "wherever possible.\n",
    "\n",
    "To demonstrate this, let’s extend our experimentation with a new\n",
    "post-processing step that will take the probabilities returned by one of\n",
    "our previous `ClassifierExperiment` tasks and assign a probability of\n",
    "`1` to the most likely class for each record (and conversely assigning a\n",
    "probability of `0` to all other classes).\n",
    "\n",
    "To achieve this, we will define a new `MinMaxProbabilityExperiment` task\n",
    "type that accepts a `ClassifierExperiment` as a parameter. Labtech will\n",
    "consider any task in a parameter to be a *dependency* of the task.\n",
    "Dependency tasks will be executed before any of their dependent tasks,\n",
    "allowing us to access the result from the `.result` attribute of the\n",
    "task parameter (i.e. `self.classifier_experiment.result`):"
   ],
   "id": "277e71ce-ed37-4f6e-9fe7-1c250f7153ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class MinMaxProbabilityExperiment:\n",
    "    classifier_experiment: ClassifierExperiment\n",
    "\n",
    "    def run(self):\n",
    "        prob_y = self.classifier_experiment.result\n",
    "        # Replace the maximum probability in each row with 1,\n",
    "        # and replace all other probabilities with 0.\n",
    "        min_max_prob_y = np.zeros(prob_y.shape)\n",
    "        min_max_prob_y[np.arange(len(prob_y)), prob_y.argmax(axis=1)] = 1\n",
    "        return min_max_prob_y"
   ],
   "id": "886c5d6f-9a05-4b91-b3ec-4b21011f1568"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct and run a list of `MinMaxProbabilityExperiment`\n",
    "tasks that depend on our previous `classifier_experiments`. Labtech will\n",
    "ensure each of the `classifier_experiments` has been run before it’s\n",
    "dependent `MinMaxProbabilityExperiment` is run, re-using results\n",
    "depended on by multiple tasks and loading previously cached results\n",
    "wherever possible:"
   ],
   "id": "5677365c-c013-4ac3-bb30-dc48b65a487e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_prob_experiments = [\n",
    "    MinMaxProbabilityExperiment(\n",
    "        classifier_experiment=classifier_experiment,\n",
    "    )\n",
    "    for classifier_experiment in classifier_experiments\n",
    "]\n",
    "\n",
    "results = lab.run_tasks(min_max_prob_experiments)\n",
    "for experiment, prob_y in results.items():\n",
    "    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')"
   ],
   "id": "78d4435e-4c4c-4d7a-9bf6-d2623451a8a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply specifying task dependencies, you can construct any task\n",
    "structure that can be expressed as a [directed acyclic graph (or\n",
    "DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) and let\n",
    "labtech handle running tasks concurrently, sharing results between\n",
    "dependent tasks, and using caches wherever possible.\n",
    "\n",
    "### Parameterising tasks with complex objects\n",
    "\n",
    "Now let’s extend our experimentation to compare different classifier\n",
    "models. We’d like to make the classifier itself a parameter to the task,\n",
    "but. However, task parameters can only be [json-serializable\n",
    "values](https://ben-denham.github.io/labtech/core/#labtech.task) or\n",
    "dependency tasks, so we will use dependency tasks to construct and\n",
    "return classifier objects to our experiment tasks. We achieve this in\n",
    "the following code by:\n",
    "\n",
    "1.  We define `RFClassifierTask` and `LRClassifierTask` task types.\n",
    "    -   `RFClassifierTask` returns a random forest classifier\n",
    "        parameterised by an `n_estimators` value.\n",
    "    -   `LRClassifierTask` returns a logistic regression classifier.\n",
    "    -   Because constructing a classifier object is inexpensive, we\n",
    "        don’t need to cache them, so we set `cache=None` in the\n",
    "        `@labtech.task` decorator for these task types.\n",
    "    -   We will identify these task types in type hints with the\n",
    "        `ClassifierTask`\n",
    "        [Protocol](https://docs.python.org/3/library/typing.html#typing.Protocol),\n",
    "        which will match any task type that returns a sklearn\n",
    "        classifier.\n",
    "2.  We redefine `ClassifierExperiment` to be parameterised by a\n",
    "    `ClassifierTask`, which provides the classifier through\n",
    "    `self.classifier_task.result` to be trained and applied. Because one\n",
    "    `ClassifierTask` result may be shared by many `ClassifierExperiment`\n",
    "    tasks, the `run()` method first creates its own copy of the\n",
    "    classifier with `clone()`.\n",
    "\n",
    "Constructing a classifier object is inexpensive, so we don’t need to\n",
    "cache the result"
   ],
   "id": "41cdc625-76fa-4b24-a72b-8c00adcfb10e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "from sklearn.base import clone, ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class ClassifierTask(Protocol):\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        pass\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class RFClassifierTask:\n",
    "    n_estimators: int\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class LRClassifierTask:\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return LogisticRegression(\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    classifier_task: ClassifierTask\n",
    "\n",
    "    def run(self):\n",
    "        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "        digits_X = StandardScaler().fit_transform(digits_X)\n",
    "\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        prob_y = clf.predict_proba(digits_X)\n",
    "        return prob_y"
   ],
   "id": "0914096c-bc16-4556-9d60-17b63ced2988"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate and run a set of `RFClassifierTask` tasks for\n",
    "various `n_estimators` values, and construct a `ClassifierExperiment`\n",
    "for each of these `RFClassifierTask` tasks as well as a\n",
    "`LRClassifierTask` task:"
   ],
   "id": "23237c78-7508-47c7-974f-5f576c762c73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_tasks = [\n",
    "    RFClassifierTask(\n",
    "        n_estimators=n_estimators,\n",
    "    )\n",
    "    for n_estimators in range(1, 11)\n",
    "]\n",
    "classifier_experiments = [\n",
    "    ClassifierExperiment(\n",
    "        classifier_task=classifier_task,\n",
    "    )\n",
    "    for classifier_task in [\n",
    "        LRClassifierTask(),\n",
    "        *rf_classifier_tasks,\n",
    "    ]\n",
    "]\n",
    "\n",
    "lab = labtech.Lab(\n",
    "    storage='storage/classification_lab_3',\n",
    "    notebook=True,\n",
    ")\n",
    "\n",
    "results = lab.run_tasks(classifier_experiments)\n",
    "for experiment, prob_y in results.items():\n",
    "    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')"
   ],
   "id": "e498a194-392f-4a70-9310-622e5abb42e0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing large objects as context\n",
    "\n",
    "Sometimes we want to pass large, unchanging objects to our tasks, but\n",
    "don’t want to be restricted to loading them in a dependent task. For\n",
    "example, it would be convenient to load a collection of datasets outside\n",
    "of any tasks, allowing us to inspect these datasets before and after the\n",
    "tasks have been run:"
   ],
   "id": "2c0f1016-756b-47f5-a974-d8cb9dcc96ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "iris_X = StandardScaler().fit_transform(iris_X)\n",
    "\n",
    "DATASETS = {\n",
    "    'digits': {'X': digits_X, 'y': digits_y},\n",
    "    'iris': {'X': iris_X, 'y': iris_y},\n",
    "}"
   ],
   "id": "64f156f6-e5ef-414b-99c9-4f9798feaec7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this, a labtech lab can be provided with a *context* that is\n",
    "made available to all tasks. In the following code, we:\n",
    "\n",
    "1.  Redefine `ClassifierExperiment` to accept a `dataset_key` parameter\n",
    "    and use it to look up a dataset inside the `'DATASETS'` key of\n",
    "    `self.context`.\n",
    "2.  We pass a `context` to the `labtech.Lab()` constructor, which sets\n",
    "    the `'DATASETS'` key to the set of `DATASETS` defined above.\n",
    "3.  We alter the task generation and evaluation code to handle multiple\n",
    "    datasets."
   ],
   "id": "c00a14a0-c07c-419f-be57-a5deee30ccc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    classifier_task: ClassifierTask\n",
    "    dataset_key: str\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.context['DATASETS'][self.dataset_key]\n",
    "        X, y = dataset['X'], dataset['y']\n",
    "\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        clf.fit(X, y)\n",
    "        prob_y = clf.predict_proba(X)\n",
    "        return prob_y\n",
    "\n",
    "\n",
    "classifier_experiments = [\n",
    "    ClassifierExperiment(\n",
    "        classifier_task=classifier_task,\n",
    "        dataset_key=dataset_key,\n",
    "    )\n",
    "    # By including multiple for clauses, we will produce a ClassifierExperiment\n",
    "    # for every combination of dataset_key and classifier_task\n",
    "    for dataset_key in DATASETS.keys()\n",
    "    for classifier_task in [LRClassifierTask(), *rf_classifier_tasks]\n",
    "]\n",
    "\n",
    "lab = labtech.Lab(\n",
    "    storage='storage/classification_lab_4',\n",
    "    notebook=True,\n",
    "    context={\n",
    "        'DATASETS': DATASETS,\n",
    "    },\n",
    ")\n",
    "\n",
    "results = lab.run_tasks(classifier_experiments)\n",
    "for experiment, prob_y in results.items():\n",
    "    print(f'{experiment}: {log_loss(DATASETS[experiment.dataset_key][\"y\"], prob_y) = :.3}')"
   ],
   "id": "97bbfddf-e6aa-491c-86d0-9308245b8301"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab context can also be useful for passing parameters to tasks that\n",
    "won’t affect the result and therefore don’t need to be part of the tasks\n",
    "formal parameters. E.g. log levels and task-internal parallelism\n",
    "settings.\n",
    "\n",
    "> Warning: It is important that you do NOT make changes to context\n",
    "> values that impact task results after you have started caching\n",
    "> experiment results - otherwise your cached results may not reflect\n",
    "> your latest context values.\n",
    "\n",
    "### Bringing it all together and aggregating results\n",
    "\n",
    "The following code brings all the steps from this tutorial together in\n",
    "one place, with some additional improvements:\n",
    "\n",
    "-   The “experiment-like” `ClassifierExperiment` and\n",
    "    `MinMaxProbabilityExperiment` task types are now identified by a\n",
    "    common `ExperimentTask` base class.\n",
    "-   A final `ExperimentEvaluationTask` task that depends on all\n",
    "    `ExperimentTask` tasks is used to compute the loss metric for all\n",
    "    experiments. A final task like this is useful once we have a large\n",
    "    number of experiments as it allows us to cache the final evaluation\n",
    "    of all tasks, meaning we only need to load experiment results and\n",
    "    re-calculate metrics if any of the experiments have changed or new\n",
    "    experiments have been added.\n",
    "-   We enable labtech’s integration with [`mlflow`](https://mlflow.org/)\n",
    "    by the following additions (see [How can I use labtech with\n",
    "    mlfow?](https://ben-denham.github.io/labtech/cookbook/#how-can-i-use-labtech-with-mlflow)\n",
    "    for details):\n",
    "    1.  Adding `mlflow_run=True` to the `@labtech.task` decorator of\n",
    "        `ClassifierExperiment` and `MinMaxProbabilityExperiment`,\n",
    "        indicating that each task of these types should be recorded as a\n",
    "        “run” in mflow.\n",
    "    2.  Naming the over-arching labtech “experiment” with\n",
    "        `mlflow.set_experiment('example_labtech_experiment')` before the\n",
    "        tasks are run."
   ],
   "id": "7615d803-f152-40ac-9e31-6323f85bea3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "import labtech\n",
    "from sklearn.base import clone, ClassifierMixin\n",
    "\n",
    "\n",
    "# === Prepare Datasets ===\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "digits_X = StandardScaler().fit_transform(digits_X)\n",
    "\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "iris_X = StandardScaler().fit_transform(iris_X)\n",
    "\n",
    "DATASETS = {\n",
    "    'digits': {'X': digits_X, 'y': digits_y},\n",
    "    'iris': {'X': iris_X, 'y': iris_y},\n",
    "}\n",
    "\n",
    "\n",
    "# === Classifier Tasks ===\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class ClassifierTask(Protocol):\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Constructing a classifier object is inexpensive, so we don't need to\n",
    "# cache the result\n",
    "@labtech.task(cache=None)\n",
    "class RFClassifierTask:\n",
    "    n_estimators: int\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class LRClassifierTask:\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return LogisticRegression(\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "\n",
    "# === Experiment Tasks ===\n",
    "\n",
    "class ExperimentTask:\n",
    "    pass\n",
    "\n",
    "\n",
    "@labtech.task(mlflow_run=True)\n",
    "class ClassifierExperiment(ExperimentTask):\n",
    "    classifier_task: ClassifierTask\n",
    "    dataset_key: str\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        dataset = self.context['DATASETS'][self.dataset_key]\n",
    "        X, y = dataset['X'], dataset['y']\n",
    "\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        prob_y = clf.predict_proba(X)\n",
    "        return prob_y\n",
    "\n",
    "\n",
    "@labtech.task(mlflow_run=True)\n",
    "class MinMaxProbabilityExperiment(ExperimentTask):\n",
    "    experiment: ExperimentTask\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        prob_y = self.experiment.result\n",
    "        # Replace the maximum probability in each row with 1,\n",
    "        # and replace all other probabilities with 0.\n",
    "        min_max_prob_y = np.zeros(prob_y.shape)\n",
    "        min_max_prob_y[np.arange(len(prob_y)), prob_y.argmax(axis=1)] = 1\n",
    "        return min_max_prob_y\n",
    "\n",
    "\n",
    "# === Results Aggregation ===\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "@labtech.task\n",
    "class ExperimentEvaluationTask:\n",
    "    experiments: list[ExperimentTask]\n",
    "\n",
    "    def run(self):\n",
    "        return {\n",
    "            experiment: {'log_loss': log_loss(\n",
    "                self.context['DATASETS'][experiment.dataset_key]['y'],\n",
    "                experiment.result,\n",
    "            )}\n",
    "            for experiment in self.experiments\n",
    "        }\n",
    "\n",
    "\n",
    "# === Task Construction ===\n",
    "\n",
    "rf_classifier_tasks = [\n",
    "    RFClassifierTask(\n",
    "        n_estimators=n_estimators,\n",
    "    )\n",
    "    for n_estimators in range(1, 11)\n",
    "]\n",
    "\n",
    "classifier_experiments = [\n",
    "    ClassifierExperiment(\n",
    "        classifier_task=classifier_task,\n",
    "        dataset_key=dataset_key,\n",
    "    )\n",
    "    for dataset_key in DATASETS.keys()\n",
    "    for classifier_task in [LRClassifierTask(), *rf_classifier_tasks]\n",
    "]\n",
    "\n",
    "min_max_prob_experiments = [\n",
    "    MinMaxProbabilityExperiment(\n",
    "        experiment=classifier_experiment,\n",
    "    )\n",
    "    for classifier_experiment in classifier_experiments\n",
    "]\n",
    "\n",
    "evaluation_task = ExperimentEvaluationTask(\n",
    "    experiments=[\n",
    "        *classifier_experiments,\n",
    "        *min_max_prob_experiments,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# === Task Execution ===\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('example_labtech_experiment')\n",
    "lab = labtech.Lab(\n",
    "    storage='storage/classification_lab_final',\n",
    "    notebook=True,\n",
    "    context={\n",
    "        'DATASETS': DATASETS,\n",
    "    },\n",
    ")\n",
    "\n",
    "evaluation_result = lab.run_task(evaluation_task)\n",
    "print(evaluation_result)"
   ],
   "id": "0bfa3414-cb54-447e-9e56-77c658b4b6d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Congratulations on completing the labtech tutorial! You’re now ready to\n",
    "manage complex experiment workflows with ease!\n",
    "\n",
    "To learn more about labtech, you can dive into the following resources:\n",
    "\n",
    "-   [Cookbook of common\n",
    "    patterns](https://ben-denham.github.io/labtech/cookbook) ([as an\n",
    "    interactive\n",
    "    notebook](https://mybinder.org/v2/gh/ben-denham/labtech/main?filepath=examples/cookbook.ipynb))\n",
    "-   [API reference for Labs and\n",
    "    Tasks](https://ben-denham.github.io/labtech/core)\n",
    "-   [More options for cache formats and storage\n",
    "    providers](https://ben-denham.github.io/labtech/caching)\n",
    "-   [More\n",
    "    examples](https://github.com/ben-denham/labtech/tree/main/examples)"
   ],
   "id": "a4a292b4-0953-4bf1-9783-65114a25ca2d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
