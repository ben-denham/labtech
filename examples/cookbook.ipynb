{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookbook\n",
    "\n",
    "The following cookbook presents labtech patterns for common use cases.\n",
    "\n",
    "You can also run this cookbook as an ([interactive\n",
    "notebook](https://mybinder.org/v2/gh/ben-denham/labtech/main?filepath=examples/cookbook.ipynb))."
   ],
   "id": "c6855f2c-389e-4082-9026-e05fa509c37c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install labtech mlflow scikit-learn setuptools"
   ],
   "id": "d1a14b8e-b71e-4329-b246-74c37486551a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labtech\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.base import clone, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare a dataset for examples\n",
    "digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "digits_X = StandardScaler().fit_transform(digits_X)"
   ],
   "id": "f28cb3a0-df7f-4ec8-9428-58d53e8617dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I print log messages from my task?\n",
    "\n",
    "Using `labtech.logger` (a standard [Python logger\n",
    "object](https://docs.python.org/3/library/logging.html#logger-objects))\n",
    "is the recommended approach for logging from a task, but all output that\n",
    "is sent to `STDOUT` (e.g. calls to `print()`) or `STDERR` (e.g. uncaught\n",
    "exceptions) will also be captured and logged:"
   ],
   "id": "eae2b022-4f46-4e6f-be1a-04cbc71a59ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labtech.task\n",
    "class PrintingExperiment:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        labtech.logger.warning(f'Warning, the seed is: {self.seed}')\n",
    "        print(f'The seed is: {self.seed}')\n",
    "        return self.seed * self.seed\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    PrintingExperiment(\n",
    "        seed=seed\n",
    "    )\n",
    "    for seed in range(5)\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True,\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "cb358811-002d-43a7-bd35-3f6aa798e92c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I specify a complex object, like a model or dataset, as a task parameter?\n",
    "\n",
    "Because labtech needs to be able to reconstitute `Task` objects from\n",
    "caches, task parameters can only be:\n",
    "\n",
    "-   Simple scalar types: `str`, `bool`, `float`, `int`, `None`\n",
    "-   Any member of an `Enum` type.\n",
    "-   Task types: A task parameter is a “nested task” that will be\n",
    "    executed before its parent so that it may make use of the nested\n",
    "    result.\n",
    "-   Collections of any of these types: `list`, `tuple`, `dict`,\n",
    "    [`frozendict`](https://pypi.org/project/frozendict/)\n",
    "    -   Note: Mutable `list` and `dict` collections will be converted to\n",
    "        immutable `tuple` and\n",
    "        [`frozendict`](https://pypi.org/project/frozendict/)\n",
    "        collections.\n",
    "\n",
    "The are three primary patterns you can use to provide a more complex\n",
    "object as a parameter to a task:\n",
    "\n",
    "-   Constructing the object in a dependent task\n",
    "-   Passing the object in an `Enum` parameter\n",
    "-   Passing the object in the lab context\n",
    "\n",
    "#### Constructing objects in dependent tasks\n",
    "\n",
    "If your object can be constructed from its own set of parameters, then\n",
    "you can use a dependent task as a “factory” to construct your object.\n",
    "\n",
    "For example, you could define a task type to construct a machine\n",
    "learning model (like `LRClassifierTask` below), and then make a task of\n",
    "that type a parameter for your primary experiment task:"
   ],
   "id": "2cfe1b63-c3a8-405e-aadb-c257921002e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Constructing a classifier object is inexpensive, so we don't need to cache the result\n",
    "@labtech.task(cache=None)\n",
    "class LRClassifierTask:\n",
    "    random_state: int\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return LogisticRegression(\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    classifier_task: LRClassifierTask\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        # Because the classifier task result may be shared between experiments,\n",
    "        # we clone it before fitting.\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        return clf.predict_proba(digits_X)\n",
    "\n",
    "\n",
    "experiment = ClassifierExperiment(\n",
    "    classifier_task=LRClassifierTask(random_state=42),\n",
    ")\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True,\n",
    ")\n",
    "results = lab.run_tasks([experiment])"
   ],
   "id": "67017216-f06d-4005-8361-9887cb7010e4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this example with additional task types to cater for\n",
    "different types of classifiers with a Protocol that defines their common\n",
    "result type:"
   ],
   "id": "6d457100-432f-4930-913b-a8e49ae23f50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "class ClassifierTask(Protocol):\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        pass\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class LRClassifierTask:\n",
    "    random_state: int\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return LogisticRegression(\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class NBClassifierTask:\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return GaussianNB()\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    classifier_task: ClassifierTask\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        # Because the classifier task result may be shared between experiments,\n",
    "        # we clone it before fitting.\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        return clf.predict_proba(digits_X)\n",
    "\n",
    "\n",
    "classifier_tasks = [\n",
    "    LRClassifierTask(random_state=42),\n",
    "    NBClassifierTask(),\n",
    "]\n",
    "experiments = [\n",
    "    ClassifierExperiment(classifier_task=classifier_task)\n",
    "    for classifier_task in classifier_tasks\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True,\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "13499c2f-6c17-48f2-a43f-88c4a2f58ec1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing objects in `Enum` parameters\n",
    "\n",
    "For simple object parameters that have a fixed set of known values, an\n",
    "`Enum` of possible values can be used to provide parameter values.\n",
    "\n",
    "The following example shows how an `Enum` of functions can be used for a\n",
    "parameter to specify the operation that an experiment performs:\n",
    "\n",
    "> Note: Because parameter values will be\n",
    "> [pickled](https://docs.python.org/3/library/pickle.html) when they are\n",
    "> copied to parallel task sub-processes, the type used in a parameter\n",
    "> `Enum` must support equality between identical (but distinct) object\n",
    "> instances."
   ],
   "id": "7fca348f-5019-4899-84f7-03c0ca517742"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# The custom class we want to provide objects of as parameters.\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.data = datasets.fetch_openml(key, parser='auto').data\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        # Support equality check to allow pickling of Enum values\n",
    "        if type(self) != type(other):\n",
    "            return False\n",
    "        return self.key == other.key\n",
    "\n",
    "\n",
    "class DatasetOption(Enum):\n",
    "    TIC_TAC_TOE=Dataset('tic-tac-toe')\n",
    "    EEG_EYE_STATE=Dataset('eeg-eye-state')\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class DatasetExperiment:\n",
    "    dataset: DatasetOption\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.dataset.value\n",
    "        return dataset.data.shape\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    DatasetExperiment(\n",
    "        dataset=dataset\n",
    "    )\n",
    "    for dataset in DatasetOption\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True,\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "03bcce16-9695-4371-8b73-4ef651814d2b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing objects in the lab context\n",
    "\n",
    "If an object cannot be conveniently defined in an `Enum` (such as types\n",
    "like Numpy arrays or Pandas DataFrames that cannot be directly specified\n",
    "as an `Enum` value, or large values that cannot be loaded into memory\n",
    "all at once), then the lab context can be used to pass the object to a\n",
    "task.\n",
    "\n",
    "> Note: Because values provided in the lab context are not cached, they\n",
    "> should be kept constant between runs or should not affect task results\n",
    "> (e.g. parallel worker counts, log levels).\n",
    "\n",
    "The following example demonstrates specifying a `dataset_key` parameter\n",
    "to a task that is used to look up a dataset from the lab context:"
   ],
   "id": "039752d9-0f38-4110-b44d-b905f6598d80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    'zeros': np.zeros((50, 10)),\n",
    "    'ones': np.ones((50, 10)),\n",
    "}\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class SumExperiment:\n",
    "    dataset_key: str\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.context['DATASETS'][self.dataset_key]\n",
    "        return np.sum(dataset)\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    SumExperiment(\n",
    "        dataset_key=dataset_key\n",
    "    )\n",
    "    for dataset_key in DATASETS.keys()\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True,\n",
    "    context={\n",
    "        'DATASETS': DATASETS,\n",
    "    },\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "d02f78ac-a8ee-4118-ab0f-76a7deec2052"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I control multi-processing myself within a task?\n",
    "\n",
    "By default, Labtech executes tasks in parallel on all available CPU\n",
    "cores. However, if you wish to control multi-processing yourself\n",
    "\n",
    "The following example uses `max_parallel` to allow only one\n",
    "`CVExperiment` to be executed at a time, and then performs\n",
    "cross-validation within the task using a number of workers specified in\n",
    "the lab context as `within_task_workers`:"
   ],
   "id": "acce3446-5de3-4af3-b78d-4551e989b918"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "@labtech.task(max_parallel=1)\n",
    "class CVExperiment:\n",
    "    cv_folds: int\n",
    "\n",
    "    def run(self):\n",
    "        clf = GaussianNB()\n",
    "        return cross_val_score(\n",
    "            clf,\n",
    "            digits_X,\n",
    "            digits_y,\n",
    "            cv=self.cv_folds,\n",
    "            n_jobs=self.context['within_task_workers'],\n",
    "        )\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    CVExperiment(\n",
    "        cv_folds=cv_folds\n",
    "    )\n",
    "    for cv_folds in [5, 10]\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True,\n",
    "    context={\n",
    "        'within_task_workers': 4,\n",
    "    },\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "f2116516-73a1-4d5d-9f03-6ace08dcb3aa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Instead of limiting parallelism for a single task type by\n",
    "> specifying `max_parallel` in the `@labtech.task` decorator, you can\n",
    "> limit parallelism across all tasks with `max_workers` when\n",
    "> constructing a `labtech.Lab`.\n",
    "\n",
    "> Note: The `joblib` library used by `sklearn` does not behave correctly\n",
    "> when run from within a task sub-process, but setting `max_parallel=1`\n",
    "> or `max_workers=1` ensures tasks are run inside the main process.\n",
    "\n",
    "### How can I make labtech continue executing tasks even when one or more fail?\n",
    "\n",
    "Labtech’s default behaviour is to stop executing any new tasks as soon\n",
    "as any individual task fails. However, when executing tasks over a long\n",
    "period of time (e.g. a large number of tasks, or even a few long running\n",
    "tasks), it is sometimes helpful to have labtech continue to execute\n",
    "tasks even if one or more fail.\n",
    "\n",
    "If you set `continue_on_failure=True` when creating your lab, exceptions\n",
    "raised during the execution of a task will be logged, but the execution\n",
    "of other tasks will continue:"
   ],
   "id": "d4615596-0bca-4dfc-8182-9ac0b6a5f8d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lab = labtech.Lab(continue_on_failure=True)"
   ],
   "id": "b88826df-eaf8-44ca-a734-d205ce822b9b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens to my cached results if I change or move the definition of a task?\n",
    "\n",
    "TODO:\n",
    "\n",
    "-   It is easiest to keep behaviour, fields, and location fixed once you\n",
    "    start to seriously run experiments\n",
    "-   New field or behaviour -\\> create a child class\n",
    "-   Moved class -\\> use jq to manually update cache\n",
    "\n",
    "### How can I find what results I have cached?\n",
    "\n",
    "TODO\n",
    "\n",
    "### How can I clear cached results?\n",
    "\n",
    "TODO:\n",
    "\n",
    "-   `uncache_tasks`\n",
    "-   `bust_cache`\n",
    "\n",
    "### How can I cache task results in a format other than pickle?\n",
    "\n",
    "TODO\n",
    "\n",
    "### How can I cache task results somewhere other than my filesystem?\n",
    "\n",
    "TODO\n",
    "\n",
    "### Loading lots of cached results is slow, how can I make it faster?\n",
    "\n",
    "TODO\n",
    "\n",
    "### How can I construct a multi-step experiment pipeline?\n",
    "\n",
    "TODO\n",
    "\n",
    "### How can I access the results of intermediate/dependency tasks?\n",
    "\n",
    "TODO\n",
    "\n",
    "### How can I see when a task was run and how long it took to execute?\n",
    "\n",
    "TODO\n",
    "\n",
    "### How can I use labtech with mlflow?\n",
    "\n",
    "If you want to log a task type as an mlflow “run”, simply add\n",
    "`mlflow_run=True` to the call to `@labtech.task()`, which will:\n",
    "\n",
    "-   Wrap each run of the task with `mlflow.start_run()`\n",
    "-   Tag the run with `labtech_task_type` equal to the task class name\n",
    "-   Log all task parameters with `mlflow.log_param()`\n",
    "\n",
    "The following example demonstrates using labtech with mlflow. Note that\n",
    "you can still make any configuration changes (such as\n",
    "`mlflow.set_experiment()`) before the tasks are run, and you can make\n",
    "additional tracking calls (such as `mlflow.log_metric()` or\n",
    "`mlflow.log_model()`) in the body of your task’s `run()` method:"
   ],
   "id": "834e9670-37fa-4db0-b7c0-d8a918d6c50d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@labtech.task(mlflow_run=True)\n",
    "class MLRun:\n",
    "    penalty_norm: str | None\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        clf = LogisticRegression(penalty=self.penalty_norm)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "\n",
    "        labels = clf.predict(digits_X)\n",
    "\n",
    "        train_accuracy = accuracy_score(digits_y, labels)\n",
    "        mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=clf,\n",
    "            artifact_path='digits_model',\n",
    "            input_example=digits_X,\n",
    "            registered_model_name='digits-model',\n",
    "        )\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "runs = [\n",
    "    MLRun(\n",
    "        penalty_norm=penalty_norm,\n",
    "    )\n",
    "    for penalty_norm in [None, 'l2']\n",
    "]\n",
    "\n",
    "mlflow.set_experiment('example_labtech_experiment')\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    notebook=True\n",
    ")\n",
    "results = lab.run_tasks(runs)"
   ],
   "id": "efd03068-ef5a-4e7b-9907-a3718aa6d6cb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: While the [mlflow\n",
    "> documentation](https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html#step-4-log-the-model-and-its-metadata-to-mlflow)\n",
    "> recommends wrapping only your tracking code with `mlflow.start_run()`,\n",
    "> labtech wraps the entire call to the `run()` method of your task in\n",
    "> order to track execution times in mlflow."
   ],
   "id": "6f57d1d6-e865-40d1-8e54-eacf6732ae00"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
