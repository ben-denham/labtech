{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labtech Cookbook\n",
    "\n",
    "The following cookbook presents labtech patterns for common use cases.\n",
    "\n",
    "You can also run this cookbook as an [interactive\n",
    "notebook](https://mybinder.org/v2/gh/ben-denham/labtech/main?filepath=examples/cookbook.ipynb)."
   ],
   "id": "018a57d7-11f4-490b-b726-e6c183758c1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install labtech fsspec mlflow pandas scikit-learn setuptools"
   ],
   "id": "03016d94-70c8-4dde-890e-20d7fd2aea33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir storage"
   ],
   "id": "b63bc804-ff16-4f9a-aa57-adfa0477f713"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labtech\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.base import clone, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare a dataset for examples\n",
    "digits_X, digits_y = datasets.load_digits(return_X_y=True)\n",
    "digits_X = StandardScaler().fit_transform(digits_X)"
   ],
   "id": "63c832d8-2ba5-4cbd-8cd8-6cdc8dd5ae41"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I print log messages from my task?\n",
    "\n",
    "Using `labtech.logger` (a standard [Python logger\n",
    "object](https://docs.python.org/3/library/logging.html#logger-objects))\n",
    "is the recommended approach for logging from a task, but all output that\n",
    "is sent to `STDOUT` (e.g. calls to `print()`) or `STDERR` (e.g. uncaught\n",
    "exceptions) will also be captured and logged:"
   ],
   "id": "9690c1c2-ba16-4ea7-aa57-36796b01dec2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labtech.task\n",
    "class PrintingExperiment:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        labtech.logger.warning(f'Warning, the seed is: {self.seed}')\n",
    "        print(f'The seed is: {self.seed}')\n",
    "        return self.seed * self.seed\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    PrintingExperiment(\n",
    "        seed=seed\n",
    "    )\n",
    "    for seed in range(5)\n",
    "]\n",
    "lab = labtech.Lab(storage=None)\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "7503db7d-0335-4502-836a-2150e48b1927"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I specify a complex object, like a model or dataset, as a task parameter?\n",
    "\n",
    "Because labtech needs to be able to reconstitute `Task` objects from\n",
    "caches, task parameters can only be:\n",
    "\n",
    "-   Simple scalar types: `str`, `bool`, `float`, `int`, `None`\n",
    "-   Any member of an `Enum` type.\n",
    "-   Task types: A task parameter is a “nested task” that will be\n",
    "    executed before its parent so that it may make use of the nested\n",
    "    result.\n",
    "-   Collections of any of these types: `list`, `tuple`, `dict`,\n",
    "    [`frozendict`](https://pypi.org/project/frozendict/)\n",
    "    -   Note: Mutable `list` and `dict` collections will be converted to\n",
    "        immutable `tuple` and\n",
    "        [`frozendict`](https://pypi.org/project/frozendict/)\n",
    "        collections.\n",
    "\n",
    "The are three primary patterns you can use to provide a more complex\n",
    "object as a parameter to a task:\n",
    "\n",
    "-   Constructing the object in a dependent task\n",
    "-   Passing the object in an `Enum` parameter\n",
    "-   Passing the object in the lab context\n",
    "\n",
    "#### Constructing objects in dependent tasks\n",
    "\n",
    "If your object can be constructed from its own set of parameters, then\n",
    "you can use a dependent task as a “factory” to construct your object.\n",
    "\n",
    "For example, you could define a task type to construct a machine\n",
    "learning model (like `LRClassifierTask` below), and then make a task of\n",
    "that type a parameter for your primary experiment task:"
   ],
   "id": "2df30640-45bf-4d43-9528-33e2ca826db8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Constructing a classifier object is inexpensive, so we don't need to\n",
    "# cache the result\n",
    "@labtech.task(cache=None)\n",
    "class LRClassifierTask:\n",
    "    random_state: int\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return LogisticRegression(\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    classifier_task: LRClassifierTask\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        # Because the classifier task result may be shared between experiments,\n",
    "        # we clone it before fitting.\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        return clf.predict_proba(digits_X)\n",
    "\n",
    "\n",
    "experiment = ClassifierExperiment(\n",
    "    classifier_task=LRClassifierTask(random_state=42),\n",
    ")\n",
    "lab = labtech.Lab(storage=None)\n",
    "results = lab.run_tasks([experiment])"
   ],
   "id": "37132698-d3f7-49b4-b17b-77727d4ae493"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this example with additional task types to cater for\n",
    "different types of classifiers with a\n",
    "[Protocol](https://docs.python.org/3/library/typing.html#typing.Protocol)\n",
    "that defines their common result type:"
   ],
   "id": "18373985-4a11-4a4a-9102-f38894034e07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "class ClassifierTask(Protocol):\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        pass\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class LRClassifierTask:\n",
    "    random_state: int\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return LogisticRegression(\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task(cache=None)\n",
    "class NBClassifierTask:\n",
    "\n",
    "    def run(self) -> ClassifierMixin:\n",
    "        return GaussianNB()\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ClassifierExperiment:\n",
    "    classifier_task: ClassifierTask\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        # Because the classifier task result may be shared between experiments,\n",
    "        # we clone it before fitting.\n",
    "        clf = clone(self.classifier_task.result)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "        return clf.predict_proba(digits_X)\n",
    "\n",
    "\n",
    "classifier_tasks = [\n",
    "    LRClassifierTask(random_state=42),\n",
    "    NBClassifierTask(),\n",
    "]\n",
    "experiments = [\n",
    "    ClassifierExperiment(classifier_task=classifier_task)\n",
    "    for classifier_task in classifier_tasks\n",
    "]\n",
    "lab = labtech.Lab(storage=None)\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "5e68d591-4290-4688-9b09-30ed6f26341e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing objects in `Enum` parameters\n",
    "\n",
    "For simple object parameters that have a fixed set of known values, an\n",
    "`Enum` of possible values can be used to provide parameter values.\n",
    "\n",
    "The following example shows how an `Enum` of functions can be used for a\n",
    "parameter to specify the operation that an experiment performs:\n",
    "\n",
    "> Note: Because parameter values will be\n",
    "> [pickled](https://docs.python.org/3/library/pickle.html) when they are\n",
    "> copied to parallel task sub-processes, the type used in a parameter\n",
    "> `Enum` must support equality between identical (but distinct) object\n",
    "> instances."
   ],
   "id": "439c38de-bf0f-4e55-9002-f94645754724"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "# The custom class we want to provide objects of as parameters.\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.data = datasets.fetch_openml(key, parser='auto').data\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        # Support equality check to allow pickling of Enum values\n",
    "        if type(self) is not type(other):\n",
    "            return False\n",
    "        return self.key == other.key\n",
    "\n",
    "\n",
    "class DatasetOption(Enum):\n",
    "    TIC_TAC_TOE=Dataset('tic-tac-toe')\n",
    "    EEG_EYE_STATE=Dataset('eeg-eye-state')\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class DatasetExperiment:\n",
    "    dataset: DatasetOption\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.dataset.value\n",
    "        return dataset.data.shape\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    DatasetExperiment(\n",
    "        dataset=dataset\n",
    "    )\n",
    "    for dataset in DatasetOption\n",
    "]\n",
    "lab = labtech.Lab(storage=None)\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "99a44858-f1fd-4afe-8a7b-78249dc99ccd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing objects in the lab context\n",
    "\n",
    "If an object cannot be conveniently defined in an `Enum` (such as types\n",
    "like Numpy arrays or Pandas DataFrames that cannot be directly specified\n",
    "as an `Enum` value, or large values that cannot all be loaded into\n",
    "memory every time the `Enum` is loaded), then the lab context can be\n",
    "used to pass the object to a task.\n",
    "\n",
    "> Warning: Because values provided in the lab context are not cached,\n",
    "> they should be kept constant between runs or should not affect task\n",
    "> results (e.g. parallel worker counts, log levels). If changing context\n",
    "> values cause task results to change, then cached results may no longer\n",
    "> be valid.\n",
    "\n",
    "The following example demonstrates specifying a `dataset_key` parameter\n",
    "to a task that is used to look up a dataset from the lab context:"
   ],
   "id": "49b5a378-0716-486b-98c0-7f83100c4d74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    'zeros': np.zeros((50, 10)),\n",
    "    'ones': np.ones((50, 10)),\n",
    "}\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class SumExperiment:\n",
    "    dataset_key: str\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.context['DATASETS'][self.dataset_key]\n",
    "        return np.sum(dataset)\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    SumExperiment(\n",
    "        dataset_key=dataset_key\n",
    "    )\n",
    "    for dataset_key in DATASETS.keys()\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    context={\n",
    "        'DATASETS': DATASETS,\n",
    "    },\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "0f312541-c489-40d9-90fc-6125b3b47591"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I control multi-processing myself within a task?\n",
    "\n",
    "By default, Labtech executes tasks in parallel on all available CPU\n",
    "cores. However, you can control multi-processing yourself by disabling\n",
    "task parallelism and performing your own parallelism within a task’s\n",
    "`run()` method.\n",
    "\n",
    "The following example uses `max_parallel` to allow only one\n",
    "`CVExperiment` to be executed at a time, and then performs\n",
    "cross-validation within the task using a number of workers specified in\n",
    "the lab context as `within_task_workers`:"
   ],
   "id": "a1e3b0d3-183a-4dc6-a78a-47385fd17180"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "@labtech.task(max_parallel=1)\n",
    "class CVExperiment:\n",
    "    cv_folds: int\n",
    "\n",
    "    def run(self):\n",
    "        clf = GaussianNB()\n",
    "        return cross_val_score(\n",
    "            clf,\n",
    "            digits_X,\n",
    "            digits_y,\n",
    "            cv=self.cv_folds,\n",
    "            n_jobs=self.context['within_task_workers'],\n",
    "        )\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    CVExperiment(\n",
    "        cv_folds=cv_folds\n",
    "    )\n",
    "    for cv_folds in [5, 10]\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    context={\n",
    "        'within_task_workers': 4,\n",
    "    },\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "0f769852-2804-4c92-8160-1dc9f8ce11ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Instead of limiting parallelism for a single task type by\n",
    "> specifying `max_parallel` in the `@labtech.task` decorator, you can\n",
    "> limit parallelism across all tasks with `max_workers` when\n",
    "> constructing a `labtech.Lab`.\n",
    "\n",
    "> Note: The `joblib` library used by `sklearn` does not behave correctly\n",
    "> when run from within a task sub-process, but setting `max_parallel=1`\n",
    "> or `max_workers=1` ensures tasks are run inside the main process.\n",
    "\n",
    "### How can I make labtech continue executing tasks even when one or more fail?\n",
    "\n",
    "Labtech’s default behaviour is to stop executing any new tasks as soon\n",
    "as any individual task fails. However, when executing tasks over a long\n",
    "period of time (e.g. a large number of tasks, or even a few long running\n",
    "tasks), it is sometimes helpful to have labtech continue to execute\n",
    "tasks even if one or more fail.\n",
    "\n",
    "If you set `continue_on_failure=True` when creating your lab, exceptions\n",
    "raised during the execution of a task will be logged, but the execution\n",
    "of other tasks will continue:"
   ],
   "id": "094a2009-e173-4e0e-b3c9-7baf023641e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labtech.Lab(\n",
    "    storage=None,\n",
    "    continue_on_failure=True,\n",
    ")"
   ],
   "id": "a62c7ae5-91e4-46b4-a35d-56ab1db69d29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens to my cached results if I change or move the definition of a task?\n",
    "\n",
    "A task’s cache will store all details necessary to reinstantiate the\n",
    "task object, including the qualified name of the task’s class and all of\n",
    "the task’s parameters. Because of this, it is best not to change the\n",
    "parameters and location of a task’s definition once you are seriously\n",
    "relying on cached results.\n",
    "\n",
    "If you need to add a new parameter or behaviour to an existing task type\n",
    "for which you have previously cached results, consider defining a\n",
    "sub-class for that extension so that you can continue using caches for\n",
    "the base class:"
   ],
   "id": "f3f9b209-6fbd-4e5c-afcd-612ea0653d93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labtech.task\n",
    "class Experiment:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.seed * self.seed\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ExtendedExperiment(Experiment):\n",
    "    multiplier: int\n",
    "\n",
    "    def run(self):\n",
    "        base_result = super().run()\n",
    "        return base_result * self.multiplier"
   ],
   "id": "0c8dbd57-b80c-4346-bda7-537877e92c71"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I find what results I have cached?\n",
    "\n",
    "You can use the `cached_task()` method of a `Lab` instance to retrieve\n",
    "all cached task instances for a list of task types. You can then “run”\n",
    "the tasks to load their cached results:"
   ],
   "id": "94c8f311-8ded-46ac-a693-d696b9c9fb25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_cvexperiment_tasks = lab.cached_tasks([CVExperiment])\n",
    "results = lab.run_tasks(cached_cvexperiment_tasks)"
   ],
   "id": "90c62001-0620-4f58-95c9-b95f31cc38bd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I clear cached results?\n",
    "\n",
    "You can clear the cache for a list of tasks using the `uncache_tasks()`\n",
    "method of a `Lab` instance:"
   ],
   "id": "96458c81-c7dc-4bba-ab39-849571597207"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.uncache_tasks(cached_cvexperiment_tasks)"
   ],
   "id": "1eaa11f0-de72-42f7-a157-221256acfaf8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also ignore all previously cached results when running a list of\n",
    "tasks by passing the `bust_cache` option to `run_tasks()`:"
   ],
   "id": "f7ad7ac6-e3ef-4850-acb5-8a8ec24ec6e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.run_tasks(cached_cvexperiment_tasks, bust_cache=True)"
   ],
   "id": "d9bc2ae9-6d7b-4424-83a1-b70c1b744b6d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will Labtech ignore previously cached results if I change the implementation of a task?\n",
    "\n",
    "Whenever you make a change that will impact the behaviour of a task\n",
    "(i.e. most changes to the `run()` method or the code it depends on) you\n",
    "should add or updated the `code_version` in `@task`. For example:"
   ],
   "id": "15457df7-41dd-4a07-a7d9-bb794f323b67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labtech.task(code_version='v2')\n",
    "class Experiment:\n",
    "    ..."
   ],
   "id": "77d7e97f-d244-45ff-8f35-5fb6d8a34013"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labtech will re-run tasks if there are no cached results with a\n",
    "`code_version` matching your current code. If you don’t update the\n",
    "`code_version` or otherwise clear your cache, then the returned cached\n",
    "results may no longer reflect the actual results of your current code.\n",
    "\n",
    "You may also like to save storage space by clearing up old cached\n",
    "results where the `code_version` does not match the\n",
    "`current_code_version`:"
   ],
   "id": "558730bd-60d8-4016-b81c-278f3270ebcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stale_cached_tasks = [\n",
    "    cached_task for cached_task in lab.cached_tasks([\n",
    "       # Make sure to include all task types to ensure you clear\n",
    "       # all intermediate results\n",
    "       Experiment,\n",
    "    ])\n",
    "    if cached_task.code_version != cached_task.current_code_version\n",
    "]\n",
    "lab.uncache_tasks(stale_cached_tasks)"
   ],
   "id": "1bee9543-e6c8-4d12-aaba-f0af650a1134"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What types of values should my tasks return to be cached?\n",
    "\n",
    "While you can define your task to return any Python object you like to\n",
    "be cached, one generally useful approach is to return a dictionary\n",
    "comprised of built-in (e.g. lists, strings, numbers) or otherwise\n",
    "standard data types (e.g. arrays, dataframes). This is for two reasons:\n",
    "\n",
    "1.  If the returned dictionary needs to be extended to include\n",
    "    additional keys, it will often be straightforward to adapt code that\n",
    "    uses task results to safely continue using previously cached results\n",
    "    that do not contain those keys.\n",
    "2.  Using custom objects (such as dataclasses) could cause issues when\n",
    "    loading cached objects if the definition of the class ever changes.\n",
    "\n",
    "If you want to keep the typing benefits of a custom dataclass, you can\n",
    "consider using a\n",
    "[`TypeDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict):"
   ],
   "id": "af017217-bd24-47a4-b577-a338aecacaf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, NotRequired\n",
    "\n",
    "\n",
    "class MyTaskResult(TypedDict):\n",
    "    predictions: np.ndarray\n",
    "    # Key added in a later version of the task. Requires Python >= 3.11.\n",
    "    model_weights: NotRequired[np.ndarray]\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class ExampleTask:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        return MyTaskResult(\n",
    "            predictions=np.array([1, 2, 3]),\n",
    "            model_weights=np.array([self.seed, self.seed ** 2]),\n",
    "        )"
   ],
   "id": "72cb728b-20d3-4e47-b1fa-a00f7fd6f7c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I cache task results in a format other than pickle?\n",
    "\n",
    "You can define your own cache type to support storing cached results in\n",
    "a format other than pickle. To do so, you must define a class that\n",
    "extends `labtech.cache.BaseCache` and defines `KEY_PREFIX`,\n",
    "`save_result()`, and `load_result()`. You can then configure any task\n",
    "type by passing an instance of your new cache type for the `cache`\n",
    "option of the `@labtech.task` decorator.\n",
    "\n",
    "The following example demonstrates defining and using a custom cache\n",
    "type to store Pandas DataFrames as parquet files:"
   ],
   "id": "66b47a9a-3deb-495c-84d1-7e7dac8aab7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labtech.cache import BaseCache\n",
    "from labtech.types import Task, ResultT\n",
    "from labtech.storage import Storage\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ParquetCache(BaseCache):\n",
    "    \"\"\"Caches a Pandas DataFrame result as a parquet file.\"\"\"\n",
    "    KEY_PREFIX = 'parquet__'\n",
    "\n",
    "    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n",
    "        if not isinstance(result, pd.DataFrame):\n",
    "            raise ValueError('ParquetCache can only cache DataFrames')\n",
    "        with storage.file_handle(task.cache_key, 'result.parquet', mode='wb') as data_file:\n",
    "            result.to_parquet(data_file)\n",
    "\n",
    "    def load_result(self, storage: Storage, task: Task[ResultT]) -> ResultT:\n",
    "        with storage.file_handle(task.cache_key, 'result.parquet', mode='rb') as data_file:\n",
    "            return pd.read_parquet(data_file)\n",
    "\n",
    "\n",
    "@labtech.task(cache=ParquetCache())\n",
    "class TabularTask:\n",
    "\n",
    "    def run(self):\n",
    "        return pd.DataFrame({\n",
    "            'x': [1, 2, 3],\n",
    "            'y': [1, 4, 9],\n",
    "        })\n",
    "\n",
    "\n",
    "lab = labtech.Lab(storage='storage/parquet_example')\n",
    "lab.run_tasks([TabularTask()])"
   ],
   "id": "e5ec6c85-34ba-4df5-8936-191c96688b0a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I cache task results somewhere other than my filesystem?\n",
    "\n",
    "For any storage provider that has an\n",
    "[`fsspec`](https://filesystem-spec.readthedocs.io)-compatible\n",
    "implementation, you can define your own storage type that extends\n",
    "`labtech.storage.FsspecStorage`. You can then pass an instance of your\n",
    "new storage type for the `storage` option when constructing a `Lab`\n",
    "instance.\n",
    "\n",
    "The following example demonstrates constructing a storage type for an\n",
    "Amazon S3 bucket using the\n",
    "[`s3fs`](https://s3fs.readthedocs.io/en/latest/) library. This example\n",
    "could be adapted for other `fsspec` implementations, such as cloud\n",
    "storage providers like [Azure Blob\n",
    "Storage](https://github.com/fsspec/adlfs)."
   ],
   "id": "5c19dc03-4ded-4569-904a-54e58d796ef7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install s3fs"
   ],
   "id": "3e71090e-6555-4fa8-a2e9-edca1ce6e193"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labtech.storage import FsspecStorage\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "\n",
    "class S3fsStorage(FsspecStorage):\n",
    "\n",
    "    def fs_constructor(self):\n",
    "        return S3FileSystem(\n",
    "            endpoint_url='...',\n",
    "            key='...',\n",
    "            secret='...',\n",
    "        )\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class Experiment:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.seed * self.seed\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    Experiment(\n",
    "        seed=seed\n",
    "    )\n",
    "    for seed in range(100)\n",
    "]\n",
    "lab = labtech.Lab(\n",
    "    storage=S3fsStorage('my-s3-bucket/lab_directory'),\n",
    "    # s3fs does not support forked processes, so make sure we are spawning\n",
    "    # subprocesses: https://s3fs.readthedocs.io/en/latest/#multiprocessing\n",
    "    runner_backend='spawn',\n",
    ")\n",
    "results = lab.run_tasks(experiments)"
   ],
   "id": "fabb8e24-cd23-4073-a340-7421d6e3517d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if there is no `fsspec` implementation for my storage provider?\n",
    "\n",
    "You can also define your own storage type that extends\n",
    "`labtech.storage.Storage` and defines `find_keys()`, `exists()`,\n",
    "`file_handle()` and `delete()`. For an example, refer to the\n",
    "implementation of `labtech.storage.LocalStorage`.\n",
    "\n",
    "### Loading lots of cached results is slow, how can I make it faster?\n",
    "\n",
    "If you have a large number of tasks, you may find that the overhead of\n",
    "loading each individual task result from the cache is unacceptably slow\n",
    "when you need to frequently reload previous results for analysis.\n",
    "\n",
    "In such cases, you may find it helpful to create a final task that\n",
    "depends on all of your individual tasks and aggregates all of their\n",
    "results into a single cached result. Note that this final result cache\n",
    "will need to be rebuilt whenever any of its dependent tasks changes or\n",
    "new dependent tasks are added. Furthermore, this approach will require\n",
    "additional storage for the final cache in addition to the individual\n",
    "result caches.\n",
    "\n",
    "The following example demonstrates defining and using an\n",
    "`AggregationTask` to aggregate the results from many individual tasks to\n",
    "create an aggregated cache that can be loaded more efficiently:"
   ],
   "id": "dd941f48-da9b-4299-88d9-2871c8425c78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labtech.types import Task\n",
    "\n",
    "@labtech.task\n",
    "class Experiment:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.seed * self.seed\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class AggregationTask:\n",
    "    sub_tasks: list[Task]\n",
    "\n",
    "    def run(self):\n",
    "        return [\n",
    "            sub_task.result\n",
    "            for sub_task in self.sub_tasks\n",
    "        ]\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    Experiment(\n",
    "        seed=seed\n",
    "    )\n",
    "    for seed in range(1000)\n",
    "]\n",
    "aggregation_task = AggregationTask(\n",
    "    sub_tasks=experiments,\n",
    ")\n",
    "lab = labtech.Lab(storage='storage/aggregation_lab')\n",
    "result = lab.run_task(aggregation_task)"
   ],
   "id": "72ba856c-2b12-491c-bd02-220fd5da8012"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I optimise memory usage in labtech?\n",
    "\n",
    "If you are running a Lab with with `runner_backend='spawn'` (the default\n",
    "on macOS and Windows), Labtech duplicates the results of dependent tasks\n",
    "and the lab context into each task’s process. Therefore, to reduce\n",
    "memory usage (and the computational cost of [pickling and\n",
    "unpickling](https://docs.python.org/3/library/pickle.html) these values\n",
    "when copying them between processes), you should try to keep these\n",
    "values as small as possible. One way to achieve this is to define a\n",
    "\\[`filter_context()`\\]\\[labtech.types.Task.filter_context\\] in order to\n",
    "only pass necessary parts of the context to each task.\n",
    "\n",
    "If you are running a Lab with with `runner_backend='fork'` (the default\n",
    "on Linux), then you can rely on Labtech to share the context between\n",
    "task processes using shared memory. Furthermore,\n",
    "`runner_backend='fork-per-task'` will also share task results between\n",
    "processes using shared memory, but at the cost of forking a new\n",
    "subprocess for each task - `runner_backend='fork-per-task'` is best used\n",
    "when dependency task results are large (so time will be saved through\n",
    "memory sharing) compared to the overall number of tasks (for large\n",
    "numbers of tasks, forking a separate process for each may be a\n",
    "substantial overhead).\n",
    "\n",
    "### How can I see when a task was run and how long it took to execute?\n",
    "\n",
    "Once a task has been executed (or loaded from cache), you can see when\n",
    "it was originally executed and how long it took to execute from the\n",
    "task’s `.result_meta` attribute:"
   ],
   "id": "3a70d99f-5623-4a28-8c5d-7638b433266c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The task was executed at: {aggregation_task.result_meta.start}')\n",
    "print(f'The task execution took: {aggregation_task.result_meta.duration}')"
   ],
   "id": "57ab3724-e692-4dd3-a6a5-79c8d5764864"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I access the results of intermediate/dependency tasks?\n",
    "\n",
    "To conserve memory, labtech unloads the results of\n",
    "intermediate/dependency tasks once their directly dependent tasks have\n",
    "finished executing.\n",
    "\n",
    "A simple approach to access the results of an intermediate task may\n",
    "simply be to include it’s results as part of the result of the task that\n",
    "depends on it - that way you only need to look at the results of the\n",
    "final task(s).\n",
    "\n",
    "Another approach is to include all of the intermediate tasks for which\n",
    "you wish to access the results for in the call to `run_tasks()`:"
   ],
   "id": "1645b811-e319-43b6-bd7d-b0cf94fb2ed5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    Experiment(\n",
    "        seed=seed\n",
    "    )\n",
    "    for seed in range(10)\n",
    "]\n",
    "aggregation_task = AggregationTask(\n",
    "    sub_tasks=experiments,\n",
    ")\n",
    "lab = labtech.Lab(storage=None)\n",
    "results = lab.run_tasks([\n",
    "    aggregation_task,\n",
    "    # Include intermediate tasks to access their results\n",
    "    *experiments,\n",
    "])\n",
    "print([\n",
    "    results[experiment]\n",
    "    for experiment in experiments\n",
    "])"
   ],
   "id": "a5a73bed-a845-415b-bbc4-ec2e2a0cf2c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I construct a multi-step experiment pipeline?\n",
    "\n",
    "Say you want to model a multi-step experiment pipeline, where `StepA` is\n",
    "run before `StepB`, which is run before `StepC`:\n",
    "\n",
    "    StepA -> StepB -> StepC\n",
    "\n",
    "This is modeled in labtech by defining a task type for each step, and\n",
    "having each step depend on the result from the previous step:"
   ],
   "id": "707ce19f-7485-4af4-89c8-5573d61e3f9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labtech.task\n",
    "class StepA:\n",
    "    seed_a: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.seed_a\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class StepB:\n",
    "    task_a: StepA\n",
    "    seed_b: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.task_a.result * self.seed_b\n",
    "\n",
    "\n",
    "@labtech.task\n",
    "class StepC:\n",
    "    task_b: StepB\n",
    "    seed_c: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.task_b.result * self.seed_c\n",
    "\n",
    "\n",
    "task_a = StepA(\n",
    "    seed_a=2,\n",
    ")\n",
    "task_b = StepB(\n",
    "    seed_b=3,\n",
    "    task_a=task_a,\n",
    ")\n",
    "task_c = StepC(\n",
    "    seed_c=5,\n",
    "    task_b=task_b,\n",
    ")\n",
    "\n",
    "lab = labtech.Lab(storage=None)\n",
    "result = lab.run_task(task_c)\n",
    "print(result)"
   ],
   "id": "f2b1a32d-3202-436f-b2f3-cd7cf963ee74"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I visualise my task types, including their parameters and dependencies?\n",
    "\n",
    "`labtech.diagram.display_task_diagram()` can be used to display a\n",
    "[Mermaid diagram](https://mermaid.js.org/syntax/classDiagram.html) of\n",
    "task types for a given list of tasks:"
   ],
   "id": "00f1c855-33a3-446c-abec-33ad9114bc87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labtech.diagram import display_task_diagram\n",
    "\n",
    "display_task_diagram(\n",
    "    [task_c],\n",
    "    direction='RL',\n",
    ")"
   ],
   "id": "15d93d7b-2a49-455a-a8c0-e5f78c5c1bab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`labtech.diagram.build_task_diagram()` can be similarly used to return\n",
    "the Mermaid syntax for the diagram.\n",
    "\n",
    "### How can I use labtech with mlflow?\n",
    "\n",
    "If you want to log a task type as an mlflow “run”, simply add\n",
    "`mlflow_run=True` to the call to `@labtech.task()`, which will:\n",
    "\n",
    "-   Wrap each run of the task with `mlflow.start_run()`\n",
    "-   Tag the run with `labtech_task_type` equal to the task class name\n",
    "-   Log all task parameters with `mlflow.log_param()`\n",
    "\n",
    "The following example demonstrates using labtech with mlflow. Note that\n",
    "you can still make any configuration changes (such as\n",
    "`mlflow.set_experiment()`) before the tasks are run, and you can make\n",
    "additional tracking calls (such as `mlflow.log_metric()` or\n",
    "`mlflow.log_model()`) in the body of your task’s `run()` method:"
   ],
   "id": "3309dd84-6557-496b-adac-9a8072e754e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@labtech.task(mlflow_run=True)\n",
    "class MLRun:\n",
    "    penalty_norm: str | None\n",
    "\n",
    "    def run(self) -> np.ndarray:\n",
    "        clf = LogisticRegression(penalty=self.penalty_norm)\n",
    "        clf.fit(digits_X, digits_y)\n",
    "\n",
    "        labels = clf.predict(digits_X)\n",
    "\n",
    "        train_accuracy = accuracy_score(digits_y, labels)\n",
    "        mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=clf,\n",
    "            artifact_path='digits_model',\n",
    "            input_example=digits_X,\n",
    "            registered_model_name='digits-model',\n",
    "        )\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "runs = [\n",
    "    MLRun(\n",
    "        penalty_norm=penalty_norm,\n",
    "    )\n",
    "    for penalty_norm in [None, 'l2']\n",
    "]\n",
    "\n",
    "mlflow.set_experiment('example_labtech_experiment')\n",
    "lab = labtech.Lab(storage=None)\n",
    "results = lab.run_tasks(runs)"
   ],
   "id": "6ea43d30-4ec6-4487-baec-57c89f552923"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: While the [mlflow\n",
    "> documentation](https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html#step-4-log-the-model-and-its-metadata-to-mlflow)\n",
    "> recommends wrapping only your tracking code with `mlflow.start_run()`,\n",
    "> labtech wraps the entire call to the `run()` method of your task in\n",
    "> order to track execution times in mlflow.\n",
    "\n",
    "> Note: Because mlflow logging will be performed from a separate process\n",
    "> for each task, you must use an mlflow tracking backend that supports\n",
    "> multiple simultaneous connections. Specifically, using an SQLite\n",
    "> backend directly from multiple processes may result in database\n",
    "> locking errors. Instead, consider using local files (the default used\n",
    "> by mlflow), an SQL database that runs as a server (e.g. postgresql,\n",
    "> mysql, or mssql), or running a local mlflow tracking server (which may\n",
    "> itself connect to an sqlite database). For more details, see the\n",
    "> [mlflow backend\n",
    "> documentation](https://mlflow.org/docs/latest/tracking/backend-stores.html).\n",
    "\n",
    "### Why do I see the following error: `An attempt has been made to start a new process before the current process has finished`?\n",
    "\n",
    "When running a Lab with `runner_backend='spawn'` (the default on macOS\n",
    "and Windows), you will see the following error if you do not guard your\n",
    "experiment and lab creation and other non-definition code with\n",
    "`__name__ == '__main__'`:\n",
    "\n",
    "    RuntimeError:\n",
    "            An attempt has been made to start a new process before the\n",
    "            current process has finished its bootstrapping phase.\n",
    "\n",
    "            This probably means that you are not using fork to start your\n",
    "            child processes and you have forgotten to use the proper idiom\n",
    "            in the main module:\n",
    "\n",
    "                if __name__ == '__main__':\n",
    "                    freeze_support()\n",
    "                    ...\n",
    "\n",
    "            The \"freeze_support()\" line can be omitted if the program\n",
    "            is not going to be frozen to produce an executable.\n",
    "\n",
    "To avoid this error, it is recommended that you write all of your\n",
    "non-definition code for a Python script in a `main()` function, and then\n",
    "guard the call to `main()` with `__name__ == '__main__'`:"
   ],
   "id": "e955e54a-e479-4467-9d5a-14975d39ff24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labtech\n",
    "\n",
    "@labtech.task\n",
    "class Experiment:\n",
    "    seed: int\n",
    "\n",
    "    def run(self):\n",
    "        return self.seed * self.seed\n",
    "\n",
    "def main():\n",
    "    experiments = [\n",
    "        Experiment(\n",
    "            seed=seed\n",
    "        )\n",
    "        for seed in range(1000)\n",
    "    ]\n",
    "    lab = labtech.Lab(storage='storage/guarded_lab')\n",
    "    result = lab.run_tasks(experiments)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "5be23e12-734c-4e38-bcfa-86f1e0a9624a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details, see [Safe importing of main\n",
    "module](https://docs.python.org/3/library/multiprocessing.html#multiprocessing-safe-main-import).\n",
    "\n",
    "### Why do I see the following error: `RunnerError: Unable to submit YourTaskType tasks to SpawnProcessRunner because the task type is defined in the __main__ module from an interactive Python session`?\n",
    "\n",
    "You may see this error when running a Lab with `runner_backend='spawn'`\n",
    "(the default on macOS and Windows) from an interactive Python shell\n",
    "(e.g. a Jupyter notebook session or a Python script).\n",
    "\n",
    "The solution to this error is to define all of the classes you are using\n",
    "from your labtech context and tasks (including task types) in a separate\n",
    "`.py` Python module file which you can import into your interactive\n",
    "shell session (e.g. `from my_module import MyClass`).\n",
    "\n",
    "The reason for this error is that “spawned” task subprocesses will not\n",
    "receive a copy the current state of your `__main__` module (which\n",
    "contains the variables you declare interactively in the Python shell,\n",
    "including class definitions). This error does not occur with\n",
    "`runner_backend='fork'` (the default on Linux) because forked\n",
    "subprocesses *do* receive the current state of all modules (including\n",
    "`__main__`) from the parent process.\n",
    "\n",
    "### Why do I see the following error: `AttributeError: Can't get attribute 'YOUR_CLASS' on <module '__main__' (built-in)>`?\n",
    "\n",
    "[See the answer to the question directly\n",
    "above.](#spawn-interactive-main)"
   ],
   "id": "7f15e42b-1c86-4c68-87b6-c162559c5347"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
