{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"README","text":"labtech <p> GitHub - Documentation </p> <p>Labtech makes it easy to define multi-step experiment pipelines and run them with maximal parallelism and result caching:</p> <ul> <li>Defining tasks is simple; write a class with a single <code>run()</code>   method and parameters as dataclass-style attributes.</li> <li>Flexible experiment configuration; simply create task objects   for all of your parameter permutations.</li> <li>Handles pipelines of tasks; any task parameter that is itself a   task will be executed first and make its result available to its   dependent task(s).</li> <li>Implicit parallelism; Labtech resolves task dependencies and   runs tasks in sub-processes with as much parallelism as possible.</li> <li>Implicit caching and loading of task results; configurable and   extensible options for how and where task results are cached.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install labtech\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from time import sleep\n\nimport labtech\n\n# Decorate your task class with @labtech.task:\n@labtech.task\nclass Experiment:\n    # Each Experiment task instance will take `base` and `power` parameters:\n    base: int\n    power: int\n\n    def run(self):\n        # Define the task's run() method to return the result of the experiment:\n        labtech.logger.info(f'Raising {self.base} to the power of {self.power}')\n        sleep(1)\n        return self.base ** self.power\n\n# Configure Experiment parameter permutations\nexperiments = [\n    Experiment(\n        base=base,\n        power=power,\n    )\n    for base in range(5)\n    for power in range(5)\n]\n\n# Configure a Lab to run the experiments:\nlab = labtech.Lab(\n    # Specify a directory to cache results in (running the experiments a second\n    # time will just load results from the cache!):\n    storage='demo_lab',\n    # Control the degree of parallelism:\n    max_workers=5,\n)\n\n# Run the experiments!\nresults = lab.run_tasks(experiments)\nprint([results[experiment] for experiment in experiments])\n</code></pre> <p>Labtech can also produce graphical progress bars in Jupyter when the <code>Lab</code> is initialized with <code>notebook=True</code>:</p> <p></p> <p>Tasks parameters can be any of the following types:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>, <code>dict</code>, <code>Enum</code></li> <li>Task types: A task parameter is a \"nested task\" that will be   executed before its parent so that it may make use of the nested   result.</li> </ul> <p>Here's an example of defining a single long-running task to produce a result for a large number of dependent tasks:</p> <pre><code>from time import sleep\n\nimport labtech\n\n@labtech.task\nclass SlowTask:\n    base: int\n\n    def run(self):\n        sleep(5)\n        return self.base ** 2\n\n@labtech.task\nclass DependentTask:\n    slow_task: SlowTask\n    multiplier: int\n\n    def run(self):\n        return self.multiplier * self.slow_task.result\n\nsome_slow_task = SlowTask(base=42)\ndependent_tasks = [\n    DependentTask(\n        slow_task=some_slow_task,\n        multiplier=multiplier,\n    )\n    for multiplier in range(10)\n]\n\nlab = labtech.Lab(storage='demo_lab')\nresults = lab.run_tasks(dependent_tasks)\nprint([results[task] for task in dependent_tasks])\n</code></pre> <p>To learn more, see:</p> <ul> <li>The API reference for Labs and Tasks</li> <li>More options for cache formats and storage providers</li> <li>More examples</li> </ul>"},{"location":"#mypy-plugin","title":"Mypy Plugin","text":"<p>For mypy type-checking of classes decorated with <code>labtech.task</code>, simply enable the labtech mypy plugin in your <code>mypy.ini</code> file:</p> <pre><code>[mypy]\nplugins = labtech/mypy_plugin.py\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Install Poetry dependencies with <code>make deps</code></li> <li>Run linting, mypy, and tests with <code>make check</code></li> <li>Documentation:<ul> <li>Run local server: <code>make docs-serve</code></li> <li>Build docs: <code>make docs-build</code></li> <li>Deploy docs to GitHub Pages: <code>make docs-github</code></li> <li>Docstring style follows the Google style guide</li> </ul> </li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>Add unit tests</li> </ul>"},{"location":"caching/","title":"Caches and Storage","text":""},{"location":"caching/#caches","title":"Caches","text":"<p>You can control how the results of a particular task type should be formatted for caching by specifying an instance of one of the following Cache classes for the <code>cache</code> argument of the <code>labtech.task</code> decorator:</p>"},{"location":"caching/#labtech.cache.PickleCache","title":"<code>labtech.cache.PickleCache</code>","text":"<p>             Bases: <code>BaseCache</code></p> <p>Default cache that stores results as pickled Python objects.</p> <p>NOTE: As pickle is not secure, you should only load pickle cache results that you trust.</p> Source code in <code>labtech/cache.py</code> <pre><code>class PickleCache(BaseCache):\n\"\"\"Default cache that stores results as\n    [pickled](https://docs.python.org/3/library/pickle.html) Python\n    objects.\n\n    NOTE: As pickle is not secure, you should only load pickle cache\n    results that you trust.\n\n    \"\"\"\n\n    KEY_PREFIX = 'pickle__'\n    RESULT_FILENAME = 'data.pickle'\n\n    def __init__(self, *, serializer: Optional[Serializer] = None,\n                 pickle_protocol: int = pickle.HIGHEST_PROTOCOL):\n        super().__init__(serializer=serializer)\n        self.pickle_protocol = pickle_protocol\n\n    def save_result(self, storage: Storage, task: Task, result: Any):\n        data_file = storage.file_handle(task.cache_key, self.RESULT_FILENAME, mode='wb')\n        with data_file:\n            pickle.dump(result, data_file, protocol=self.pickle_protocol)\n\n    def load_result(self, storage: Storage, task: Task) -&gt; Any:\n        data_file = storage.file_handle(task.cache_key, self.RESULT_FILENAME, mode='rb')\n        with data_file:\n            return pickle.load(data_file)\n</code></pre>"},{"location":"caching/#labtech.cache.NullCache","title":"<code>labtech.cache.NullCache</code>","text":"<p>             Bases: <code>Cache</code></p> <p>Cache that never stores results in the storage provider.</p> Source code in <code>labtech/cache.py</code> <pre><code>class NullCache(Cache):\n\"\"\"Cache that never stores results in the storage provider.\"\"\"\n\n    def cache_key(self, task: Task) -&gt; str:\n        return 'null'\n\n    def is_cached(self, storage: Storage, task: Task) -&gt; bool:\n        return False\n\n    def save(self, storage: Storage, task: Task, result: Any):\n        pass\n\n    def load_task(self, storage: Storage, task_type: Type[Task], key: str) -&gt; Task:\n        raise TaskNotFound\n\n    def load_result(self, storage: Storage, task: Task) -&gt; Any:\n        raise CacheError('Loading a result from a NullCache is not supported.')\n\n    def load_cache_timestamp(self, storage: Storage, task: Task) -&gt; Any:\n        raise CacheError('Loading a cache_timestamp from a NullCache is not supported.')\n\n    def delete(self, storage: Storage, task: Task):\n        pass\n</code></pre>"},{"location":"caching/#custom-caches","title":"Custom Caches","text":"<p>You can define your own type of Cache with its own format or behaviour by inheriting from <code>BaseCache</code>:</p>"},{"location":"caching/#labtech.cache.BaseCache","title":"<code>labtech.cache.BaseCache</code>","text":"<p>             Bases: <code>Cache</code></p> <p>Base class for defining a Cache that will store results in a storage provider.</p> Source code in <code>labtech/cache.py</code> <pre><code>class BaseCache(Cache):\n\"\"\"Base class for defining a Cache that will store results in a\n    storage provider.\"\"\"\n\n    KEY_PREFIX = ''\n\"\"\"Prefix for all files created by this Cache type - should be\n    different for each Cache type to avoid conflicts.\"\"\"\n\n    METADATA_FILENAME = 'metadata.json'\n\n    def __init__(self, *, serializer: Optional[Serializer] = None):\n        self.serializer = serializer or Serializer()\n\n    def cache_key(self, task: Task) -&gt; str:\n        serialized_str = json.dumps(self.serializer.serialize_task(task)).encode('utf-8')\n        # Use sha1, as it is the same hash as git, produces short\n        # hashes, and security concerns with sha1 are not relevant to\n        # our use case.\n        hashed = hashlib.sha1(serialized_str).hexdigest()\n        return f'{self.KEY_PREFIX}{task.__class__.__qualname__}__{hashed}'\n\n    def is_cached(self, storage: Storage, task: Task) -&gt; bool:\n        return storage.exists(task.cache_key)\n\n    def save(self, storage: Storage, task: Task, result: Any):\n        metadata = {\n            'labtech_version': labtech_version,\n            'cache': self.__class__.__qualname__,\n            'cache_key': task.cache_key,\n            'datetime': datetime.now().isoformat(),\n            'task': self.serializer.serialize_task(task),\n        }\n        metadata_file = storage.file_handle(task.cache_key, self.METADATA_FILENAME, mode='w')\n        with metadata_file:\n            json.dump(metadata, metadata_file, indent=2)\n        self.save_result(storage, task, result)\n\n    def load_metadata(self, storage: Storage, task_type: Type[Task], key: str) -&gt; dict[str, Any]:\n        if not key.startswith(f'{self.KEY_PREFIX}{task_type.__qualname__}'):\n            raise TaskNotFound\n        with storage.file_handle(key, self.METADATA_FILENAME, mode='r') as metadata_file:\n            metadata = json.load(metadata_file)\n        if metadata.get('cache') != self.__class__.__qualname__:\n            raise TaskNotFound\n        return metadata\n\n    def load_task(self, storage: Storage, task_type: Type[Task], key: str) -&gt; Task:\n        metadata = self.load_metadata(storage, task_type, key)\n        task = self.serializer.deserialize_task(metadata['task'], cache_timestamp=metadata['datetime'])\n        if not isinstance(task, task_type):\n            raise TaskNotFound\n        return task\n\n    def load_cache_timestamp(self, storage: Storage, task: Task):\n        metadata = self.load_metadata(storage, type(task), task.cache_key)\n        return metadata['datetime']\n\n    def delete(self, storage: Storage, task: Task):\n        storage.delete(task.cache_key)\n\n    @abstractmethod\n    def save_result(self, storage: Storage, task: Task, result: Any):\n\"\"\"Saves the given task result into the storage provider.\n\n        Args:\n            storage: Storage provider to save the result into\n            task: task instance the result belongs to\n            result: result to save\n\n        \"\"\"\n</code></pre>"},{"location":"caching/#labtech.cache.BaseCache.KEY_PREFIX","title":"<code>KEY_PREFIX = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prefix for all files created by this Cache type - should be different for each Cache type to avoid conflicts.</p>"},{"location":"caching/#labtech.cache.BaseCache.save_result","title":"<code>save_result(storage: Storage, task: Task, result: Any)</code>  <code>abstractmethod</code>","text":"<p>Saves the given task result into the storage provider.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>             (<code>Storage</code>)         \u2013          <p>Storage provider to save the result into</p> </li> <li> <code>task</code>             (<code>Task</code>)         \u2013          <p>task instance the result belongs to</p> </li> <li> <code>result</code>             (<code>Any</code>)         \u2013          <p>result to save</p> </li> </ul> Source code in <code>labtech/cache.py</code> <pre><code>@abstractmethod\ndef save_result(self, storage: Storage, task: Task, result: Any):\n\"\"\"Saves the given task result into the storage provider.\n\n    Args:\n        storage: Storage provider to save the result into\n        task: task instance the result belongs to\n        result: result to save\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#labtech.cache.BaseCache.load_result","title":"<code>load_result(storage: Storage, task: Task) -&gt; Any</code>  <code>abstractmethod</code>","text":"<p>Loads the result for the given task from the storage provider.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>             (<code>Storage</code>)         \u2013          <p>Storage provider to load the result from</p> </li> <li> <code>task</code>             (<code>Task</code>)         \u2013          <p>task instance to load the result for</p> </li> </ul> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef load_result(self, storage: Storage, task: Task) -&gt; Any:\n\"\"\"Loads the result for the given task from the storage provider.\n\n    Args:\n        storage: Storage provider to load the result from\n        task: task instance to load the result for\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#storage","title":"Storage","text":"<p>You can set the storage location for caching task results by specifying an instance of one of the following Storage classes for the <code>storage</code> argument of your <code>Lab</code>:</p>"},{"location":"caching/#labtech.storage.LocalStorage","title":"<code>labtech.storage.LocalStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Storage provider that stores cached results in a local filesystem directory.</p> Source code in <code>labtech/storage.py</code> <pre><code>class LocalStorage(Storage):\n\"\"\"Storage provider that stores cached results in a local filesystem\n    directory.\"\"\"\n\n    def __init__(self, storage_dir: str):\n\"\"\"\n        Args:\n            storage_dir: Path to the directory where cached results will be\n                stored. The directory will be created if it does not already\n                exist.\n        \"\"\"\n        self._storage_path = Path(storage_dir).resolve()\n        if not self._storage_path.exists():\n            self._storage_path.mkdir()\n\n    def _key_path(self, key: str) -&gt; Path:\n        key_path = (self._storage_path / key).resolve()\n        if key_path.parent != self._storage_path:\n            raise StorageError((f\"Key '{key}' should only reference a directory directly \"\n                                f\"under the storage directory '{self._storage_path}'\"))\n        return key_path\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return sorted([key_path.name for key_path in self._storage_path.iterdir()])\n\n    def exists(self, key: str) -&gt; bool:\n        key_path = self._key_path(key)\n        return key_path.exists()\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        key_path = self._key_path(key)\n        try:\n            key_path.mkdir()\n        except FileExistsError:\n            pass\n        file_path = (key_path / filename).resolve()\n        if file_path.parent != key_path:\n            raise StorageError((f\"Filename '{filename}' should only reference a directory directly \"\n                                f\"under the storage key directory '{key_path}'\"))\n        return file_path.open(mode=mode)\n\n    def delete(self, key: str):\n        key_path = self._key_path(key)\n        if key_path.exists():\n            shutil.rmtree(key_path)\n</code></pre>"},{"location":"caching/#labtech.storage.LocalStorage.__init__","title":"<code>__init__(storage_dir: str)</code>","text":"<p>Parameters:</p> <ul> <li> <code>storage_dir</code>             (<code>str</code>)         \u2013          <p>Path to the directory where cached results will be stored. The directory will be created if it does not already exist.</p> </li> </ul> Source code in <code>labtech/storage.py</code> <pre><code>def __init__(self, storage_dir: str):\n\"\"\"\n    Args:\n        storage_dir: Path to the directory where cached results will be\n            stored. The directory will be created if it does not already\n            exist.\n    \"\"\"\n    self._storage_path = Path(storage_dir).resolve()\n    if not self._storage_path.exists():\n        self._storage_path.mkdir()\n</code></pre>"},{"location":"caching/#labtech.storage.NullStorage","title":"<code>labtech.storage.NullStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Storage provider that does not store cached results.</p> Source code in <code>labtech/storage.py</code> <pre><code>class NullStorage(Storage):\n\"\"\"Storage provider that does not store cached results.\"\"\"\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return []\n\n    def exists(self, key: str) -&gt; bool:\n        return False\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        return open(os.devnull, mode=mode)\n\n    def delete(self, key: str):\n        pass\n</code></pre>"},{"location":"caching/#custom-storage","title":"Custom Storage","text":"<p>To store cached results with an alternative storage provider (such as a storage bucket in the cloud), you can define your own type of Storage by inheriting from <code>Storage</code>:</p>"},{"location":"caching/#labtech.storage.Storage","title":"<code>labtech.storage.Storage</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Storage provider for persisting cached task results.</p> Source code in <code>labtech/types.py</code> <pre><code>class Storage(ABC):\n\"\"\"Storage provider for persisting cached task results.\"\"\"\n\n    @abstractmethod\n    def find_keys(self) -&gt; Sequence[str]:\n\"\"\"Returns the keys of all currently cached task results.\"\"\"\n\n    @abstractmethod\n    def exists(self, key: str) -&gt; bool:\n\"\"\"Returns `True` if the given task `key` is present in the storage\n        cache.\"\"\"\n\n    @abstractmethod\n    def file_handle(self, key: str, filename: str, *, mode: str) -&gt; IO:\n\"\"\"Opens and returns a File-like object for a single file within the\n        storage cache.\n\n        Args:\n            key: The task key of the cached result containing the file.\n            filename: The name of the file to open.\n            mode: The file mode to open the file with.\n\n        \"\"\"\n\n    @abstractmethod\n    def delete(self, key: str) -&gt; None:\n\"\"\"Deletes the cached result for the task with the given `key`.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.find_keys","title":"<code>find_keys() -&gt; Sequence[str]</code>  <code>abstractmethod</code>","text":"<p>Returns the keys of all currently cached task results.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef find_keys(self) -&gt; Sequence[str]:\n\"\"\"Returns the keys of all currently cached task results.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.exists","title":"<code>exists(key: str) -&gt; bool</code>  <code>abstractmethod</code>","text":"<p>Returns <code>True</code> if the given task <code>key</code> is present in the storage cache.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef exists(self, key: str) -&gt; bool:\n\"\"\"Returns `True` if the given task `key` is present in the storage\n    cache.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.file_handle","title":"<code>file_handle(key: str, filename: str, *, mode: str) -&gt; IO</code>  <code>abstractmethod</code>","text":"<p>Opens and returns a File-like object for a single file within the storage cache.</p> <p>Parameters:</p> <ul> <li> <code>key</code>             (<code>str</code>)         \u2013          <p>The task key of the cached result containing the file.</p> </li> <li> <code>filename</code>             (<code>str</code>)         \u2013          <p>The name of the file to open.</p> </li> <li> <code>mode</code>             (<code>str</code>)         \u2013          <p>The file mode to open the file with.</p> </li> </ul> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef file_handle(self, key: str, filename: str, *, mode: str) -&gt; IO:\n\"\"\"Opens and returns a File-like object for a single file within the\n    storage cache.\n\n    Args:\n        key: The task key of the cached result containing the file.\n        filename: The name of the file to open.\n        mode: The file mode to open the file with.\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.delete","title":"<code>delete(key: str) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Deletes the cached result for the task with the given <code>key</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef delete(self, key: str) -&gt; None:\n\"\"\"Deletes the cached result for the task with the given `key`.\"\"\"\n</code></pre>"},{"location":"core/","title":"Labs and Tasks","text":"<p>In this document you'll find API reference documentation for configuring Labs and tasks. For a tutorial-style explanation, see the README.</p> <p>Any task type decorated with <code>labtech.task</code> will provide the following attributes and methods:</p>"},{"location":"core/#labtech.Lab","title":"<code>labtech.Lab</code>","text":"<p>Primary interface for configuring, running, and getting results of tasks.</p> <p>A Lab can be used to run tasks with <code>run_tasks()</code>.</p> <p>Previously cached tasks can be retrieved with <code>cached_tasks()</code>, and can then have their results loaded with <code>run_tasks()</code> or be removed from the cache storage with <code>uncache_tasks()</code>.</p> Source code in <code>labtech/lab.py</code> <pre><code>class Lab:\n\"\"\"Primary interface for configuring, running, and getting results of tasks.\n\n    A Lab can be used to run tasks with [`run_tasks()`][labtech.Lab.run_tasks].\n\n    Previously cached tasks can be retrieved with\n    [`cached_tasks()`][labtech.Lab.cached_tasks], and can then have\n    their results loaded with [`run_tasks()`][labtech.Lab.run_tasks]\n    or be removed from the cache storage with\n    [`uncache_tasks()`][labtech.Lab.uncache_tasks].\n\n    \"\"\"\n\n    def __init__(self, *,\n                 storage: Union[str, None, Storage],\n                 continue_on_failure: bool = True,\n                 max_workers: Optional[int] = None,\n                 notebook: bool = False,\n                 context: Optional[dict[str, Any]] = None):\n\"\"\"\n        Args:\n            storage: Where task results should be cached to. A string will be\n                interpreted as the path to a local directory, `None` will result\n                in no caching. Any [Storage][labtech.types.Storage] instance may\n                also be specified.\n            continue_on_failure: If `True`, exceptions raised by tasks will be\n                logged, but execution of other tasks will continue.\n            max_workers: The maximum number of parallel worker processes for\n                running tasks. Uses the same default as\n                `concurrent.futures.ProcessPoolExecutor`: the number of\n                processors on the machine.\n            notebook: Should be set to `True` if run from a Jupyter notebook\n                for graphical progress bars.\n            context: A dictionary of additional variables to make available to\n                tasks. The context will not be cached, so the values should not\n                affect results (e.g. parallelism factors) or should be kept\n                constant between runs (e.g. datasets).\n        \"\"\"\n        if isinstance(storage, str):\n            storage = LocalStorage(storage)\n        elif storage is None:\n            storage = NullStorage()\n        self._storage = storage\n        self.continue_on_failure = continue_on_failure\n        self.max_workers = max_workers\n        self.notebook = notebook\n        if context is None:\n            context = {}\n        self.context = context\n\n    def run_tasks(self, tasks: Sequence[Task], *,\n                  bust_cache: bool = False,\n                  keep_nested_results: bool = False,\n                  disable_progress: bool = False) -&gt; Dict[Task, Any]:\n\"\"\"Run the given tasks with as much process parallelism as possible.\n        Loads task results from the cache storage where possible and\n        caches results of executed tasks.\n\n        Any attribute of a task that is itself a task object is\n        considered a \"nested task\", and will be executed or loaded so\n        that it's result is made available to its parent task. If the\n        same task is nested inside multiple task objects, it will only\n        be executed/loaded once.\n\n        As well as returning the results, each task's result will be\n        assigned to a `result` attribute on the task itself (including\n        nested tasks when `keep_nested_results` is `True`).\n\n        Args:\n            tasks: The tasks to execute. Each should be an instance of a class\n                decorated with [`labtech.task`][labtech.task].\n            bust_cache: If `True`, no task results will be loaded from the\n                cache storage; all tasks will be re-executed.\n            keep_nested_results: If `False`, results of nested tasks that were\n                executed or loaded in order to complete the provided tasks will\n                be cleared from memory once they are no longer needed.\n            disable_progress: If `True`, do not display a tqdm progress bar\n                tracking task execution.\n\n        Returns:\n            A dictionary mapping each of the provided tasks to its\n                corresponding result.\n\n        \"\"\"\n        runner = TaskRunner(self,\n                            bust_cache=bust_cache,\n                            keep_nested_results=keep_nested_results,\n                            disable_progress=disable_progress)\n        return runner.run(tasks)\n\n    def cached_tasks(self, task_types: Sequence[Type[Task]]) -&gt; Sequence[Task]:\n\"\"\"Returns all task instances present in the Lab's cache storage for\n        the given `task_types`, each of which should be a task class\n        decorated with [`labtech.task`][labtech.task].\n\n        Does not load task results from the cache storage, but they\n        can be loaded by calling\n        [`run_tasks()`][labtech.Lab.run_tasks] with the returned task\n        instances.\n\n        \"\"\"\n        keys = self._storage.find_keys()\n        tasks = []\n        for key in keys:\n            for task_type in task_types:\n                try:\n                    task = task_type._lt.cache.load_task(self._storage, task_type, key)\n                except TaskNotFound:\n                    pass\n                else:\n                    tasks.append(task)\n                    break\n        return tasks\n\n    def is_cached(self, task: Task) -&gt; bool:\n\"\"\"Checks if a result is present for given task in the Lab's cache\n        storage.\"\"\"\n        return task._lt.cache.is_cached(self._storage, task)\n\n    def uncache_tasks(self, tasks: Sequence[Task]):\n\"\"\"Removes cached results for the given tasks from the Lab's cache\n        storage.\"\"\"\n        for task in tasks:\n            if self.is_cached(task):\n                task._lt.cache.delete(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.Lab.__init__","title":"<code>__init__(*, storage: Union[str, None, Storage], continue_on_failure: bool = True, max_workers: Optional[int] = None, notebook: bool = False, context: Optional[dict[str, Any]] = None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>storage</code>             (<code>Union[str, None, Storage]</code>)         \u2013          <p>Where task results should be cached to. A string will be interpreted as the path to a local directory, <code>None</code> will result in no caching. Any Storage instance may also be specified.</p> </li> <li> <code>continue_on_failure</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If <code>True</code>, exceptions raised by tasks will be logged, but execution of other tasks will continue.</p> </li> <li> <code>max_workers</code>             (<code>Optional[int]</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of parallel worker processes for running tasks. Uses the same default as <code>concurrent.futures.ProcessPoolExecutor</code>: the number of processors on the machine.</p> </li> <li> <code>notebook</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Should be set to <code>True</code> if run from a Jupyter notebook for graphical progress bars.</p> </li> <li> <code>context</code>             (<code>Optional[dict[str, Any]]</code>, default:                 <code>None</code> )         \u2013          <p>A dictionary of additional variables to make available to tasks. The context will not be cached, so the values should not affect results (e.g. parallelism factors) or should be kept constant between runs (e.g. datasets).</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def __init__(self, *,\n             storage: Union[str, None, Storage],\n             continue_on_failure: bool = True,\n             max_workers: Optional[int] = None,\n             notebook: bool = False,\n             context: Optional[dict[str, Any]] = None):\n\"\"\"\n    Args:\n        storage: Where task results should be cached to. A string will be\n            interpreted as the path to a local directory, `None` will result\n            in no caching. Any [Storage][labtech.types.Storage] instance may\n            also be specified.\n        continue_on_failure: If `True`, exceptions raised by tasks will be\n            logged, but execution of other tasks will continue.\n        max_workers: The maximum number of parallel worker processes for\n            running tasks. Uses the same default as\n            `concurrent.futures.ProcessPoolExecutor`: the number of\n            processors on the machine.\n        notebook: Should be set to `True` if run from a Jupyter notebook\n            for graphical progress bars.\n        context: A dictionary of additional variables to make available to\n            tasks. The context will not be cached, so the values should not\n            affect results (e.g. parallelism factors) or should be kept\n            constant between runs (e.g. datasets).\n    \"\"\"\n    if isinstance(storage, str):\n        storage = LocalStorage(storage)\n    elif storage is None:\n        storage = NullStorage()\n    self._storage = storage\n    self.continue_on_failure = continue_on_failure\n    self.max_workers = max_workers\n    self.notebook = notebook\n    if context is None:\n        context = {}\n    self.context = context\n</code></pre>"},{"location":"core/#labtech.Lab.run_tasks","title":"<code>run_tasks(tasks: Sequence[Task], *, bust_cache: bool = False, keep_nested_results: bool = False, disable_progress: bool = False) -&gt; Dict[Task, Any]</code>","text":"<p>Run the given tasks with as much process parallelism as possible. Loads task results from the cache storage where possible and caches results of executed tasks.</p> <p>Any attribute of a task that is itself a task object is considered a \"nested task\", and will be executed or loaded so that it's result is made available to its parent task. If the same task is nested inside multiple task objects, it will only be executed/loaded once.</p> <p>As well as returning the results, each task's result will be assigned to a <code>result</code> attribute on the task itself (including nested tasks when <code>keep_nested_results</code> is <code>True</code>).</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>             (<code>Sequence[Task]</code>)         \u2013          <p>The tasks to execute. Each should be an instance of a class decorated with <code>labtech.task</code>.</p> </li> <li> <code>bust_cache</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>True</code>, no task results will be loaded from the cache storage; all tasks will be re-executed.</p> </li> <li> <code>keep_nested_results</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>False</code>, results of nested tasks that were executed or loaded in order to complete the provided tasks will be cleared from memory once they are no longer needed.</p> </li> <li> <code>disable_progress</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>True</code>, do not display a tqdm progress bar tracking task execution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[Task, Any]</code>         \u2013          <p>A dictionary mapping each of the provided tasks to its corresponding result.</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def run_tasks(self, tasks: Sequence[Task], *,\n              bust_cache: bool = False,\n              keep_nested_results: bool = False,\n              disable_progress: bool = False) -&gt; Dict[Task, Any]:\n\"\"\"Run the given tasks with as much process parallelism as possible.\n    Loads task results from the cache storage where possible and\n    caches results of executed tasks.\n\n    Any attribute of a task that is itself a task object is\n    considered a \"nested task\", and will be executed or loaded so\n    that it's result is made available to its parent task. If the\n    same task is nested inside multiple task objects, it will only\n    be executed/loaded once.\n\n    As well as returning the results, each task's result will be\n    assigned to a `result` attribute on the task itself (including\n    nested tasks when `keep_nested_results` is `True`).\n\n    Args:\n        tasks: The tasks to execute. Each should be an instance of a class\n            decorated with [`labtech.task`][labtech.task].\n        bust_cache: If `True`, no task results will be loaded from the\n            cache storage; all tasks will be re-executed.\n        keep_nested_results: If `False`, results of nested tasks that were\n            executed or loaded in order to complete the provided tasks will\n            be cleared from memory once they are no longer needed.\n        disable_progress: If `True`, do not display a tqdm progress bar\n            tracking task execution.\n\n    Returns:\n        A dictionary mapping each of the provided tasks to its\n            corresponding result.\n\n    \"\"\"\n    runner = TaskRunner(self,\n                        bust_cache=bust_cache,\n                        keep_nested_results=keep_nested_results,\n                        disable_progress=disable_progress)\n    return runner.run(tasks)\n</code></pre>"},{"location":"core/#labtech.Lab.cached_tasks","title":"<code>cached_tasks(task_types: Sequence[Type[Task]]) -&gt; Sequence[Task]</code>","text":"<p>Returns all task instances present in the Lab's cache storage for the given <code>task_types</code>, each of which should be a task class decorated with <code>labtech.task</code>.</p> <p>Does not load task results from the cache storage, but they can be loaded by calling <code>run_tasks()</code> with the returned task instances.</p> Source code in <code>labtech/lab.py</code> <pre><code>def cached_tasks(self, task_types: Sequence[Type[Task]]) -&gt; Sequence[Task]:\n\"\"\"Returns all task instances present in the Lab's cache storage for\n    the given `task_types`, each of which should be a task class\n    decorated with [`labtech.task`][labtech.task].\n\n    Does not load task results from the cache storage, but they\n    can be loaded by calling\n    [`run_tasks()`][labtech.Lab.run_tasks] with the returned task\n    instances.\n\n    \"\"\"\n    keys = self._storage.find_keys()\n    tasks = []\n    for key in keys:\n        for task_type in task_types:\n            try:\n                task = task_type._lt.cache.load_task(self._storage, task_type, key)\n            except TaskNotFound:\n                pass\n            else:\n                tasks.append(task)\n                break\n    return tasks\n</code></pre>"},{"location":"core/#labtech.Lab.is_cached","title":"<code>is_cached(task: Task) -&gt; bool</code>","text":"<p>Checks if a result is present for given task in the Lab's cache storage.</p> Source code in <code>labtech/lab.py</code> <pre><code>def is_cached(self, task: Task) -&gt; bool:\n\"\"\"Checks if a result is present for given task in the Lab's cache\n    storage.\"\"\"\n    return task._lt.cache.is_cached(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.Lab.uncache_tasks","title":"<code>uncache_tasks(tasks: Sequence[Task])</code>","text":"<p>Removes cached results for the given tasks from the Lab's cache storage.</p> Source code in <code>labtech/lab.py</code> <pre><code>def uncache_tasks(self, tasks: Sequence[Task]):\n\"\"\"Removes cached results for the given tasks from the Lab's cache\n    storage.\"\"\"\n    for task in tasks:\n        if self.is_cached(task):\n            task._lt.cache.delete(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.task","title":"<code>labtech.task(*args, cache: Union[CacheDefault, None, Cache] = CACHE_DEFAULT, max_parallel: Optional[int] = None)</code>","text":"<p>Class decorator for defining task type classes.</p> <p>Attribute definitions in task types are handled in the same way as [<code>dataclasses</code>], and they should capture all parameters of the task type. Parameter attributes can be any of the following types:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>, <code>dict</code>, <code>Enum</code></li> <li>Task types: A task parameter is a \"nested task\" that will be executed   before its parent so that it may make use of the nested result.</li> </ul> <p>The task type is expected to define a <code>run()</code> method that takes no arguments (other than <code>self</code>). The <code>run()</code> method should execute the task as parameterised by the task's attributes and return the result of the task.</p> <p>Example usage:</p> <pre><code>@labtech.task\nclass Experiment:\n    seed: int\n    multiplier: int\n\n    def run(self):\n        return self.seed * self.multiplier.value\n\nexperiment = Experiment(seed=1, multiplier=2)\n</code></pre> <p>You can also provide arguments to the decorator to control caching and parallelism:</p> <pre><code>@labtech.task(cache=None, max_parallel=1)\nclass Experiment:\n    ...\n\n    def run(self):\n        ...\n</code></pre> <p>Parameters:</p> <ul> <li> <code>cache</code>             (<code>Union[CacheDefault, None, Cache]</code>, default:                 <code>CACHE_DEFAULT</code> )         \u2013          <p>The Cache that controls how task results are formatted for caching. Can be set to an instance of any <code>Cache</code> class, or <code>None</code> to disable caching of this type of task. Defaults to a <code>PickleCache</code>.</p> </li> <li> <code>max_parallel</code>             (<code>Optional[int]</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of instances of this task type that are allowed to run simultaneously in separate sub-processes. Useful to set if running too many instances of this particular task simultaneously will exhaust system memory or processing resources.</p> </li> </ul> Source code in <code>labtech/tasks.py</code> <pre><code>def task(*args,\n         cache: Union[CacheDefault, None, Cache] = CACHE_DEFAULT,\n         max_parallel: Optional[int] = None):\n\"\"\"Class decorator for defining task type classes.\n\n    Attribute definitions in task types are handled in the same way as\n    [`dataclasses`], and they should capture all parameters of the\n    task type. Parameter attributes can be any of the following types:\n\n    * Simple scalar types: `str`, `bool`, `float`, `int`, `None`\n    * Collections of any of these types: `list`, `tuple`, `dict`, `Enum`\n    * Task types: A task parameter is a \"nested task\" that will be executed\n      before its parent so that it may make use of the nested result.\n\n    The task type is expected to define a `run()` method that takes no\n    arguments (other than `self`). The `run()` method should execute\n    the task as parameterised by the task's attributes and return the\n    result of the task.\n\n    Example usage:\n\n    ```python\n    @labtech.task\n    class Experiment:\n        seed: int\n        multiplier: int\n\n        def run(self):\n            return self.seed * self.multiplier.value\n\n    experiment = Experiment(seed=1, multiplier=2)\n    ```\n\n    You can also provide arguments to the decorator to control caching\n    and parallelism:\n\n    ```python\n    @labtech.task(cache=None, max_parallel=1)\n    class Experiment:\n        ...\n\n        def run(self):\n            ...\n    ```\n\n    Args:\n        cache: The Cache that controls how task results are formatted for\n            caching. Can be set to an instance of any\n            [`Cache`](caching.md#caches) class, or `None` to disable caching\n            of this type of task. Defaults to a\n            [`PickleCache`][labtech.cache.PickleCache].\n        max_parallel: The maximum number of instances of this task type that\n            are allowed to run simultaneously in separate sub-processes. Useful\n            to set if running too many instances of this particular task\n            simultaneously will exhaust system memory or processing resources.\n\n    \"\"\"\n\n    def decorator(cls):\n        nonlocal cache\n\n        reserved_attrs = [\n            '_lt', '_is_task', 'cache_key', 'result', '_results_map', '_set_results_map',\n            'cache_timestamp', '_set_cache_timestamp', 'context', 'set_context',\n        ]\n        if not is_task_type(cls):\n            for reserved_attr in reserved_attrs:\n                if hasattr(cls, reserved_attr):\n                    raise AttributeError(f\"Task type already defines reserved attribute '{reserved_attr}'.\")\n\n        orig_post_init = getattr(cls, '__post_init__', None)\n        cls.__post_init__ = _task_post_init\n\n        cls = dataclass(frozen=True, eq=True, order=True)(cls)\n\n        run_func = getattr(cls, 'run', None)\n        if not callable(run_func):\n            raise NotImplementedError(f\"Task type '{cls.__name__}' must define a 'run' method\")\n\n        if cache is CACHE_DEFAULT:\n            cache = PickleCache()\n        elif cache is None:\n            cache = NullCache()\n\n        cls._lt = TaskInfo(\n            cache=cast(Cache, cache),\n            orig_post_init=orig_post_init,\n            max_parallel=max_parallel,\n        )\n        cls.__getstate__ = _task__getstate__\n        cls.__setstate__ = _task__setstate__\n        cls._set_results_map = _task_set_results_map\n        cls._set_cache_timestamp = _task_set_cache_timestamp\n        cls.result = property(_task_result)\n        cls.set_context = _task_set_context\n        return cls\n\n    if len(args) &gt; 0 and isclass(args[0]):\n        return decorator(args[0], *args[1:])\n    else:\n        return decorator\n</code></pre>"},{"location":"core/#labtech.types.Task","title":"<code>labtech.types.Task</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Interface provided by any class that is decorated by <code>labtech.task</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>class Task(Protocol):\n\"\"\"Interface provided by any class that is decorated by\n    [`labtech.task`][labtech.task].\"\"\"\n    _lt: TaskInfo\n    _is_task: Literal[True]\n    _results_map: Optional[ResultsMap]\n    cache_key: str\n\"\"\"The key that uniquely identifies the location for this task within cache storage.\"\"\"\n    context: Optional[dict[str, Any]]\n\"\"\"Context variables from the Lab that can be accessed when the task is running.\"\"\"\n    cache_timestamp: Optional[datetime]\n\"\"\"The timestamp of cache creation if the task's result was loaded from cache.\"\"\"\n\n    def _set_results_map(self, results_map: ResultsMap):\n        pass\n\n    def _set_cache_timestamp(self, cache_timestamp: datetime):\n        pass\n\n    @property\n    def result(self) -&gt; Any:\n\"\"\"Returns the result executed/loaded for this task. If no result is\n        available in memory, accessing this property raises a `TaskError`.\"\"\"\n\n    def set_context(self, context: dict[str, Any]):\n\"\"\"Set the context that is made available to the task while it is\n        running.\"\"\"\n\n    def run(self):\n\"\"\"User-provided method that executes the task parameterised by the\n        attributes of the task.\n\n        Usually executed by [`Lab.run_tasks()`][labtech.Lab.run_tasks]\n        instead of being called directly.\n\n        \"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.cache_key","title":"<code>cache_key: str</code>  <code>instance-attribute</code>","text":"<p>The key that uniquely identifies the location for this task within cache storage.</p>"},{"location":"core/#labtech.types.Task.context","title":"<code>context: Optional[dict[str, Any]]</code>  <code>instance-attribute</code>","text":"<p>Context variables from the Lab that can be accessed when the task is running.</p>"},{"location":"core/#labtech.types.Task.cache_timestamp","title":"<code>cache_timestamp: Optional[datetime]</code>  <code>instance-attribute</code>","text":"<p>The timestamp of cache creation if the task's result was loaded from cache.</p>"},{"location":"core/#labtech.types.Task.result","title":"<code>result: Any</code>  <code>property</code>","text":"<p>Returns the result executed/loaded for this task. If no result is available in memory, accessing this property raises a <code>TaskError</code>.</p>"},{"location":"core/#labtech.types.Task.set_context","title":"<code>set_context(context: dict[str, Any])</code>","text":"<p>Set the context that is made available to the task while it is running.</p> Source code in <code>labtech/types.py</code> <pre><code>def set_context(self, context: dict[str, Any]):\n\"\"\"Set the context that is made available to the task while it is\n    running.\"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.run","title":"<code>run()</code>","text":"<p>User-provided method that executes the task parameterised by the attributes of the task.</p> <p>Usually executed by <code>Lab.run_tasks()</code> instead of being called directly.</p> Source code in <code>labtech/types.py</code> <pre><code>def run(self):\n\"\"\"User-provided method that executes the task parameterised by the\n    attributes of the task.\n\n    Usually executed by [`Lab.run_tasks()`][labtech.Lab.run_tasks]\n    instead of being called directly.\n\n    \"\"\"\n</code></pre>"},{"location":"core/#labtech.is_task_type","title":"<code>labtech.is_task_type(cls)</code>","text":"<p>Returns <code>True</code> if the given <code>cls</code> is a class decorated with <code>labtech.task</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>def is_task_type(cls):\n\"\"\"Returns `True` if the given `cls` is a class decorated with\n    [`labtech.task`][labtech.task].\"\"\"\n    return isclass(cls) and hasattr(cls, '_lt')\n</code></pre>"},{"location":"core/#labtech.is_task","title":"<code>labtech.is_task(obj)</code>","text":"<p>Returns <code>True</code> if the given <code>obj</code> is an instance of a task class.</p> Source code in <code>labtech/types.py</code> <pre><code>def is_task(obj):\n\"\"\"Returns `True` if the given `obj` is an instance of a task class.\"\"\"\n    return hasattr(obj, '_is_task')\n</code></pre>"},{"location":"core/#labtech.logger","title":"<code>labtech.logger = get_logger()</code>  <code>module-attribute</code>","text":"<p><code>logging.Logger</code> object that labtech logs events to during task execution.</p> <p>Can be used to customize logging and to write additional logs from task <code>run()</code> methods:</p> <pre><code>import logging\nfrom labtech import logger\n\n# Change verbosity of logging\nlogger.setLevel(logging.ERROR)\n\n# Logging methods to call from inside your task's run() method:\nlogger.info('Useful info from task: ...')\nlogger.warning('Warning from task: ...')\nlogger.error('Error from task: ...')\n</code></pre>"}]}