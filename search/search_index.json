{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"README","text":"labtech <p> GitHub - Documentation </p> <p>Labtech makes it easy to define multi-step experiment pipelines and run them with maximal parallelism and result caching:</p> <ul> <li>Defining tasks is simple; write a class with a single <code>run()</code>   method and parameters as dataclass-style attributes.</li> <li>Flexible experiment configuration; simply create task objects   for all of your parameter permutations.</li> <li>Handles pipelines of tasks; any task parameter that is itself a   task will be executed first and make its result available to its   dependent task(s).</li> <li>Implicit parallelism; Labtech resolves task dependencies and   runs tasks in sub-processes with as much parallelism as possible.</li> <li>Implicit caching and loading of task results; configurable and   extensible options for how and where task results are cached.</li> <li>Integration with mlflow; Automatically   log task runs to mlflow with all of their parameters.</li> <li>Easily scale to a multi-machine cluster; Built-in support for   running tasks across an easy-to-setup Ray   cluster.</li> </ul> <p>To learn more about how Labtech can speed up your experiments, check out the Kiwi Pycon presentation:</p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install labtech\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from time import sleep\n\nimport labtech\n\n# Decorate your task class with @labtech.task:\n@labtech.task\nclass Experiment:\n    # Each Experiment task instance will take `base` and `power` parameters:\n    base: int\n    power: int\n\n    def run(self) -&gt; int:\n        # Define the task's run() method to return the result of the experiment:\n        labtech.logger.info(f'Raising {self.base} to the power of {self.power}')\n        sleep(1)\n        return self.base ** self.power\n\ndef main():\n    # Configure Experiment parameter permutations\n    experiments = [\n        Experiment(\n            base=base,\n            power=power,\n        )\n        for base in range(5)\n        for power in range(5)\n    ]\n\n    # Configure a Lab to run the experiments:\n    lab = labtech.Lab(\n        # Specify a directory to cache results in (running the experiments a second\n        # time will just load results from the cache!):\n        storage='demo_lab',\n        # Control the degree of parallelism:\n        max_workers=5,\n    )\n\n    # Run the experiments!\n    results = lab.run_tasks(experiments)\n    print([results[experiment] for experiment in experiments])\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Labtech can also produce graphical progress bars in Jupyter notebooks:</p> <p></p> <p>Tasks parameters can be any of the following types:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>, <code>dict</code>, <code>Enum</code></li> <li>Task types: A task parameter is a \"nested task\" that will be   executed before its parent so that it may make use of the nested   result.</li> </ul> <p>Here's an example of defining a single long-running task to produce a result for a large number of dependent tasks:</p> <pre><code>from time import sleep\n\nimport labtech\n\n@labtech.task\nclass SlowTask:\n    base: int\n\n    def run(self) -&gt; int:\n        sleep(5)\n        return self.base ** 2\n\n@labtech.task\nclass DependentTask:\n    slow_task: SlowTask\n    multiplier: int\n\n    def run(self) -&gt; int:\n        return self.multiplier * self.slow_task.result\n\ndef main():\n    some_slow_task = SlowTask(base=42)\n    dependent_tasks = [\n        DependentTask(\n            slow_task=some_slow_task,\n            multiplier=multiplier,\n        )\n        for multiplier in range(10)\n    ]\n\n    lab = labtech.Lab(storage='demo_lab')\n    results = lab.run_tasks(dependent_tasks)\n    print([results[task] for task in dependent_tasks])\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Labtech can even generate a Mermaid diagram to visualise your tasks:</p> <pre><code>from labtech.diagram import display_task_diagram\n\nsome_slow_task = SlowTask(base=42)\ndependent_tasks = [\n    DependentTask(\n        slow_task=some_slow_task,\n        multiplier=multiplier,\n    )\n    for multiplier in range(10)\n]\n\ndisplay_task_diagram(dependent_tasks)\n</code></pre> <pre><code>classDiagram\n    direction BT\n\n    class DependentTask\n    DependentTask : SlowTask slow_task\n    DependentTask : int multiplier\n    DependentTask : run() int\n\n    class SlowTask\n    SlowTask : int base\n    SlowTask : run() int\n\n\n    DependentTask &lt;-- SlowTask: slow_task</code></pre> <p>To learn more, dive into the following resources:</p> <ul> <li>The labtech tutorial (as an interactive notebook)</li> <li>Cookbook of common patterns (as an interactive notebook)</li> <li>API reference for Labs and Tasks</li> <li>More options for cache formats and storage providers</li> <li>Option for customising task execution backends</li> <li>Diagramming tools</li> <li>Distributing across multiple machines</li> <li>More examples</li> </ul>"},{"location":"#mypy-plugin","title":"Mypy Plugin","text":"<p>For mypy type-checking of classes decorated with <code>labtech.task</code>, simply enable the labtech mypy plugin in your <code>mypy.ini</code> file:</p> <pre><code>[mypy]\nplugins = labtech.mypy_plugin\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Install uv dependencies with <code>make deps</code></li> <li>Run linting, mypy, and tests with <code>make check</code></li> <li>Documentation:<ul> <li>Run local server: <code>make docs-serve</code></li> <li>Build docs: <code>make docs-build</code></li> <li>Deploy docs to GitHub Pages: <code>make docs-github</code></li> <li>Docstring style follows the Google style guide</li> </ul> </li> </ul>"},{"location":"caching/","title":"Caches and Storage","text":""},{"location":"caching/#caches","title":"Caches","text":"<p>You can control how the results of a particular task type should be formatted for caching by specifying an instance of one of the following Cache classes for the <code>cache</code> argument of the <code>labtech.task</code> decorator:</p>"},{"location":"caching/#labtech.cache.PickleCache","title":"<code>labtech.cache.PickleCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>Default cache that stores results as pickled Python objects.</p> <p>NOTE: As pickle is not secure, you should only load pickle cache results that you trust.</p> Source code in <code>labtech/cache.py</code> <pre><code>class PickleCache(BaseCache):\n    \"\"\"Default cache that stores results as\n    [pickled](https://docs.python.org/3/library/pickle.html) Python\n    objects.\n\n    NOTE: As pickle is not secure, you should only load pickle cache\n    results that you trust.\n\n    \"\"\"\n\n    KEY_PREFIX = 'pickle__'\n    RESULT_FILENAME = 'data.pickle'\n\n    def __init__(self, *, serializer: Serializer | None = None,\n                 pickle_protocol: int = pickle.HIGHEST_PROTOCOL):\n        super().__init__(serializer=serializer)\n        self.pickle_protocol = pickle_protocol\n\n    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n        data_file = storage.file_handle(task.cache_key, self.RESULT_FILENAME, mode='wb')\n        with data_file:\n            pickle.dump(result, data_file, protocol=self.pickle_protocol)\n\n    def load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n        data_file = storage.file_handle(task.cache_key, self.RESULT_FILENAME, mode='rb')\n        with data_file:\n            return pickle.load(data_file)\n</code></pre>"},{"location":"caching/#labtech.cache.NullCache","title":"<code>labtech.cache.NullCache</code>","text":"<p>               Bases: <code>Cache</code></p> <p>Cache that never stores results in the storage provider.</p> Source code in <code>labtech/cache.py</code> <pre><code>class NullCache(Cache):\n    \"\"\"Cache that never stores results in the storage provider.\"\"\"\n\n    def cache_key(self, task: Task) -&gt; str:\n        return 'null'\n\n    def is_cached(self, storage: Storage, task: Task) -&gt; bool:\n        return False\n\n    def save(self, storage: Storage, task: Task[ResultT], result: TaskResult[ResultT]):\n        pass\n\n    def load_task(self, storage: Storage, task_type: type[TaskT], key: str) -&gt; TaskT:\n        raise TaskNotFound\n\n    def load_result_with_meta(self, storage: Storage, task: Task[ResultT]) -&gt; TaskResult[ResultT]:\n        raise CacheError('Loading a result from a NullCache is not supported.')\n\n    def load_cache_timestamp(self, storage: Storage, task: Task) -&gt; Any:\n        raise CacheError('Loading a cache_timestamp from a NullCache is not supported.')\n\n    def delete(self, storage: Storage, task: Task):\n        pass\n</code></pre>"},{"location":"caching/#custom-caches","title":"Custom Caches","text":"<p>You can define your own type of Cache with its own format or behaviour by inheriting from <code>BaseCache</code>:</p>"},{"location":"caching/#labtech.cache.BaseCache","title":"<code>labtech.cache.BaseCache</code>","text":"<p>               Bases: <code>Cache</code></p> <p>Base class for defining a Cache that will store results in a storage provider.</p> Source code in <code>labtech/cache.py</code> <pre><code>class BaseCache(Cache):\n    \"\"\"Base class for defining a Cache that will store results in a\n    storage provider.\"\"\"\n\n    KEY_PREFIX = ''\n    \"\"\"Prefix for all files created by this Cache type - should be\n    different for each Cache type to avoid conflicts.\"\"\"\n\n    METADATA_FILENAME = 'metadata.json'\n\n    def __init__(self, *, serializer: Serializer | None = None):\n        self.serializer = serializer or Serializer()\n\n    def cache_key(self, task: Task) -&gt; str:\n        serialized_str = json.dumps(self.serializer.serialize_task(task)).encode('utf-8')\n        # Use sha1, as it is the same hash as git, produces short\n        # hashes, and security concerns with sha1 are not relevant to\n        # our use case.\n        hashed = hashlib.sha1(serialized_str).hexdigest()\n        return f'{self.KEY_PREFIX}{task.__class__.__qualname__}__{hashed}'\n\n    def is_cached(self, storage: Storage, task: Task) -&gt; bool:\n        return storage.exists(task.cache_key)\n\n    def save(self, storage: Storage, task: Task[ResultT], task_result: TaskResult[ResultT]):\n        start_timestamp = None\n        if task_result.meta.start is not None:\n            start_timestamp = task_result.meta.start.isoformat()\n\n        duration_seconds = None\n        if task_result.meta.duration is not None:\n            duration_seconds = task_result.meta.duration.total_seconds()\n\n        metadata = {\n            'labtech_version': labtech_version,\n            'cache': self.__class__.__qualname__,\n            'cache_key': task.cache_key,\n            'task': self.serializer.serialize_task(task),\n            'start_timestamp': start_timestamp,\n            'duration_seconds': duration_seconds,\n        }\n        metadata_file = storage.file_handle(task.cache_key, self.METADATA_FILENAME, mode='w')\n        with metadata_file:\n            json.dump(metadata, metadata_file, indent=2)\n        self.save_result(storage, task, task_result.value)\n\n    def load_metadata(self, storage: Storage, task_type: type[Task], key: str) -&gt; dict[str, Any]:\n        if not key.startswith(f'{self.KEY_PREFIX}{task_type.__qualname__}'):\n            raise TaskNotFound\n        with storage.file_handle(key, self.METADATA_FILENAME, mode='r') as metadata_file:\n            metadata = json.load(metadata_file)\n        if metadata.get('cache') != self.__class__.__qualname__:\n            raise TaskNotFound\n        return metadata\n\n    def build_result_meta(self, metadata: dict[str, Any]) -&gt; ResultMeta:\n        start = None\n        if 'start_timestamp' in metadata:\n            start = datetime.fromisoformat(metadata['start_timestamp'])\n\n        duration = None\n        if 'duration_seconds' in metadata:\n            duration = timedelta(seconds=metadata['duration_seconds'])\n\n        return ResultMeta(\n            start=start,\n            duration=duration,\n        )\n\n    def load_task(self, storage: Storage, task_type: type[TaskT], key: str) -&gt; TaskT:\n        metadata = self.load_metadata(storage, task_type, key)\n        result_meta = self.build_result_meta(metadata)\n        task = self.serializer.deserialize_task(metadata['task'], result_meta=result_meta)\n        if not isinstance(task, task_type):\n            raise TaskNotFound\n        return task\n\n    def load_result_with_meta(self, storage: Storage, task: Task[ResultT]) -&gt; TaskResult[ResultT]:\n        result = self.load_result(storage, task)\n        metadata = self.load_metadata(storage, type(task), task.cache_key)\n        return TaskResult(\n            value=result,\n            meta=self.build_result_meta(metadata),\n        )\n\n    def delete(self, storage: Storage, task: Task):\n        storage.delete(task.cache_key)\n\n    @abstractmethod\n    def load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n        \"\"\"Loads the result for the given task from the storage provider.\n\n        Args:\n            storage: Storage provider to load the result from\n            task: task instance to load the result for\n\n        \"\"\"\n\n    @abstractmethod\n    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n        \"\"\"Saves the given task result into the storage provider.\n\n        Args:\n            storage: Storage provider to save the result into\n            task: task instance the result belongs to\n            result: result to save\n\n        \"\"\"\n</code></pre>"},{"location":"caching/#labtech.cache.BaseCache.KEY_PREFIX","title":"<code>KEY_PREFIX = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prefix for all files created by this Cache type - should be different for each Cache type to avoid conflicts.</p>"},{"location":"caching/#labtech.cache.BaseCache.save_result","title":"<code>save_result(storage: Storage, task: Task[ResultT], result: ResultT)</code>  <code>abstractmethod</code>","text":"<p>Saves the given task result into the storage provider.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>Storage</code>)           \u2013            <p>Storage provider to save the result into</p> </li> <li> <code>task</code>               (<code>Task[ResultT]</code>)           \u2013            <p>task instance the result belongs to</p> </li> <li> <code>result</code>               (<code>ResultT</code>)           \u2013            <p>result to save</p> </li> </ul> Source code in <code>labtech/cache.py</code> <pre><code>@abstractmethod\ndef save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n    \"\"\"Saves the given task result into the storage provider.\n\n    Args:\n        storage: Storage provider to save the result into\n        task: task instance the result belongs to\n        result: result to save\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#labtech.cache.BaseCache.load_result","title":"<code>load_result(storage: Storage, task: Task[ResultT]) -&gt; ResultT</code>  <code>abstractmethod</code>","text":"<p>Loads the result for the given task from the storage provider.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>Storage</code>)           \u2013            <p>Storage provider to load the result from</p> </li> <li> <code>task</code>               (<code>Task[ResultT]</code>)           \u2013            <p>task instance to load the result for</p> </li> </ul> Source code in <code>labtech/cache.py</code> <pre><code>@abstractmethod\ndef load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n    \"\"\"Loads the result for the given task from the storage provider.\n\n    Args:\n        storage: Storage provider to load the result from\n        task: task instance to load the result for\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#storage","title":"Storage","text":"<p>You can set the storage location for caching task results by specifying an instance of one of the following Storage classes for the <code>storage</code> argument of your <code>Lab</code>:</p>"},{"location":"caching/#labtech.storage.LocalStorage","title":"<code>labtech.storage.LocalStorage</code>","text":"<p>               Bases: <code>Storage</code></p> <p>Storage provider that stores cached results in a local filesystem directory.</p> Source code in <code>labtech/storage.py</code> <pre><code>class LocalStorage(Storage):\n    \"\"\"Storage provider that stores cached results in a local filesystem\n    directory.\"\"\"\n\n    def __init__(self, storage_dir: str | Path, *, with_gitignore: bool = True):\n        \"\"\"\n        Args:\n            storage_dir: Path to the directory where cached results will be\n                stored. The directory will be created if it does not already\n                exist.\n            with_gitignore: If `True`, a `.gitignore` file will be created\n                inside the storage directory to ignore the entire storage\n                directory. If an existing `.gitignore` file exists, it will be\n                replaced.\n        \"\"\"\n        if isinstance(storage_dir, str):\n            storage_dir = Path(storage_dir)\n        self._storage_path = storage_dir.resolve()\n        if not self._storage_path.exists():\n            self._storage_path.mkdir()\n\n        if with_gitignore:\n            gitignore_path = self._storage_path / '.gitignore'\n            with gitignore_path.open('w') as gitignore_file:\n                gitignore_file.write('*\\n')\n\n    def _key_to_path(self, key: str) -&gt; Path:\n        validate_file_path_key(key, storage_path=self._storage_path)\n        return (self._storage_path / key).resolve()\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return sorted([\n            key_path.name for key_path in self._storage_path.iterdir()\n            if key_path.is_dir()\n        ])\n\n    def exists(self, key: str) -&gt; bool:\n        key_path = self._key_to_path(key)\n        return key_path.exists()\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        key_path = self._key_to_path(key)\n        try:\n            key_path.mkdir()\n        except FileExistsError:\n            pass\n        file_path = (key_path / filename).resolve()\n        if file_path.parent != key_path:\n            raise StorageError((f\"Filename '{filename}' should only reference a directory directly \"\n                                f\"under the storage key directory '{key_path}'\"))\n        return file_path.open(mode=mode)\n\n    def delete(self, key: str):\n        key_path = self._key_to_path(key)\n        if key_path.exists():\n            shutil.rmtree(key_path)\n</code></pre>"},{"location":"caching/#labtech.storage.LocalStorage.__init__","title":"<code>__init__(storage_dir: str | Path, *, with_gitignore: bool = True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>storage_dir</code>               (<code>str | Path</code>)           \u2013            <p>Path to the directory where cached results will be stored. The directory will be created if it does not already exist.</p> </li> <li> <code>with_gitignore</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, a <code>.gitignore</code> file will be created inside the storage directory to ignore the entire storage directory. If an existing <code>.gitignore</code> file exists, it will be replaced.</p> </li> </ul> Source code in <code>labtech/storage.py</code> <pre><code>def __init__(self, storage_dir: str | Path, *, with_gitignore: bool = True):\n    \"\"\"\n    Args:\n        storage_dir: Path to the directory where cached results will be\n            stored. The directory will be created if it does not already\n            exist.\n        with_gitignore: If `True`, a `.gitignore` file will be created\n            inside the storage directory to ignore the entire storage\n            directory. If an existing `.gitignore` file exists, it will be\n            replaced.\n    \"\"\"\n    if isinstance(storage_dir, str):\n        storage_dir = Path(storage_dir)\n    self._storage_path = storage_dir.resolve()\n    if not self._storage_path.exists():\n        self._storage_path.mkdir()\n\n    if with_gitignore:\n        gitignore_path = self._storage_path / '.gitignore'\n        with gitignore_path.open('w') as gitignore_file:\n            gitignore_file.write('*\\n')\n</code></pre>"},{"location":"caching/#labtech.storage.NullStorage","title":"<code>labtech.storage.NullStorage</code>","text":"<p>               Bases: <code>Storage</code></p> <p>Storage provider that does not store cached results.</p> Source code in <code>labtech/storage.py</code> <pre><code>class NullStorage(Storage):\n    \"\"\"Storage provider that does not store cached results.\"\"\"\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return []\n\n    def exists(self, key: str) -&gt; bool:\n        return False\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        return open(os.devnull, mode=mode)\n\n    def delete(self, key: str):\n        pass\n</code></pre>"},{"location":"caching/#custom-storage","title":"Custom Storage","text":"<p>To store cached results with an alternative storage provider (such as a storage bucket in the cloud), you can define your own type of Storage.</p> <p>Many cloud storage providers can be implemented by inheriting from <code>FsspecStorage</code> and defining an <code>fs_constructor()</code> method to return an <code>fsspec</code>-compatible filesystem:</p> <p>For other storage providers, inherit from <code>Storage</code>:</p>"},{"location":"caching/#labtech.storage.FsspecStorage","title":"<code>labtech.storage.FsspecStorage</code>","text":"<p>               Bases: <code>Storage</code>, <code>ABC</code></p> <p>Base class for using an <code>fsspec</code> filesystem for storage.</p> Source code in <code>labtech/storage.py</code> <pre><code>class FsspecStorage(Storage, ABC):\n    \"\"\"Base class for using an\n    [`fsspec`](https://filesystem-spec.readthedocs.io) filesystem for\n    storage.\"\"\"\n\n    def __init__(self, storage_dir: str | Path):\n        \"\"\"\n        Args:\n            storage_dir: Path to the directory where cached results will be\n                stored.\n        \"\"\"\n        if isinstance(storage_dir, str):\n            # Default to posix paths, which should work for most\n            # fsspec filesystems.\n            storage_dir = PosixPath(storage_dir)\n        self._storage_path = storage_dir\n        fs = self.fs_constructor()\n        fs.mkdirs(str(self._storage_path), exist_ok=True)\n\n    def _key_to_path(self, key):\n        validate_file_path_key(key, storage_path=self._storage_path)\n        return self._storage_path / key\n\n    def find_keys(self) -&gt; Sequence[str]:\n        fs = self.fs_constructor()\n        return [\n            str(Path(entry).relative_to(self._storage_path))\n            for entry in fs.ls(str(self._storage_path))\n        ]\n\n    def exists(self, key: str) -&gt; bool:\n        fs = self.fs_constructor()\n        return fs.exists(str(self._key_to_path(key)))\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        fs = self.fs_constructor()\n        key_path = self._key_to_path(key)\n        fs.mkdirs(str(key_path), exist_ok=True)\n        file_path = key_path / filename\n        if file_path.parent != key_path:\n            raise ValueError((f\"Filename '{filename}' should only reference a directory directly \"\n                              f\"under the storage key directory '{key_path}'\"))\n        return fs.open(str(file_path), mode)\n\n    def delete(self, key: str):\n        fs = self.fs_constructor()\n        path = self._key_to_path(key)\n        if fs.exists(str(path)):\n            fs.rm(str(path), recursive=True)\n\n    @abstractmethod\n    def fs_constructor(self):\n        \"\"\"Return an [`fsspec.AbstractFileSystem`](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.spec.AbstractFileSystem)\n        to use for file storage.\"\"\"\n        pass\n</code></pre>"},{"location":"caching/#labtech.storage.FsspecStorage.fs_constructor","title":"<code>fs_constructor()</code>  <code>abstractmethod</code>","text":"<p>Return an <code>fsspec.AbstractFileSystem</code> to use for file storage.</p> Source code in <code>labtech/storage.py</code> <pre><code>@abstractmethod\ndef fs_constructor(self):\n    \"\"\"Return an [`fsspec.AbstractFileSystem`](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.spec.AbstractFileSystem)\n    to use for file storage.\"\"\"\n    pass\n</code></pre>"},{"location":"caching/#labtech.storage.Storage","title":"<code>labtech.storage.Storage</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Storage provider for persisting cached task results.</p> Source code in <code>labtech/types.py</code> <pre><code>class Storage(ABC):\n    \"\"\"Storage provider for persisting cached task results.\"\"\"\n\n    @abstractmethod\n    def find_keys(self) -&gt; Sequence[str]:\n        \"\"\"Returns the keys of all currently cached task results.\"\"\"\n\n    @abstractmethod\n    def exists(self, key: str) -&gt; bool:\n        \"\"\"Returns `True` if the given task `key` is present in the storage\n        cache.\"\"\"\n\n    @abstractmethod\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        \"\"\"Opens and returns a File-like object for a single file within the\n        storage cache.\n\n        Args:\n            key: The task key of the cached result containing the file.\n            filename: The name of the file to open.\n            mode: The file mode to open the file with.\n\n        \"\"\"\n\n    @abstractmethod\n    def delete(self, key: str) -&gt; None:\n        \"\"\"Deletes the cached result for the task with the given `key`.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.find_keys","title":"<code>find_keys() -&gt; Sequence[str]</code>  <code>abstractmethod</code>","text":"<p>Returns the keys of all currently cached task results.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef find_keys(self) -&gt; Sequence[str]:\n    \"\"\"Returns the keys of all currently cached task results.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.exists","title":"<code>exists(key: str) -&gt; bool</code>  <code>abstractmethod</code>","text":"<p>Returns <code>True</code> if the given task <code>key</code> is present in the storage cache.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef exists(self, key: str) -&gt; bool:\n    \"\"\"Returns `True` if the given task `key` is present in the storage\n    cache.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.file_handle","title":"<code>file_handle(key: str, filename: str, *, mode: str = 'r') -&gt; IO</code>  <code>abstractmethod</code>","text":"<p>Opens and returns a File-like object for a single file within the storage cache.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The task key of the cached result containing the file.</p> </li> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>The name of the file to open.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r'</code> )           \u2013            <p>The file mode to open the file with.</p> </li> </ul> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n    \"\"\"Opens and returns a File-like object for a single file within the\n    storage cache.\n\n    Args:\n        key: The task key of the cached result containing the file.\n        filename: The name of the file to open.\n        mode: The file mode to open the file with.\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.delete","title":"<code>delete(key: str) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Deletes the cached result for the task with the given <code>key</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef delete(self, key: str) -&gt; None:\n    \"\"\"Deletes the cached result for the task with the given `key`.\"\"\"\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#labtech-cookbook","title":"Labtech Cookbook","text":"<p>The following cookbook presents labtech patterns for common use cases.</p> <p>You can also run this cookbook as an interactive notebook.</p> <pre><code>%pip install labtech fsspec mlflow pandas scikit-learn setuptools\n</code></pre> <pre><code>!mkdir storage\n</code></pre> <pre><code>import labtech\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.base import clone, ClassifierMixin\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare a dataset for examples\ndigits_X, digits_y = datasets.load_digits(return_X_y=True)\ndigits_X = StandardScaler().fit_transform(digits_X)\n</code></pre>"},{"location":"cookbook/#how-can-i-print-log-messages-from-my-task","title":"How can I print log messages from my task?","text":"<p>Using <code>labtech.logger</code> (a standard Python logger object) is the recommended approach for logging from a task, but all output that is sent to <code>STDOUT</code> (e.g. calls to <code>print()</code>) or <code>STDERR</code> (e.g. uncaught exceptions) will also be captured and logged:</p> <pre><code>@labtech.task\nclass PrintingExperiment:\n    seed: int\n\n    def run(self):\n        labtech.logger.warning(f'Warning, the seed is: {self.seed}')\n        print(f'The seed is: {self.seed}')\n        return self.seed * self.seed\n\n\nexperiments = [\n    PrintingExperiment(\n        seed=seed\n    )\n    for seed in range(5)\n]\nlab = labtech.Lab(storage=None)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#how-do-i-specify-a-complex-object-like-a-model-or-dataset-as-a-task-parameter","title":"How do I specify a complex object, like a model or dataset, as a task parameter?","text":"<p>Because labtech needs to be able to reconstitute <code>Task</code> objects from caches, task parameters can only be:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Any member of an <code>Enum</code> type.</li> <li>Task types: A task parameter is a \"nested task\" that will be executed   before its parent so that it may make use of the nested result.</li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>,   <code>dict</code>, <code>frozendict</code></li> <li>Note: Mutable <code>list</code> and <code>dict</code> collections will be converted to     immutable <code>tuple</code> and <code>frozendict</code>     collections.</li> </ul> <p>The are three primary patterns you can use to provide a more complex object as a parameter to a task:</p> <ul> <li>Constructing the object in a dependent task</li> <li>Passing the object in an <code>Enum</code> parameter</li> <li>Passing the object in the lab context</li> </ul>"},{"location":"cookbook/#constructing-objects-in-dependent-tasks","title":"Constructing objects in dependent tasks","text":"<p>If your object can be constructed from its own set of parameters, then you can use a dependent task as a \"factory\" to construct your object.</p> <p>For example, you could define a task type to construct a machine learning model (like <code>LRClassifierTask</code> below), and then make a task of that type a parameter for your primary experiment task:</p> <pre><code>from sklearn.linear_model import LogisticRegression\n\n\n# Constructing a classifier object is inexpensive, so we don't need to\n# cache the result\n@labtech.task(cache=None)\nclass LRClassifierTask:\n    random_state: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=self.random_state,\n        )\n\n\n@labtech.task\nclass ClassifierExperiment:\n    classifier_task: LRClassifierTask\n\n    def run(self) -&gt; np.ndarray:\n        # Because the classifier task result may be shared between experiments,\n        # we clone it before fitting.\n        clf = clone(self.classifier_task.result)\n        clf.fit(digits_X, digits_y)\n        return clf.predict_proba(digits_X)\n\n\nexperiment = ClassifierExperiment(\n    classifier_task=LRClassifierTask(random_state=42),\n)\nlab = labtech.Lab(storage=None)\nresults = lab.run_tasks([experiment])\n</code></pre> <p>We can extend this example with additional task types to cater for different types of classifiers with a Protocol that defines their common result type:</p> <pre><code>from typing import Protocol\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\n\nclass ClassifierTask(Protocol):\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(cache=None)\nclass LRClassifierTask:\n    random_state: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=self.random_state,\n        )\n\n\n@labtech.task(cache=None)\nclass NBClassifierTask:\n\n    def run(self) -&gt; ClassifierMixin:\n        return GaussianNB()\n\n\n@labtech.task\nclass ClassifierExperiment:\n    classifier_task: ClassifierTask\n\n    def run(self) -&gt; np.ndarray:\n        # Because the classifier task result may be shared between experiments,\n        # we clone it before fitting.\n        clf = clone(self.classifier_task.result)\n        clf.fit(digits_X, digits_y)\n        return clf.predict_proba(digits_X)\n\n\nclassifier_tasks = [\n    LRClassifierTask(random_state=42),\n    NBClassifierTask(),\n]\nexperiments = [\n    ClassifierExperiment(classifier_task=classifier_task)\n    for classifier_task in classifier_tasks\n]\nlab = labtech.Lab(storage=None)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#passing-objects-in-enum-parameters","title":"Passing objects in <code>Enum</code> parameters","text":"<p>For simple object parameters that have a fixed set of known values, an <code>Enum</code> of possible values can be used to provide parameter values.</p> <p>The following example shows how an <code>Enum</code> of functions can be used for a parameter to specify the operation that an experiment performs:</p> <p>Note: Because parameter values will be pickled when they are copied to parallel task sub-processes, the type used in a parameter <code>Enum</code> must support equality between identical (but distinct) object instances.</p> <pre><code>from enum import Enum\n\n\n# The custom class we want to provide objects of as parameters.\nclass Dataset:\n\n    def __init__(self, key):\n        self.key = key\n        self.data = datasets.fetch_openml(key, parser='auto').data\n\n    def __eq__(self, other):\n        # Support equality check to allow pickling of Enum values\n        if type(self) is not type(other):\n            return False\n        return self.key == other.key\n\n\nclass DatasetOption(Enum):\n    TIC_TAC_TOE=Dataset('tic-tac-toe')\n    EEG_EYE_STATE=Dataset('eeg-eye-state')\n\n\n@labtech.task\nclass DatasetExperiment:\n    dataset: DatasetOption\n\n    def run(self):\n        dataset = self.dataset.value\n        return dataset.data.shape\n\n\nexperiments = [\n    DatasetExperiment(\n        dataset=dataset\n    )\n    for dataset in DatasetOption\n]\nlab = labtech.Lab(storage=None)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#passing-objects-in-the-lab-context","title":"Passing objects in the lab context","text":"<p>If an object cannot be conveniently defined in an <code>Enum</code> (such as types like Numpy arrays or Pandas DataFrames that cannot be directly specified as an <code>Enum</code> value, or large values that cannot all be loaded into memory every time the <code>Enum</code> is loaded), then the lab context can be used to pass the object to a task.</p> <p>Warning: Because values provided in the lab context are not cached, they should be kept constant between runs or should not affect task results (e.g. parallel worker counts, log levels). If changing context values cause task results to change, then cached results may no longer be valid.</p> <p>The following example demonstrates specifying a <code>dataset_key</code> parameter to a task that is used to look up a dataset from the lab context:</p> <pre><code>DATASETS = {\n    'zeros': np.zeros((50, 10)),\n    'ones': np.ones((50, 10)),\n}\n\n\n@labtech.task\nclass SumExperiment:\n    dataset_key: str\n\n    def run(self):\n        dataset = self.context['DATASETS'][self.dataset_key]\n        return np.sum(dataset)\n\n\nexperiments = [\n    SumExperiment(\n        dataset_key=dataset_key\n    )\n    for dataset_key in DATASETS.keys()\n]\nlab = labtech.Lab(\n    storage=None,\n    context={\n        'DATASETS': DATASETS,\n    },\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#how-can-i-control-multi-processing-myself-within-a-task","title":"How can I control multi-processing myself within a task?","text":"<p>By default, Labtech executes tasks in parallel on all available CPU cores. However, you can control multi-processing yourself by disabling task parallelism and performing your own parallelism within a task's <code>run()</code> method.</p> <p>The following example uses <code>max_parallel</code> to allow only one <code>CVExperiment</code> to be executed at a time, and then performs cross-validation within the task using a number of workers specified in the lab context as <code>within_task_workers</code>:</p> <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\n\n\n@labtech.task(max_parallel=1)\nclass CVExperiment:\n    cv_folds: int\n\n    def run(self):\n        clf = GaussianNB()\n        return cross_val_score(\n            clf,\n            digits_X,\n            digits_y,\n            cv=self.cv_folds,\n            n_jobs=self.context['within_task_workers'],\n        )\n\n\nexperiments = [\n    CVExperiment(\n        cv_folds=cv_folds\n    )\n    for cv_folds in [5, 10]\n]\nlab = labtech.Lab(\n    storage=None,\n    context={\n        'within_task_workers': 4,\n    },\n)\nresults = lab.run_tasks(experiments)\n</code></pre> <p>Note: Instead of limiting parallelism for a single task type by specifying <code>max_parallel</code> in the <code>@labtech.task</code> decorator, you can limit parallelism across all tasks with <code>max_workers</code> when constructing a <code>labtech.Lab</code>.</p> <p>Note: The <code>joblib</code> library used by <code>sklearn</code> does not behave correctly when run from within a task sub-process, but setting <code>max_parallel=1</code> or <code>max_workers=1</code> ensures tasks are run inside the main process.</p>"},{"location":"cookbook/#how-can-i-make-labtech-continue-executing-tasks-even-when-one-or-more-fail","title":"How can I make labtech continue executing tasks even when one or more fail?","text":"<p>Labtech's default behaviour is to stop executing any new tasks as soon as any individual task fails. However, when executing tasks over a long period of time (e.g. a large number of tasks, or even a few long running tasks), it is sometimes helpful to have labtech continue to execute tasks even if one or more fail.</p> <p>If you set <code>continue_on_failure=True</code> when creating your lab, exceptions raised during the execution of a task will be logged, but the execution of other tasks will continue:</p> <pre><code>lab = labtech.Lab(\n    storage=None,\n    continue_on_failure=True,\n)\n</code></pre>"},{"location":"cookbook/#what-happens-to-my-cached-results-if-i-change-or-move-the-definition-of-a-task","title":"What happens to my cached results if I change or move the definition of a task?","text":"<p>A task's cache will store all details necessary to reinstantiate the task object, including the qualified name of the task's class and all of the task's parameters. Because of this, it is best not to change the parameters and location of a task's definition once you are seriously relying on cached results.</p> <p>If you need to add a new parameter or behaviour to an existing task type for which you have previously cached results, consider defining a sub-class for that extension so that you can continue using caches for the base class:</p> <pre><code>@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\n\n@labtech.task\nclass ExtendedExperiment(Experiment):\n    multiplier: int\n\n    def run(self):\n        base_result = super().run()\n        return base_result * self.multiplier\n</code></pre>"},{"location":"cookbook/#how-can-i-find-what-results-i-have-cached","title":"How can I find what results I have cached?","text":"<p>You can use the <code>cached_task()</code> method of a <code>Lab</code> instance to retrieve all cached task instances for a list of task types. You can then \"run\" the tasks to load their cached results:</p> <pre><code>cached_cvexperiment_tasks = lab.cached_tasks([CVExperiment])\nresults = lab.run_tasks(cached_cvexperiment_tasks)\n</code></pre>"},{"location":"cookbook/#how-can-i-clear-cached-results","title":"How can I clear cached results?","text":"<p>You can clear the cache for a list of tasks using the <code>uncache_tasks()</code> method of a <code>Lab</code> instance:</p> <pre><code>lab.uncache_tasks(cached_cvexperiment_tasks)\n</code></pre> <p>You can also ignore all previously cached results when running a list of tasks by passing the <code>bust_cache</code> option to <code>run_tasks()</code>:</p> <pre><code>lab.run_tasks(cached_cvexperiment_tasks, bust_cache=True)\n</code></pre>"},{"location":"cookbook/#will-labtech-ignore-previously-cached-results-if-i-change-the-implementation-of-a-task","title":"Will Labtech ignore previously cached results if I change the implementation of a task?","text":"<p>Whenever you make a change that will impact the behaviour of a task (i.e. most changes to the <code>run()</code> method or the code it depends on) you should add or updated the <code>code_version</code> in <code>@task</code>. For example:</p> <pre><code>@labtech.task(code_version='v2')\nclass Experiment:\n    ...\n</code></pre> <p>Labtech will re-run tasks if there are no cached results with a <code>code_version</code> matching your current code. If you don't update the <code>code_version</code> or otherwise clear your cache, then the returned cached results may no longer reflect the actual results of your current code.</p> <p>You may also like to save storage space by clearing up old cached results where the <code>code_version</code> does not match the <code>current_code_version</code>:</p> <pre><code>stale_cached_tasks = [\n    cached_task for cached_task in lab.cached_tasks([\n       # Make sure to include all task types to ensure you clear\n       # all intermediate results\n       Experiment,\n    ])\n    if cached_task.code_version != cached_task.current_code_version\n]\nlab.uncache_tasks(stale_cached_tasks)\n</code></pre>"},{"location":"cookbook/#what-types-of-values-should-my-tasks-return-to-be-cached","title":"What types of values should my tasks return to be cached?","text":"<p>While you can define your task to return any Python object you like to be cached, one generally useful approach is to return a dictionary comprised of built-in (e.g. lists, strings, numbers) or otherwise standard data types (e.g. arrays, dataframes). This is for two reasons:</p> <ol> <li>If the returned dictionary needs to be extended to include    additional keys, it will often be straightforward to adapt code    that uses task results to safely continue using previously cached    results that do not contain those keys.</li> <li>Using custom objects (such as dataclasses) could cause    issues when loading cached objects if the definition of the    class ever changes.</li> </ol> <p>If you want to keep the typing benefits of a custom dataclass, you can consider using a <code>TypeDict</code>:</p> <pre><code>from typing import TypedDict, NotRequired\n\n\nclass MyTaskResult(TypedDict):\n    predictions: np.ndarray\n    # Key added in a later version of the task. Requires Python &gt;= 3.11.\n    model_weights: NotRequired[np.ndarray]\n\n\n@labtech.task\nclass ExampleTask:\n    seed: int\n\n    def run(self):\n        return MyTaskResult(\n            predictions=np.array([1, 2, 3]),\n            model_weights=np.array([self.seed, self.seed ** 2]),\n        )\n</code></pre>"},{"location":"cookbook/#how-can-i-cache-task-results-in-a-format-other-than-pickle","title":"How can I cache task results in a format other than pickle?","text":"<p>You can define your own cache type to support storing cached results in a format other than pickle. To do so, you must define a class that extends <code>labtech.cache.BaseCache</code> and defines <code>KEY_PREFIX</code>, <code>save_result()</code>, and <code>load_result()</code>. You can then configure any task type by passing an instance of your new cache type for the <code>cache</code> option of the <code>@labtech.task</code> decorator.</p> <p>The following example demonstrates defining and using a custom cache type to store Pandas DataFrames as parquet files:</p> <pre><code>from labtech.cache import BaseCache\nfrom labtech.types import Task, ResultT\nfrom labtech.storage import Storage\nimport pandas as pd\n\n\nclass ParquetCache(BaseCache):\n    \"\"\"Caches a Pandas DataFrame result as a parquet file.\"\"\"\n    KEY_PREFIX = 'parquet__'\n\n    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n        if not isinstance(result, pd.DataFrame):\n            raise ValueError('ParquetCache can only cache DataFrames')\n        with storage.file_handle(task.cache_key, 'result.parquet', mode='wb') as data_file:\n            result.to_parquet(data_file)\n\n    def load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n        with storage.file_handle(task.cache_key, 'result.parquet', mode='rb') as data_file:\n            return pd.read_parquet(data_file)\n\n\n@labtech.task(cache=ParquetCache())\nclass TabularTask:\n\n    def run(self):\n        return pd.DataFrame({\n            'x': [1, 2, 3],\n            'y': [1, 4, 9],\n        })\n\n\nlab = labtech.Lab(storage='storage/parquet_example')\nlab.run_tasks([TabularTask()])\n</code></pre>"},{"location":"cookbook/#how-can-i-cache-task-results-somewhere-other-than-my-filesystem","title":"How can I cache task results somewhere other than my filesystem?","text":"<p>For any storage provider that has an <code>fsspec</code>-compatible implementation, you can define your own storage type that extends <code>labtech.storage.FsspecStorage</code>. You can then pass an instance of your new storage type for the <code>storage</code> option when constructing a <code>Lab</code> instance.</p> <p>The following example demonstrates constructing a storage type for an Amazon S3 bucket using the <code>s3fs</code> library. This example could be adapted for other <code>fsspec</code> implementations, such as cloud storage providers like Azure Blob Storage.</p> <pre><code>%pip install s3fs\n</code></pre> <pre><code>from labtech.storage import FsspecStorage\nfrom s3fs import S3FileSystem\n\n\nclass S3fsStorage(FsspecStorage):\n\n    def fs_constructor(self):\n        return S3FileSystem(\n            endpoint_url='...',\n            key='...',\n            secret='...',\n        )\n\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\n\nexperiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(100)\n]\nlab = labtech.Lab(\n    storage=S3fsStorage('my-s3-bucket/lab_directory'),\n    # s3fs does not support forked processes, so make sure we are spawning\n    # subprocesses: https://s3fs.readthedocs.io/en/latest/#multiprocessing\n    runner_backend='spawn',\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#what-if-there-is-no-fsspec-implementation-for-my-storage-provider","title":"What if there is no <code>fsspec</code> implementation for my storage provider?","text":"<p>You can also define your own storage type that extends <code>labtech.storage.Storage</code> and defines <code>find_keys()</code>, <code>exists()</code>, <code>file_handle()</code> and <code>delete()</code>. For an example, refer to the implementation of <code>labtech.storage.LocalStorage</code>.</p>"},{"location":"cookbook/#loading-lots-of-cached-results-is-slow-how-can-i-make-it-faster","title":"Loading lots of cached results is slow, how can I make it faster?","text":"<p>If you have a large number of tasks, you may find that the overhead of loading each individual task result from the cache is unacceptably slow when you need to frequently reload previous results for analysis.</p> <p>In such cases, you may find it helpful to create a final task that depends on all of your individual tasks and aggregates all of their results into a single cached result. Note that this final result cache will need to be rebuilt whenever any of its dependent tasks changes or new dependent tasks are added. Furthermore, this approach will require additional storage for the final cache in addition to the individual result caches.</p> <p>The following example demonstrates defining and using an <code>AggregationTask</code> to aggregate the results from many individual tasks to create an aggregated cache that can be loaded more efficiently:</p> <pre><code>from labtech.types import Task\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\n\n@labtech.task\nclass AggregationTask:\n    sub_tasks: list[Task]\n\n    def run(self):\n        return [\n            sub_task.result\n            for sub_task in self.sub_tasks\n        ]\n\n\nexperiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(1000)\n]\naggregation_task = AggregationTask(\n    sub_tasks=experiments,\n)\nlab = labtech.Lab(storage='storage/aggregation_lab')\nresult = lab.run_task(aggregation_task)\n</code></pre>"},{"location":"cookbook/#how-can-i-optimise-memory-usage-in-labtech","title":"How can I optimise memory usage in labtech?","text":"<p>If you are running a Lab with with <code>runner_backend='spawn'</code> (the default on macOS and Windows), Labtech duplicates the results of dependent tasks and the lab context into each task's process. Therefore, to reduce memory usage (and the computational cost of pickling and unpickling these values when copying them between processes), you should try to keep these values as small as possible. One way to achieve this is to define a <code>filter_context()</code> in order to only pass necessary parts of the context to each task.</p> <p>If you are running a Lab with with <code>runner_backend='fork'</code> (the default on Linux), then you can rely on Labtech to share results and context between task processes using shared memory.</p>"},{"location":"cookbook/#how-can-i-see-when-a-task-was-run-and-how-long-it-took-to-execute","title":"How can I see when a task was run and how long it took to execute?","text":"<p>Once a task has been executed (or loaded from cache), you can see when it was originally executed and how long it took to execute from the task's <code>.result_meta</code> attribute:</p> <pre><code>print(f'The task was executed at: {aggregation_task.result_meta.start}')\nprint(f'The task execution took: {aggregation_task.result_meta.duration}')\n</code></pre>"},{"location":"cookbook/#how-can-i-access-the-results-of-intermediatedependency-tasks","title":"How can I access the results of intermediate/dependency tasks?","text":"<p>To conserve memory, labtech's default behaviour is to unload the results of intermediate/dependency tasks once their directly dependent tasks have finished executing.</p> <p>A simple approach to access the results of an intermediate task may simply be to include it's results as part of the result of the task that depends on it - that way you only need to look at the results of the final task(s).</p> <p>Another approach is to include all of the intermediate tasks for which you wish to access the results for in the call to <code>run_tasks()</code>:</p> <pre><code>experiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(10)\n]\naggregation_task = AggregationTask(\n    sub_tasks=experiments,\n)\nlab = labtech.Lab(storage=None)\nresults = lab.run_tasks([\n    aggregation_task,\n    # Include intermediate tasks to access their results\n    *experiments,\n])\nprint([\n    results[experiment]\n    for experiment in experiments\n])\n</code></pre>"},{"location":"cookbook/#how-can-i-construct-a-multi-step-experiment-pipeline","title":"How can I construct a multi-step experiment pipeline?","text":"<p>Say you want to model a multi-step experiment pipeline, where <code>StepA</code> is run before <code>StepB</code>, which is run before <code>StepC</code>:</p> <pre><code>StepA -&gt; StepB -&gt; StepC\n</code></pre> <p>This is modeled in labtech by defining a task type for each step, and having each step depend on the result from the previous step:</p> <pre><code>@labtech.task\nclass StepA:\n    seed_a: int\n\n    def run(self):\n        return self.seed_a\n\n\n@labtech.task\nclass StepB:\n    task_a: StepA\n    seed_b: int\n\n    def run(self):\n        return self.task_a.result * self.seed_b\n\n\n@labtech.task\nclass StepC:\n    task_b: StepB\n    seed_c: int\n\n    def run(self):\n        return self.task_b.result * self.seed_c\n\n\ntask_a = StepA(\n    seed_a=2,\n)\ntask_b = StepB(\n    seed_b=3,\n    task_a=task_a,\n)\ntask_c = StepC(\n    seed_c=5,\n    task_b=task_b,\n)\n\nlab = labtech.Lab(storage=None)\nresult = lab.run_task(task_c)\nprint(result)\n</code></pre>"},{"location":"cookbook/#how-can-i-visualise-my-task-types-including-their-parameters-and-dependencies","title":"How can I visualise my task types, including their parameters and dependencies?","text":"<p><code>labtech.diagram.display_task_diagram()</code> can be used to display a Mermaid diagram of task types for a given list of tasks:</p> <pre><code>from labtech.diagram import display_task_diagram\n\ndisplay_task_diagram(\n    [task_c],\n    direction='RL',\n)\n</code></pre> <p><code>labtech.diagram.build_task_diagram()</code> can be similarly used to return the Mermaid syntax for the diagram.</p>"},{"location":"cookbook/#how-can-i-use-labtech-with-mlflow","title":"How can I use labtech with mlflow?","text":"<p>If you want to log a task type as an mlflow \"run\", simply add <code>mlflow_run=True</code> to the call to <code>@labtech.task()</code>, which will:</p> <ul> <li>Wrap each run of the task with <code>mlflow.start_run()</code></li> <li>Tag the run with <code>labtech_task_type</code> equal to the task class name</li> <li>Log all task parameters with <code>mlflow.log_param()</code></li> </ul> <p>The following example demonstrates using labtech with mlflow. Note that you can still make any configuration changes (such as <code>mlflow.set_experiment()</code>) before the tasks are run, and you can make additional tracking calls (such as <code>mlflow.log_metric()</code> or <code>mlflow.log_model()</code>) in the body of your task's <code>run()</code> method:</p> <pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\n@labtech.task(mlflow_run=True)\nclass MLRun:\n    penalty_norm: str | None\n\n    def run(self) -&gt; np.ndarray:\n        clf = LogisticRegression(penalty=self.penalty_norm)\n        clf.fit(digits_X, digits_y)\n\n        labels = clf.predict(digits_X)\n\n        train_accuracy = accuracy_score(digits_y, labels)\n        mlflow.log_metric('train_accuracy', train_accuracy)\n\n        mlflow.sklearn.log_model(\n            sk_model=clf,\n            artifact_path='digits_model',\n            input_example=digits_X,\n            registered_model_name='digits-model',\n        )\n\n        return labels\n\n\nruns = [\n    MLRun(\n        penalty_norm=penalty_norm,\n    )\n    for penalty_norm in [None, 'l2']\n]\n\nmlflow.set_experiment('example_labtech_experiment')\nlab = labtech.Lab(storage=None)\nresults = lab.run_tasks(runs)\n</code></pre> <p>Note: While the mlflow documentation recommends wrapping only your tracking code with <code>mlflow.start_run()</code>, labtech wraps the entire call to the <code>run()</code> method of your task in order to track execution times in mlflow.</p> <p>Note: Because mlflow logging will be performed from a separate process for each task, you must use an mlflow tracking backend that supports multiple simultaneous connections. Specifically, using an SQLite backend directly from multiple processes may result in database locking errors. Instead, consider using local files (the default used by mlflow), an SQL database that runs as a server (e.g. postgresql, mysql, or mssql), or running a local mlflow tracking server (which may itself connect to an sqlite database). For more details, see the mlflow backend documentation.</p>"},{"location":"cookbook/#why-do-i-see-the-following-error-an-attempt-has-been-made-to-start-a-new-process-before-the-current-process-has-finished","title":"Why do I see the following error: <code>An attempt has been made to start a new process before the current process has finished</code>?","text":"<p>When running a Lab with <code>runner_backend='spawn'</code> (the default on macOS and Windows), you will see the following error if you do not guard your experiment and lab creation and other non-definition code with <code>__name__ == '__main__'</code>:</p> <pre><code>RuntimeError:\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n</code></pre> <p>To avoid this error, it is recommended that you write all of your non-definition code for a Python script in a <code>main()</code> function, and then guard the call to <code>main()</code> with <code>__name__ == '__main__'</code>:</p> <pre><code>import labtech\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\ndef main():\n    experiments = [\n        Experiment(\n            seed=seed\n        )\n        for seed in range(1000)\n    ]\n    lab = labtech.Lab(storage='storage/guarded_lab')\n    result = lab.run_tasks(experiments)\n    print(result)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>For details, see Safe importing of main module.</p>"},{"location":"cookbook/#why-do-i-see-the-following-error-attributeerror-cant-get-attribute-your_task_class-on-module-__main__-built-in","title":"Why do I see the following error: <code>AttributeError: Can't get attribute 'YOUR_TASK_CLASS' on &lt;module '__main__' (built-in)&gt;</code>?","text":"<p>You will see this error (as part of a very long stack trace) when running a Lab with <code>runner_backend='spawn'</code> (the default on macOS and Windows) from an interactive Python shell.</p> <p>The solution to this error is to define all of your labtech <code>Task</code> types in a separate <code>.py</code> Python module file which you can import into your interactive shell session (e.g. <code>from my_module import MyTask</code>).</p> <p>The reason for this error is that \"spawned\" task subprocesses will not receive a copy the current state of your <code>__main__</code> module (which contains the variables you declare interactively in the Python shell, including task definitions). This error does not occur with <code>runner_backend='fork'</code> (the default on Linux) because forked subprocesses do receive the current state of all modules (including <code>__main__</code>) from the parent process.</p>"},{"location":"core/","title":"Labs and Tasks","text":"<p>In this document you'll find API reference documentation for configuring Labs and tasks. For a tutorial-style explanation, see the README.</p> <p>Any task type decorated with <code>labtech.task</code> will provide the following attributes and methods:</p>"},{"location":"core/#labtech.Lab","title":"<code>labtech.Lab</code>","text":"<p>Primary interface for configuring, running, and getting results of tasks.</p> <p>A Lab can be used to run tasks with <code>run_tasks()</code>.</p> <p>Previously cached tasks can be retrieved with <code>cached_tasks()</code>, and can then have their results loaded with <code>run_tasks()</code> or be removed from the cache storage with <code>uncache_tasks()</code>.</p> Source code in <code>labtech/lab.py</code> <pre><code>class Lab:\n    \"\"\"Primary interface for configuring, running, and getting results of tasks.\n\n    A Lab can be used to run tasks with [`run_tasks()`][labtech.Lab.run_tasks].\n\n    Previously cached tasks can be retrieved with\n    [`cached_tasks()`][labtech.Lab.cached_tasks], and can then have\n    their results loaded with [`run_tasks()`][labtech.Lab.run_tasks]\n    or be removed from the cache storage with\n    [`uncache_tasks()`][labtech.Lab.uncache_tasks].\n\n    \"\"\"\n\n    def __init__(self, *,\n                 storage: str | Path | None | Storage,\n                 continue_on_failure: bool = True,\n                 max_workers: int | None = None,\n                 notebook: bool | None = None,\n                 context: LabContext | None = None,\n                 runner_backend: str | RunnerBackend | None = None):\n        \"\"\"\n        Args:\n            storage: Where task results should be cached to. A string or\n                [`Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n                will be interpreted as the path to a local directory, `None`\n                will result in no caching. Any [Storage][labtech.types.Storage]\n                instance may also be specified.\n            continue_on_failure: If `True`, exceptions raised by tasks will be\n                logged, but execution of other tasks will continue.\n            max_workers: The maximum number of parallel worker processes for\n                running tasks. A sensible default will be determined by the\n                runner_backend (`'fork'` and `'spawn'` use the number of\n                processors on the machine given by `os.cpu_count()`, while\n                `'thread'` uses `os.cpu_count() + 4`).\n            notebook: Determines whether to use notebook-friendly graphical\n                progress bars. When set to `None` (the default), labtech will\n                detect whether the code is being run from an IPython notebook.\n            context: A dictionary of additional variables to make available to\n                tasks. The context will not be cached, so the values should not\n                affect results (e.g. parallelism factors) or should be kept\n                constant between runs (e.g. datasets).\n            runner_backend: Controls how tasks are run in parallel. It can\n                optionally be set to one of the following options:\n\n                * `'fork'`: Uses the\n                  [`ForkRunnerBackend`][labtech.runners.ForkRunnerBackend]\n                  to run each task in a forked subprocess. Memory use\n                  is reduced by sharing the context and dependency task\n                  results between tasks with memory inherited from the\n                  parent process. The default on platforms that support\n                  forked Python subprocesses when `max_workers &gt; 1`: Linux\n                  and other POSIX systems, but not macOS or Windows.\n                * `'spawn'`: Uses the\n                  [`SpawnRunnerBackend`][labtech.runners.SpawnRunnerBackend]\n                  to run each task in a spawned subprocess. The\n                  context and dependency task results are\n                  copied/duplicated into the memory of each\n                  subprocess. The default on macOS and Windows when\n                  `max_workers &gt; 1`.\n                * `'thread'`: Uses the\n                  [`ThreadRunnerBackend`][labtech.runners.ThreadRunnerBackend]\n                  to run each task in a separate Python thread. Because\n                  [Python threads do not execute in parallel](https://docs.python.org/3/glossary.html#term-global-interpreter-lock),\n                  this runner is best suited for running tasks that are\n                  constrained by non-blocking IO operations (e.g. web\n                  requests), or for running a single worker with live task\n                  monitoring. Memory use is reduced by sharing the same\n                  in-memory context and dependency task results across\n                  threads. The default when `max_workers = 1`.\n                * `'serial'`: Uses the\n                  [`SerialRunnerBackend`][labtech.runners.SerialRunnerBackend]\n                  to run each task serially in the main process and thread.\n                  The task monitor will only be updated in between tasks. Mainly\n                  useful when troubleshooting issues running tasks on different\n                  threads and processes.\n                * Any instance of a\n                  [`RunnerBackend`][labtech.types.RunnerBackend],\n                  allowing for custom task management implementations.\n\n                For details on the differences between `'fork'` and\n                `'spawn'` backends, see [the Python documentation on\n                `multiprocessing` start methods](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods).\n\n        \"\"\"\n        if isinstance(storage, str) or isinstance(storage, Path):\n            storage = LocalStorage(storage)\n        elif storage is None:\n            storage = NullStorage()\n        self._storage = storage\n        self.continue_on_failure = continue_on_failure\n        self.max_workers = max_workers\n        self.notebook = is_ipython() if notebook is None else notebook\n        if context is None:\n            context = {}\n        self.context = context\n        if runner_backend is None:\n            start_methods = get_supported_start_methods()\n            if self.max_workers == 1:\n                runner_backend = ThreadRunnerBackend()\n            elif 'fork' in start_methods:\n                runner_backend = ForkRunnerBackend()\n            elif 'spawn' in start_methods:\n                runner_backend = SpawnRunnerBackend()\n            else:\n                raise LabError(('Default \\'fork\\' and \\'spawn\\' multiprocessing runner '\n                                'backends are not supported on your system.'\n                                'Please specify a system-compatible runner_backend.'))\n        elif isinstance(runner_backend, str):\n            if runner_backend == 'fork':\n                runner_backend = ForkRunnerBackend()\n            elif runner_backend == 'spawn':\n                runner_backend = SpawnRunnerBackend()\n            elif runner_backend == 'serial':\n                runner_backend = SerialRunnerBackend()\n            elif runner_backend == 'thread':\n                runner_backend = ThreadRunnerBackend()\n            else:\n                raise LabError(f'Unrecognised runner_backend: {runner_backend}')\n        self.runner_backend = runner_backend\n\n    def run_tasks(self, tasks: Sequence[TaskT], *,\n                  bust_cache: bool = False,\n                  disable_progress: bool = False,\n                  disable_top: bool = False,\n                  top_format: str = '$name $status since $start_time CPU: $cpu MEM: $rss',\n                  top_sort: str = 'start_time',\n                  top_n: int = 10) -&gt; dict[TaskT, Any]:\n        \"\"\"Run the given tasks with as much process parallelism as possible.\n        Loads task results from the cache storage where possible and\n        caches results of executed tasks.\n\n        Any attribute of a task that is itself a task object is\n        considered a \"nested task\", and will be executed or loaded so\n        that it's result is made available to its parent task. If the\n        same task is nested inside multiple task objects, it will only\n        be executed/loaded once.\n\n        As well as returning the results, each task's result will be\n        assigned to a `result` attribute on the task itself.\n\n        Args:\n            tasks: The tasks to execute. Each should be an instance of a class\n                decorated with [`labtech.task`][labtech.task].\n            bust_cache: If `True`, no task results will be loaded from the\n                cache storage; all tasks will be re-executed.\n            disable_progress: If `True`, do not display a tqdm progress bar\n                tracking task execution.\n            disable_top: If `True`, do not display the list of top active tasks.\n            top_format: Format for each top active task. Follows the format\n                rules for\n                [template strings](https://docs.python.org/3/library/string.html#template-strings)\n                and may include any of the following attributes for\n                substitution:\n\n                * `name`: The task's name displayed in logs.\n                * `pid`: The task's primary process id.\n                * `status`: Whether the task is being run or loaded from cache.\n                * `start_time`: The time the task's primary process started.\n                * `children`: The number of child tasks of the task's primary\n                  process.\n                * `threads`: The number of active CPU threads for the task\n                  (including across any child processes).\n                * `cpu`: The CPU percentage (1 core = 100%) being used by the\n                  task (including across any child processes).\n                * `rss`: The resident set size (RSS) memory percentage being\n                  used by the task (including across any child processes). RSS\n                  is the primary measure of memory usage.\n                * `vms`: The virtual memory size (VMS) percentage being used by\n                  the task (including across any child processes).\n            top_sort: Sort order for the top active tasks. Can be any of the\n                attributes available for use in `top_format`. If the string is\n                preceded by a `-`, the sort order will be reversed. Defaults to\n                showing oldest tasks first.\n            top_n: The maximum number of top active tasks to display.\n\n        Returns:\n            A dictionary mapping each of the provided tasks to its\n                corresponding result.\n\n        \"\"\"\n        check_tasks(tasks)\n\n        for task in tasks:\n            if task.code_version != task.current_code_version:\n                raise LabError(\n                    (f'`{repr(task)}` cannot be run, as it has code_version={task.code_version!r} '\n                     f'while the current implementation of {task.__class__.__name__} has '\n                     f'code_version={task.current_code_version!r}. You should construct new '\n                     f'{task.__class__.__name__} tasks to run instead of running tasks loaded from cache.')\n                )\n\n\n        coordinator = TaskCoordinator(\n            self,\n            bust_cache=bust_cache,\n            disable_progress=disable_progress,\n            disable_top=disable_top,\n            top_format=top_format,\n            top_sort=top_sort,\n            top_n=top_n,\n        )\n        results = coordinator.run(tasks)\n\n        failed_tasks = {task for task in tasks if task not in results}\n        if failed_tasks:\n            raise LabError(f'Failed to complete {len(failed_tasks)} submitted task(s)')\n\n        # Return results in the same order as tasks\n        return {task: results[task] for task in tasks}\n\n    def run_task(self, task: Task[ResultT], **kwargs) -&gt; ResultT:\n        \"\"\"Run a single task and return its result. Supports the same keyword\n        arguments as `run_tasks`.\n\n        NOTE: If you have many tasks to run, you should use\n        `run_tasks` instead to parallelise their execution.\n\n        Returns:\n            The result of the given task.\n\n        \"\"\"\n        results = self.run_tasks([task], **kwargs)\n        return results[task]\n\n    def cached_tasks(self, task_types: Sequence[type[TaskT]]) -&gt; Sequence[TaskT]:\n        \"\"\"Returns all task instances present in the Lab's cache storage for\n        the given `task_types`, each of which should be a task class\n        decorated with [`labtech.task`][labtech.task].\n\n        Does not load task results from the cache storage, but they\n        can be loaded by calling\n        [`run_tasks()`][labtech.Lab.run_tasks] with the returned task\n        instances.\n\n        \"\"\"\n        check_task_types(task_types)\n        keys = self._storage.find_keys()\n        tasks = []\n        for key in keys:\n            for task_type in task_types:\n                try:\n                    task = task_type._lt.cache.load_task(self._storage, task_type, key)\n                except TaskNotFound:\n                    pass\n                else:\n                    tasks.append(task)\n                    break\n        return tasks\n\n    def is_cached(self, task: Task) -&gt; bool:\n        \"\"\"Checks if a result is present for given task in the Lab's cache\n        storage.\"\"\"\n        check_tasks([task])\n        return task._lt.cache.is_cached(self._storage, task)\n\n    def uncache_tasks(self, tasks: Sequence[Task]):\n        \"\"\"Removes cached results for the given tasks from the Lab's cache\n        storage.\"\"\"\n        check_tasks(tasks)\n        for task in tasks:\n            if self.is_cached(task):\n                task._lt.cache.delete(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.Lab.__init__","title":"<code>__init__(*, storage: str | Path | None | Storage, continue_on_failure: bool = True, max_workers: int | None = None, notebook: bool | None = None, context: LabContext | None = None, runner_backend: str | RunnerBackend | None = None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>str | Path | None | Storage</code>)           \u2013            <p>Where task results should be cached to. A string or <code>Path</code> will be interpreted as the path to a local directory, <code>None</code> will result in no caching. Any Storage instance may also be specified.</p> </li> <li> <code>continue_on_failure</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, exceptions raised by tasks will be logged, but execution of other tasks will continue.</p> </li> <li> <code>max_workers</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of parallel worker processes for running tasks. A sensible default will be determined by the runner_backend (<code>'fork'</code> and <code>'spawn'</code> use the number of processors on the machine given by <code>os.cpu_count()</code>, while <code>'thread'</code> uses <code>os.cpu_count() + 4</code>).</p> </li> <li> <code>notebook</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Determines whether to use notebook-friendly graphical progress bars. When set to <code>None</code> (the default), labtech will detect whether the code is being run from an IPython notebook.</p> </li> <li> <code>context</code>               (<code>LabContext | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of additional variables to make available to tasks. The context will not be cached, so the values should not affect results (e.g. parallelism factors) or should be kept constant between runs (e.g. datasets).</p> </li> <li> <code>runner_backend</code>               (<code>str | RunnerBackend | None</code>, default:                   <code>None</code> )           \u2013            <p>Controls how tasks are run in parallel. It can optionally be set to one of the following options:</p> <ul> <li><code>'fork'</code>: Uses the   <code>ForkRunnerBackend</code>   to run each task in a forked subprocess. Memory use   is reduced by sharing the context and dependency task   results between tasks with memory inherited from the   parent process. The default on platforms that support   forked Python subprocesses when <code>max_workers &gt; 1</code>: Linux   and other POSIX systems, but not macOS or Windows.</li> <li><code>'spawn'</code>: Uses the   <code>SpawnRunnerBackend</code>   to run each task in a spawned subprocess. The   context and dependency task results are   copied/duplicated into the memory of each   subprocess. The default on macOS and Windows when   <code>max_workers &gt; 1</code>.</li> <li><code>'thread'</code>: Uses the   <code>ThreadRunnerBackend</code>   to run each task in a separate Python thread. Because   Python threads do not execute in parallel,   this runner is best suited for running tasks that are   constrained by non-blocking IO operations (e.g. web   requests), or for running a single worker with live task   monitoring. Memory use is reduced by sharing the same   in-memory context and dependency task results across   threads. The default when <code>max_workers = 1</code>.</li> <li><code>'serial'</code>: Uses the   <code>SerialRunnerBackend</code>   to run each task serially in the main process and thread.   The task monitor will only be updated in between tasks. Mainly   useful when troubleshooting issues running tasks on different   threads and processes.</li> <li>Any instance of a   <code>RunnerBackend</code>,   allowing for custom task management implementations.</li> </ul> <p>For details on the differences between <code>'fork'</code> and <code>'spawn'</code> backends, see the Python documentation on <code>multiprocessing</code> start methods.</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def __init__(self, *,\n             storage: str | Path | None | Storage,\n             continue_on_failure: bool = True,\n             max_workers: int | None = None,\n             notebook: bool | None = None,\n             context: LabContext | None = None,\n             runner_backend: str | RunnerBackend | None = None):\n    \"\"\"\n    Args:\n        storage: Where task results should be cached to. A string or\n            [`Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n            will be interpreted as the path to a local directory, `None`\n            will result in no caching. Any [Storage][labtech.types.Storage]\n            instance may also be specified.\n        continue_on_failure: If `True`, exceptions raised by tasks will be\n            logged, but execution of other tasks will continue.\n        max_workers: The maximum number of parallel worker processes for\n            running tasks. A sensible default will be determined by the\n            runner_backend (`'fork'` and `'spawn'` use the number of\n            processors on the machine given by `os.cpu_count()`, while\n            `'thread'` uses `os.cpu_count() + 4`).\n        notebook: Determines whether to use notebook-friendly graphical\n            progress bars. When set to `None` (the default), labtech will\n            detect whether the code is being run from an IPython notebook.\n        context: A dictionary of additional variables to make available to\n            tasks. The context will not be cached, so the values should not\n            affect results (e.g. parallelism factors) or should be kept\n            constant between runs (e.g. datasets).\n        runner_backend: Controls how tasks are run in parallel. It can\n            optionally be set to one of the following options:\n\n            * `'fork'`: Uses the\n              [`ForkRunnerBackend`][labtech.runners.ForkRunnerBackend]\n              to run each task in a forked subprocess. Memory use\n              is reduced by sharing the context and dependency task\n              results between tasks with memory inherited from the\n              parent process. The default on platforms that support\n              forked Python subprocesses when `max_workers &gt; 1`: Linux\n              and other POSIX systems, but not macOS or Windows.\n            * `'spawn'`: Uses the\n              [`SpawnRunnerBackend`][labtech.runners.SpawnRunnerBackend]\n              to run each task in a spawned subprocess. The\n              context and dependency task results are\n              copied/duplicated into the memory of each\n              subprocess. The default on macOS and Windows when\n              `max_workers &gt; 1`.\n            * `'thread'`: Uses the\n              [`ThreadRunnerBackend`][labtech.runners.ThreadRunnerBackend]\n              to run each task in a separate Python thread. Because\n              [Python threads do not execute in parallel](https://docs.python.org/3/glossary.html#term-global-interpreter-lock),\n              this runner is best suited for running tasks that are\n              constrained by non-blocking IO operations (e.g. web\n              requests), or for running a single worker with live task\n              monitoring. Memory use is reduced by sharing the same\n              in-memory context and dependency task results across\n              threads. The default when `max_workers = 1`.\n            * `'serial'`: Uses the\n              [`SerialRunnerBackend`][labtech.runners.SerialRunnerBackend]\n              to run each task serially in the main process and thread.\n              The task monitor will only be updated in between tasks. Mainly\n              useful when troubleshooting issues running tasks on different\n              threads and processes.\n            * Any instance of a\n              [`RunnerBackend`][labtech.types.RunnerBackend],\n              allowing for custom task management implementations.\n\n            For details on the differences between `'fork'` and\n            `'spawn'` backends, see [the Python documentation on\n            `multiprocessing` start methods](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods).\n\n    \"\"\"\n    if isinstance(storage, str) or isinstance(storage, Path):\n        storage = LocalStorage(storage)\n    elif storage is None:\n        storage = NullStorage()\n    self._storage = storage\n    self.continue_on_failure = continue_on_failure\n    self.max_workers = max_workers\n    self.notebook = is_ipython() if notebook is None else notebook\n    if context is None:\n        context = {}\n    self.context = context\n    if runner_backend is None:\n        start_methods = get_supported_start_methods()\n        if self.max_workers == 1:\n            runner_backend = ThreadRunnerBackend()\n        elif 'fork' in start_methods:\n            runner_backend = ForkRunnerBackend()\n        elif 'spawn' in start_methods:\n            runner_backend = SpawnRunnerBackend()\n        else:\n            raise LabError(('Default \\'fork\\' and \\'spawn\\' multiprocessing runner '\n                            'backends are not supported on your system.'\n                            'Please specify a system-compatible runner_backend.'))\n    elif isinstance(runner_backend, str):\n        if runner_backend == 'fork':\n            runner_backend = ForkRunnerBackend()\n        elif runner_backend == 'spawn':\n            runner_backend = SpawnRunnerBackend()\n        elif runner_backend == 'serial':\n            runner_backend = SerialRunnerBackend()\n        elif runner_backend == 'thread':\n            runner_backend = ThreadRunnerBackend()\n        else:\n            raise LabError(f'Unrecognised runner_backend: {runner_backend}')\n    self.runner_backend = runner_backend\n</code></pre>"},{"location":"core/#labtech.Lab.run_tasks","title":"<code>run_tasks(tasks: Sequence[TaskT], *, bust_cache: bool = False, disable_progress: bool = False, disable_top: bool = False, top_format: str = '$name $status since $start_time CPU: $cpu MEM: $rss', top_sort: str = 'start_time', top_n: int = 10) -&gt; dict[TaskT, Any]</code>","text":"<p>Run the given tasks with as much process parallelism as possible. Loads task results from the cache storage where possible and caches results of executed tasks.</p> <p>Any attribute of a task that is itself a task object is considered a \"nested task\", and will be executed or loaded so that it's result is made available to its parent task. If the same task is nested inside multiple task objects, it will only be executed/loaded once.</p> <p>As well as returning the results, each task's result will be assigned to a <code>result</code> attribute on the task itself.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>               (<code>Sequence[TaskT]</code>)           \u2013            <p>The tasks to execute. Each should be an instance of a class decorated with <code>labtech.task</code>.</p> </li> <li> <code>bust_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, no task results will be loaded from the cache storage; all tasks will be re-executed.</p> </li> <li> <code>disable_progress</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, do not display a tqdm progress bar tracking task execution.</p> </li> <li> <code>disable_top</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, do not display the list of top active tasks.</p> </li> <li> <code>top_format</code>               (<code>str</code>, default:                   <code>'$name $status since $start_time CPU: $cpu MEM: $rss'</code> )           \u2013            <p>Format for each top active task. Follows the format rules for template strings and may include any of the following attributes for substitution:</p> <ul> <li><code>name</code>: The task's name displayed in logs.</li> <li><code>pid</code>: The task's primary process id.</li> <li><code>status</code>: Whether the task is being run or loaded from cache.</li> <li><code>start_time</code>: The time the task's primary process started.</li> <li><code>children</code>: The number of child tasks of the task's primary   process.</li> <li><code>threads</code>: The number of active CPU threads for the task   (including across any child processes).</li> <li><code>cpu</code>: The CPU percentage (1 core = 100%) being used by the   task (including across any child processes).</li> <li><code>rss</code>: The resident set size (RSS) memory percentage being   used by the task (including across any child processes). RSS   is the primary measure of memory usage.</li> <li><code>vms</code>: The virtual memory size (VMS) percentage being used by   the task (including across any child processes).</li> </ul> </li> <li> <code>top_sort</code>               (<code>str</code>, default:                   <code>'start_time'</code> )           \u2013            <p>Sort order for the top active tasks. Can be any of the attributes available for use in <code>top_format</code>. If the string is preceded by a <code>-</code>, the sort order will be reversed. Defaults to showing oldest tasks first.</p> </li> <li> <code>top_n</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum number of top active tasks to display.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[TaskT, Any]</code>           \u2013            <p>A dictionary mapping each of the provided tasks to its corresponding result.</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def run_tasks(self, tasks: Sequence[TaskT], *,\n              bust_cache: bool = False,\n              disable_progress: bool = False,\n              disable_top: bool = False,\n              top_format: str = '$name $status since $start_time CPU: $cpu MEM: $rss',\n              top_sort: str = 'start_time',\n              top_n: int = 10) -&gt; dict[TaskT, Any]:\n    \"\"\"Run the given tasks with as much process parallelism as possible.\n    Loads task results from the cache storage where possible and\n    caches results of executed tasks.\n\n    Any attribute of a task that is itself a task object is\n    considered a \"nested task\", and will be executed or loaded so\n    that it's result is made available to its parent task. If the\n    same task is nested inside multiple task objects, it will only\n    be executed/loaded once.\n\n    As well as returning the results, each task's result will be\n    assigned to a `result` attribute on the task itself.\n\n    Args:\n        tasks: The tasks to execute. Each should be an instance of a class\n            decorated with [`labtech.task`][labtech.task].\n        bust_cache: If `True`, no task results will be loaded from the\n            cache storage; all tasks will be re-executed.\n        disable_progress: If `True`, do not display a tqdm progress bar\n            tracking task execution.\n        disable_top: If `True`, do not display the list of top active tasks.\n        top_format: Format for each top active task. Follows the format\n            rules for\n            [template strings](https://docs.python.org/3/library/string.html#template-strings)\n            and may include any of the following attributes for\n            substitution:\n\n            * `name`: The task's name displayed in logs.\n            * `pid`: The task's primary process id.\n            * `status`: Whether the task is being run or loaded from cache.\n            * `start_time`: The time the task's primary process started.\n            * `children`: The number of child tasks of the task's primary\n              process.\n            * `threads`: The number of active CPU threads for the task\n              (including across any child processes).\n            * `cpu`: The CPU percentage (1 core = 100%) being used by the\n              task (including across any child processes).\n            * `rss`: The resident set size (RSS) memory percentage being\n              used by the task (including across any child processes). RSS\n              is the primary measure of memory usage.\n            * `vms`: The virtual memory size (VMS) percentage being used by\n              the task (including across any child processes).\n        top_sort: Sort order for the top active tasks. Can be any of the\n            attributes available for use in `top_format`. If the string is\n            preceded by a `-`, the sort order will be reversed. Defaults to\n            showing oldest tasks first.\n        top_n: The maximum number of top active tasks to display.\n\n    Returns:\n        A dictionary mapping each of the provided tasks to its\n            corresponding result.\n\n    \"\"\"\n    check_tasks(tasks)\n\n    for task in tasks:\n        if task.code_version != task.current_code_version:\n            raise LabError(\n                (f'`{repr(task)}` cannot be run, as it has code_version={task.code_version!r} '\n                 f'while the current implementation of {task.__class__.__name__} has '\n                 f'code_version={task.current_code_version!r}. You should construct new '\n                 f'{task.__class__.__name__} tasks to run instead of running tasks loaded from cache.')\n            )\n\n\n    coordinator = TaskCoordinator(\n        self,\n        bust_cache=bust_cache,\n        disable_progress=disable_progress,\n        disable_top=disable_top,\n        top_format=top_format,\n        top_sort=top_sort,\n        top_n=top_n,\n    )\n    results = coordinator.run(tasks)\n\n    failed_tasks = {task for task in tasks if task not in results}\n    if failed_tasks:\n        raise LabError(f'Failed to complete {len(failed_tasks)} submitted task(s)')\n\n    # Return results in the same order as tasks\n    return {task: results[task] for task in tasks}\n</code></pre>"},{"location":"core/#labtech.Lab.run_task","title":"<code>run_task(task: Task[ResultT], **kwargs) -&gt; ResultT</code>","text":"<p>Run a single task and return its result. Supports the same keyword arguments as <code>run_tasks</code>.</p> <p>NOTE: If you have many tasks to run, you should use <code>run_tasks</code> instead to parallelise their execution.</p> <p>Returns:</p> <ul> <li> <code>ResultT</code>           \u2013            <p>The result of the given task.</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def run_task(self, task: Task[ResultT], **kwargs) -&gt; ResultT:\n    \"\"\"Run a single task and return its result. Supports the same keyword\n    arguments as `run_tasks`.\n\n    NOTE: If you have many tasks to run, you should use\n    `run_tasks` instead to parallelise their execution.\n\n    Returns:\n        The result of the given task.\n\n    \"\"\"\n    results = self.run_tasks([task], **kwargs)\n    return results[task]\n</code></pre>"},{"location":"core/#labtech.Lab.cached_tasks","title":"<code>cached_tasks(task_types: Sequence[type[TaskT]]) -&gt; Sequence[TaskT]</code>","text":"<p>Returns all task instances present in the Lab's cache storage for the given <code>task_types</code>, each of which should be a task class decorated with <code>labtech.task</code>.</p> <p>Does not load task results from the cache storage, but they can be loaded by calling <code>run_tasks()</code> with the returned task instances.</p> Source code in <code>labtech/lab.py</code> <pre><code>def cached_tasks(self, task_types: Sequence[type[TaskT]]) -&gt; Sequence[TaskT]:\n    \"\"\"Returns all task instances present in the Lab's cache storage for\n    the given `task_types`, each of which should be a task class\n    decorated with [`labtech.task`][labtech.task].\n\n    Does not load task results from the cache storage, but they\n    can be loaded by calling\n    [`run_tasks()`][labtech.Lab.run_tasks] with the returned task\n    instances.\n\n    \"\"\"\n    check_task_types(task_types)\n    keys = self._storage.find_keys()\n    tasks = []\n    for key in keys:\n        for task_type in task_types:\n            try:\n                task = task_type._lt.cache.load_task(self._storage, task_type, key)\n            except TaskNotFound:\n                pass\n            else:\n                tasks.append(task)\n                break\n    return tasks\n</code></pre>"},{"location":"core/#labtech.Lab.is_cached","title":"<code>is_cached(task: Task) -&gt; bool</code>","text":"<p>Checks if a result is present for given task in the Lab's cache storage.</p> Source code in <code>labtech/lab.py</code> <pre><code>def is_cached(self, task: Task) -&gt; bool:\n    \"\"\"Checks if a result is present for given task in the Lab's cache\n    storage.\"\"\"\n    check_tasks([task])\n    return task._lt.cache.is_cached(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.Lab.uncache_tasks","title":"<code>uncache_tasks(tasks: Sequence[Task])</code>","text":"<p>Removes cached results for the given tasks from the Lab's cache storage.</p> Source code in <code>labtech/lab.py</code> <pre><code>def uncache_tasks(self, tasks: Sequence[Task]):\n    \"\"\"Removes cached results for the given tasks from the Lab's cache\n    storage.\"\"\"\n    check_tasks(tasks)\n    for task in tasks:\n        if self.is_cached(task):\n            task._lt.cache.delete(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.task","title":"<code>labtech.task(*args, code_version: str | None = None, cache: CacheDefault | None | Cache = CACHE_DEFAULT, max_parallel: int | None = None, mlflow_run: bool = False)</code>","text":"<p>Class decorator for defining task type classes.</p> <p>Task types are frozen [<code>dataclasses</code>], and attribute definitions should capture all parameters of the task type. Parameter attributes can be any of the following types:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Any member of an <code>Enum</code> type. Referring to members of an <code>Enum</code> can be   used to parameterise a task with a value that does not have one of the   types above (e.g. a Pandas/Numpy dataset).</li> <li>Task types: A task parameter is a \"nested task\" that will be executed   before its parent so that it may make use of the nested result.</li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>,   <code>dict</code>, <code>frozendict</code></li> <li>Dictionaries may only contain string keys.</li> <li>Note: Mutable <code>list</code> and <code>dict</code> collections will be converted to     immutable <code>tuple</code> and <code>frozendict</code>     collections.</li> </ul> <p>The task type is expected to define a <code>run()</code> method that takes no arguments (other than <code>self</code>). The <code>run()</code> method should execute the task as parameterised by the task's attributes and return the result of the task.</p> <p>Example usage:</p> <pre><code>@labtech.task\nclass Experiment:\n    seed: int\n    multiplier: int\n\n    def run(self):\n        return self.seed * self.multiplier.value\n\nexperiment = Experiment(seed=1, multiplier=2)\n</code></pre> <p>You can also provide arguments to the decorator to control caching, cache-busting versioning, parallelism, and mlflow tracking:</p> <pre><code>@labtech.task(cache=None, code_version='v1', max_parallel=1, mlflow_run=True)\nclass Experiment:\n    ...\n\n    def run(self):\n        ...\n</code></pre> <p>If a <code>post_init(self)</code> method is defined, it will be called after the task object is initialised (analagously to the <code>__post_init__</code> method of a dataclass). Because task types are frozen dataclasses, attributes can only be assigned to the task with <code>object.__setattr__(self, attribute_name, attribute_value)</code>.</p> <p>If a <code>filter_context(self, context: LabContext) -&gt; LabContext</code> method is defined, it will be called to transform the context provided to each task. This can be useful for selecting subsets of large contexts in order to reduce data transferred to non-forked subprocesses or other kinds of processes in parallel processing frameworks. The filtering may take into account the values of the task's attributes. If <code>filter_context()</code> is not defined, the full context will be provided to each task.</p> <p>A <code>runner_options(self) -&gt; dict[str, Any]</code> method may be defined to provide a dictionary of options to further control the behaviour of specific types of runner backend - refer to the documentation of each runner backend for supported options. The implementation may make use of the task's parameter values.</p> <p>Parameters:</p> <ul> <li> <code>cache</code>               (<code>CacheDefault | None | Cache</code>, default:                   <code>CACHE_DEFAULT</code> )           \u2013            <p>The Cache that controls how task results are formatted for caching. Can be set to an instance of any <code>Cache</code> class, or <code>None</code> to disable caching of this type of task. Defaults to a <code>PickleCache</code>.</p> </li> <li> <code>code_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional identifier for the version of task's implementation. Task results will only be loaded from the cache where they have a matching code_version, so you should change the code_version whenever the definition of the task's <code>run</code> method or any code it depends on changes in a way that will impact the result. Any string value can be used, e.g. 'v1', '2025-04-22', etc.</p> </li> <li> <code>max_parallel</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of instances of this task type that are allowed to run simultaneously in separate sub-processes. Useful to set if running too many instances of this particular task simultaneously will exhaust system memory or processing resources.</p> </li> <li> <code>mlflow_run</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, the execution of each instance of this task type will be wrapped with <code>mlflow.start_run()</code>, tags the run with <code>labtech_task_type</code> equal to the task class name, and all parameters will be logged with <code>mlflow.log_param()</code>. You can make additional mlflow logging calls from the task's <code>run()</code> method.</p> </li> </ul> Source code in <code>labtech/tasks.py</code> <pre><code>def task(*args,\n         code_version: str | None = None,\n         cache: CacheDefault | None | Cache = CACHE_DEFAULT,\n         max_parallel: int | None = None,\n         mlflow_run: bool = False):\n    \"\"\"Class decorator for defining task type classes.\n\n    Task types are frozen [`dataclasses`], and attribute definitions\n    should capture all parameters of the task type. Parameter\n    attributes can be any of the following types:\n\n    * Simple scalar types: `str`, `bool`, `float`, `int`, `None`\n    * Any member of an `Enum` type. Referring to members of an `Enum` can be\n      used to parameterise a task with a value that does not have one of the\n      types above (e.g. a Pandas/Numpy dataset).\n    * Task types: A task parameter is a \"nested task\" that will be executed\n      before its parent so that it may make use of the nested result.\n    * Collections of any of these types: `list`, `tuple`,\n      `dict`, [`frozendict`](https://pypi.org/project/frozendict/)\n      * Dictionaries may only contain string keys.\n      * Note: Mutable `list` and `dict` collections will be converted to\n        immutable `tuple` and [`frozendict`](https://pypi.org/project/frozendict/)\n        collections.\n\n    The task type is expected to define a `run()` method that takes no\n    arguments (other than `self`). The `run()` method should execute\n    the task as parameterised by the task's attributes and return the\n    result of the task.\n\n    Example usage:\n\n    ```python\n    @labtech.task\n    class Experiment:\n        seed: int\n        multiplier: int\n\n        def run(self):\n            return self.seed * self.multiplier.value\n\n    experiment = Experiment(seed=1, multiplier=2)\n    ```\n\n    You can also provide arguments to the decorator to control\n    caching, cache-busting versioning, parallelism, and\n    [mlflow](https://mlflow.org/docs/latest/tracking.html#quickstart)\n    tracking:\n\n    ```python\n    @labtech.task(cache=None, code_version='v1', max_parallel=1, mlflow_run=True)\n    class Experiment:\n        ...\n\n        def run(self):\n            ...\n    ```\n\n    If a `post_init(self)` method is defined, it will be called after\n    the task object is initialised (analagously to the `__post_init__`\n    method of a dataclass). Because task types are frozen dataclasses,\n    attributes can only be assigned to the task with\n    `object.__setattr__(self, attribute_name, attribute_value)`.\n\n    If a `filter_context(self, context: LabContext) -&gt; LabContext`\n    method is defined, it will be called to transform the context\n    provided to each task. This can be useful for selecting subsets of\n    large contexts in order to reduce data transferred to non-forked\n    subprocesses or other kinds of processes in parallel processing\n    frameworks. The filtering may take into account the values of the\n    task's attributes. If `filter_context()` is not defined, the full\n    context will be provided to each task.\n\n    A `runner_options(self) -&gt; dict[str, Any]` method may be defined\n    to provide a dictionary of options to further control the\n    behaviour of specific types of runner backend - refer to the\n    documentation of each runner backend for supported options. The\n    implementation may make use of the task's parameter values.\n\n    Args:\n        cache: The Cache that controls how task results are formatted for\n            caching. Can be set to an instance of any\n            [`Cache`](caching.md#caches) class, or `None` to disable caching\n            of this type of task. Defaults to a\n            [`PickleCache`][labtech.cache.PickleCache].\n        code_version: Optional identifier for the version of task's\n            implementation. Task results will only be loaded from the\n            cache where they have a matching code_version, so you\n            should change the code_version whenever the definition of\n            the task's `run` method or any code it depends on changes\n            in a way that will impact the result. Any string value can\n            be used, e.g. 'v1', '2025-04-22', etc.\n        max_parallel: The maximum number of instances of this task type that\n            are allowed to run simultaneously in separate sub-processes. Useful\n            to set if running too many instances of this particular task\n            simultaneously will exhaust system memory or processing resources.\n        mlflow_run: If True, the execution of each instance of this task type\n            will be wrapped with `mlflow.start_run()`, tags the run with\n            `labtech_task_type` equal to the task class name, and all parameters\n            will be logged with `mlflow.log_param()`. You can make additional\n            mlflow logging calls from the task's `run()` method.\n\n    \"\"\"\n\n    def decorator(cls):\n        nonlocal cache\n\n        if not is_task_type(cls):\n            for reserved_attr in _RESERVED_ATTRS:\n                if hasattr(cls, reserved_attr):\n                    raise AttributeError(f\"Task type already defines reserved attribute '{reserved_attr}'.\")\n\n        post_init = getattr(cls, 'post_init', None)\n        cls.__post_init__ = _task_post_init\n\n        cls = dataclass(frozen=True, eq=True, order=True)(cls)\n\n        run_func = getattr(cls, 'run', None)\n        if not callable(run_func):\n            raise NotImplementedError(f\"Task type '{cls.__name__}' must define a 'run' method\")\n\n        if cache is CACHE_DEFAULT:\n            cache = PickleCache()\n        elif cache is None:\n            cache = NullCache()\n\n        cls._lt = TaskInfo(\n            cache=cast('Cache', cache),\n            orig_post_init=post_init,\n            max_parallel=max_parallel,\n            mlflow_run=mlflow_run,\n            current_code_version=code_version,\n        )\n        cls.__getstate__ = _task__getstate__\n        cls.__setstate__ = _task__setstate__\n        cls._set_results_map = _task_set_results_map\n        cls._set_result_meta = _task_set_result_meta\n        cls._set_code_version = _task_set_code_version\n        cls.current_code_version = property(_task_current_code_version)\n        cls.cache_key = property(_task_cache_key)\n        cls.result = property(_task_result)\n        cls.set_context = _task_set_context\n        if not hasattr(cls, 'filter_context'):\n            cls.filter_context = _task_filter_context_default\n        if not hasattr(cls, 'runner_options'):\n            cls.runner_options = _task_runner_options_default\n        return cls\n\n    if len(args) &gt; 0 and isclass(args[0]):\n        return decorator(args[0], *args[1:])\n    else:\n        return decorator\n</code></pre>"},{"location":"core/#labtech.types.Task","title":"<code>labtech.types.Task</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[CovariantResultT]</code></p> <p>Interface provided by any class that is decorated by <code>labtech.task</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>@dataclass\nclass Task(Protocol, Generic[CovariantResultT]):\n    \"\"\"Interface provided by any class that is decorated by\n    [`labtech.task`][labtech.task].\"\"\"\n    _lt: TaskInfo\n    _is_task: Literal[True]\n    _results_map: ResultsMap | None\n    _cache_key: str | None\n    context: LabContext | None\n    \"\"\"Context variables from the Lab that can be accessed when the task is running.\"\"\"\n    result_meta: ResultMeta | None\n    \"\"\"Metadata about the execution of the task.\"\"\"\n    code_version: str | None\n    \"\"\"Identifier for the version of task's implementation. If this task was\n    loaded from cache, it may have a different value to that currently specified\n    in the decorator.\"\"\"\n\n    def _set_results_map(self, results_map: ResultsMap):\n        pass\n\n    def _set_result_meta(self, result_meta: ResultMeta):\n        pass\n\n    @property\n    def current_code_version(self) -&gt; str | None:\n        \"\"\"Identifier for the current version of task's implementation\n        as specified in the [`labtech.task`][labtech.task] decorator.\"\"\"\n\n    @property\n    def cache_key(self) -&gt; str:\n        \"\"\"The key that uniquely identifies the location for this task\n        within cache storage.\"\"\"\n\n    @property\n    def result(self) -&gt; CovariantResultT:\n        \"\"\"The result executed/loaded for this task. If no result is\n        available in memory, accessing this property raises a `TaskError`.\"\"\"\n\n    def set_context(self, context: LabContext):\n        \"\"\"Set the context that is made available to the task while it is\n        running.\"\"\"\n\n    def runner_options(self) -&gt; dict[str, Any]:\n        \"\"\"User-overridable method to a dictionary of options to\n        further control the behaviour of specific types of runner\n        backend - refer to the documentation of each runner backend\n        for supported options. The implementation may make use of the\n        task's parameter values.\n\n        \"\"\"\n\n    def filter_context(self, context: LabContext) -&gt; LabContext:\n        \"\"\"User-overridable method to filter/transform the context to\n        be provided to the task. The default implementation provides\n        the full context to the task. The filtering may take into\n        account the values of the task's attributes.\n\n        This can be useful for selecting subsets of large contexts in\n        order to reduce data transferred to non-forked subprocesses or\n        other kinds of processes in parallel processing frameworks.\n\n        \"\"\"\n\n    def run(self):\n        \"\"\"User-provided method that executes the task parameterised by the\n        attributes of the task.\n\n        Usually executed by [`Lab.run_tasks()`][labtech.Lab.run_tasks]\n        instead of being called directly.\n\n        \"\"\"\n\n    def __hash__(self) -&gt; int: ...\n</code></pre>"},{"location":"core/#labtech.types.Task.context","title":"<code>context: LabContext | None</code>  <code>instance-attribute</code>","text":"<p>Context variables from the Lab that can be accessed when the task is running.</p>"},{"location":"core/#labtech.types.Task.result_meta","title":"<code>result_meta: ResultMeta | None</code>  <code>instance-attribute</code>","text":"<p>Metadata about the execution of the task.</p>"},{"location":"core/#labtech.types.Task.code_version","title":"<code>code_version: str | None</code>  <code>instance-attribute</code>","text":"<p>Identifier for the version of task's implementation. If this task was loaded from cache, it may have a different value to that currently specified in the decorator.</p>"},{"location":"core/#labtech.types.Task.current_code_version","title":"<code>current_code_version: str | None</code>  <code>property</code>","text":"<p>Identifier for the current version of task's implementation as specified in the <code>labtech.task</code> decorator.</p>"},{"location":"core/#labtech.types.Task.cache_key","title":"<code>cache_key: str</code>  <code>property</code>","text":"<p>The key that uniquely identifies the location for this task within cache storage.</p>"},{"location":"core/#labtech.types.Task.result","title":"<code>result: CovariantResultT</code>  <code>property</code>","text":"<p>The result executed/loaded for this task. If no result is available in memory, accessing this property raises a <code>TaskError</code>.</p>"},{"location":"core/#labtech.types.Task.set_context","title":"<code>set_context(context: LabContext)</code>","text":"<p>Set the context that is made available to the task while it is running.</p> Source code in <code>labtech/types.py</code> <pre><code>def set_context(self, context: LabContext):\n    \"\"\"Set the context that is made available to the task while it is\n    running.\"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.runner_options","title":"<code>runner_options() -&gt; dict[str, Any]</code>","text":"<p>User-overridable method to a dictionary of options to further control the behaviour of specific types of runner backend - refer to the documentation of each runner backend for supported options. The implementation may make use of the task's parameter values.</p> Source code in <code>labtech/types.py</code> <pre><code>def runner_options(self) -&gt; dict[str, Any]:\n    \"\"\"User-overridable method to a dictionary of options to\n    further control the behaviour of specific types of runner\n    backend - refer to the documentation of each runner backend\n    for supported options. The implementation may make use of the\n    task's parameter values.\n\n    \"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.filter_context","title":"<code>filter_context(context: LabContext) -&gt; LabContext</code>","text":"<p>User-overridable method to filter/transform the context to be provided to the task. The default implementation provides the full context to the task. The filtering may take into account the values of the task's attributes.</p> <p>This can be useful for selecting subsets of large contexts in order to reduce data transferred to non-forked subprocesses or other kinds of processes in parallel processing frameworks.</p> Source code in <code>labtech/types.py</code> <pre><code>def filter_context(self, context: LabContext) -&gt; LabContext:\n    \"\"\"User-overridable method to filter/transform the context to\n    be provided to the task. The default implementation provides\n    the full context to the task. The filtering may take into\n    account the values of the task's attributes.\n\n    This can be useful for selecting subsets of large contexts in\n    order to reduce data transferred to non-forked subprocesses or\n    other kinds of processes in parallel processing frameworks.\n\n    \"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.run","title":"<code>run()</code>","text":"<p>User-provided method that executes the task parameterised by the attributes of the task.</p> <p>Usually executed by <code>Lab.run_tasks()</code> instead of being called directly.</p> Source code in <code>labtech/types.py</code> <pre><code>def run(self):\n    \"\"\"User-provided method that executes the task parameterised by the\n    attributes of the task.\n\n    Usually executed by [`Lab.run_tasks()`][labtech.Lab.run_tasks]\n    instead of being called directly.\n\n    \"\"\"\n</code></pre>"},{"location":"core/#labtech.types.ResultT","title":"<code>labtech.types.ResultT = TypeVar('ResultT')</code>  <code>module-attribute</code>","text":"<p>Type variable for result returned by the <code>run</code> method of a <code>Task</code>.</p>"},{"location":"core/#labtech.types.ResultMeta","title":"<code>labtech.types.ResultMeta</code>  <code>dataclass</code>","text":"<p>Metadata about the execution of a task. If the task is loaded from cache, the metadata is also loaded from the cache.</p> Source code in <code>labtech/types.py</code> <pre><code>@dataclass(frozen=True)\nclass ResultMeta:\n    \"\"\"Metadata about the execution of a task. If the task is loaded from\n    cache, the metadata is also loaded from the cache.\"\"\"\n    start: datetime | None\n    \"\"\"The timestamp when the task's execution began.\"\"\"\n    duration: timedelta | None\n    \"\"\"The time that the task took to execute.\"\"\"\n</code></pre>"},{"location":"core/#labtech.types.ResultMeta.start","title":"<code>start: datetime | None</code>  <code>instance-attribute</code>","text":"<p>The timestamp when the task's execution began.</p>"},{"location":"core/#labtech.types.ResultMeta.duration","title":"<code>duration: timedelta | None</code>  <code>instance-attribute</code>","text":"<p>The time that the task took to execute.</p>"},{"location":"core/#labtech.types.LabContext","title":"<code>labtech.types.LabContext: TypeAlias = dict[str, Any]</code>  <code>module-attribute</code>","text":""},{"location":"core/#labtech.is_task_type","title":"<code>labtech.is_task_type(cls)</code>","text":"<p>Returns <code>True</code> if the given <code>cls</code> is a class decorated with <code>labtech.task</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>def is_task_type(cls):\n    \"\"\"Returns `True` if the given `cls` is a class decorated with\n    [`labtech.task`][labtech.task].\"\"\"\n    return isclass(cls) and isinstance(getattr(cls, '_lt', None), TaskInfo)\n</code></pre>"},{"location":"core/#labtech.is_task","title":"<code>labtech.is_task(obj)</code>","text":"<p>Returns <code>True</code> if the given <code>obj</code> is an instance of a task class.</p> Source code in <code>labtech/types.py</code> <pre><code>def is_task(obj):\n    \"\"\"Returns `True` if the given `obj` is an instance of a task class.\"\"\"\n    return is_task_type(type(obj)) and hasattr(obj, '_is_task')\n</code></pre>"},{"location":"core/#labtech.logger","title":"<code>labtech.logger = get_logger()</code>  <code>module-attribute</code>","text":"<p><code>logging.Logger</code> object that labtech logs events to during task execution.</p> <p>Can be used to customize logging and to write additional logs from task <code>run()</code> methods:</p> <pre><code>import logging\nfrom labtech import logger\n\n# Change verbosity of logging\nlogger.setLevel(logging.ERROR)\n\n# Logging methods to call from inside your task's run() method:\nlogger.info('Useful info from task: ...')\nlogger.warning('Warning from task: ...')\nlogger.error('Error from task: ...')\n</code></pre>"},{"location":"diagram/","title":"Diagramming","text":"<p>The <code>labtech.diagram</code> module provides the following functions for automatically generating a diagram to visualise your tasks, their parameters, and their dependency relationships.</p>"},{"location":"diagram/#labtech.diagram.build_task_diagram","title":"<code>labtech.diagram.build_task_diagram(tasks: Sequence[Task], *, direction: str = 'BT') -&gt; str</code>","text":"<p>Returns a Mermaid diagram representing the task types and dependencies of the given tasks.</p> <p>Each task type lists its parameters (with their return types) and its run method (with its return type).</p> <p>Arrows between task types point from a dependency task type to the task type that depends on it, and are labelled with the dependent task's parameter that references the dependency task type.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>               (<code>Sequence[Task]</code>)           \u2013            <p>A collection of tasks to diagram.</p> </li> <li> <code>direction</code>               (<code>str</code>, default:                   <code>'BT'</code> )           \u2013            <p>Direction that task types should be laid out, from dependent tasks to their dependencies. One of:</p> <ul> <li><code>'BT'</code> (bottom-to-top)</li> <li><code>'TB'</code> (top-to-bottom)</li> <li><code>'RL'</code> (right-to-left)</li> <li><code>'LR'</code> (left-to-right)</li> </ul> </li> </ul> Source code in <code>labtech/diagram.py</code> <pre><code>def build_task_diagram(tasks: Sequence[Task], *, direction: str = 'BT') -&gt; str:\n    \"\"\"Returns a [Mermaid diagram](https://mermaid.js.org/syntax/classDiagram.html)\n    representing the task types and dependencies of the given tasks.\n\n    Each task type lists its parameters (with their return types) and\n    its run method (with its return type).\n\n    Arrows between task types point from a dependency task type to the\n    task type that depends on it, and are labelled with the dependent\n    task's parameter that references the dependency task type.\n\n    Args:\n        tasks: A collection of tasks to diagram.\n        direction: Direction that task types should be laid out, from dependent\n            tasks to their dependencies. One of:\n\n            * `'BT'` (bottom-to-top)\n            * `'TB'` (top-to-bottom)\n            * `'RL'` (right-to-left)\n            * `'LR'` (left-to-right)\n\n    \"\"\"\n    return diagram_task_structure(\n        TaskStructure.build(tasks),\n        direction=direction,\n    )\n</code></pre>"},{"location":"diagram/#labtech.diagram.display_task_diagram","title":"<code>labtech.diagram.display_task_diagram(tasks: Sequence[Task], **kwargs) -&gt; None</code>","text":"<p>Displays a Mermaid diagram representing the task types and dependencies of the given tasks.</p> <p>If IPython is available (e.g. the code is being run from a Jupyter notebook), the diagram will be displayed as a Markdown <code>mermaid</code> code block, which will be rendered as a Mermaid diagram from JupyterLab 4.1 and Notebook 7.1.</p> <p>Because Markdown may render arbitrary HTML, you should only diagram tasks that you trust.</p> <p>Accepts the same arguments as build_task_diagram.</p> Source code in <code>labtech/diagram.py</code> <pre><code>def display_task_diagram(tasks: Sequence[Task], **kwargs) -&gt; None:\n    \"\"\"Displays a [Mermaid diagram](https://mermaid.js.org/syntax/classDiagram.html)\n    representing the task types and dependencies of the given tasks.\n\n    If IPython is available (e.g. the code is being run from a Jupyter\n    notebook), the diagram will be displayed as a Markdown `mermaid`\n    code block, which will be rendered as a Mermaid diagram from\n    [JupyterLab 4.1 and Notebook 7.1](https://blog.jupyter.org/jupyterlab-4-1-and-notebook-7-1-are-here-20bfc3c10217).\n\n    Because Markdown may render arbitrary HTML, you should only\n    diagram tasks that you trust.\n\n    Accepts the same arguments as\n    [build_task_diagram][labtech.diagram.build_task_diagram].\n\n    \"\"\"\n    diagram = build_task_diagram(tasks, **kwargs)\n    if is_ipython():\n        from IPython.display import Markdown, display\n        display(Markdown(f'```mermaid\\n{diagram}\\n```'))\n    else:\n        print(diagram)\n</code></pre>"},{"location":"distributed/","title":"Multi-Machine Clusters","text":"<p>Labtech offers built-in support for distributing your tasks across a multi-machine Ray cluster with the <code>RayRunnerBackend</code>.</p> <ul> <li>Ray manages distributing tasks   to available nodes across your cluster, and its   built-in object store   is used to share the lab context and task results across nodes.</li> <li>Ray's locality-aware scheduling   will prefer scheduling tasks to nodes where the results of dependency   tasks are already available.</li> <li>For task results that are NumPy arrays, Ray   uses zero-copy deserialization   to share the in-memory array between all workers on the same node.</li> </ul> <p>You can also use distributed computation platforms other than Ray with Labtech by implementing a custom task runner backend, but this documentation will focus on Labtech's built-in Ray support.</p> <p>Tip</p> <p>Given the inherent complexities that come with managing a cluster of machines, you should always consider whether the scale of your tasks really requires it - public cloud providers offer \"spot\" (i.e. discounted cost, but evictable) virtual machines with dozens of CPU cores and hundreds of GB in memory for less than a dollar an hour, and these may be a perfect fit for easily scaling your tasks.</p>"},{"location":"distributed/#running-labtech-tasks-on-a-ray-cluster","title":"Running Labtech tasks on a Ray cluster","text":"<p>Follow these steps to get Labtech tasks running across a Ray cluster. Because Ray makes it easy to run a local cluster, you can even try Labtech with Ray without setting up a multi-machine cluster.</p>"},{"location":"distributed/#installing-ray","title":"Installing Ray","text":"<p><code>ray</code> is an optional dependency of Labtech, so you must explicitly install it with <code>pip</code> (or your preferred Python package manager). It is recommended that you install <code>ray[default]</code> on the machine you intend to start the cluster from so that you can enable Ray's built-in dashboard:</p> <pre><code>pip install \"ray[default]\"\n</code></pre>"},{"location":"distributed/#using-distributed-storage","title":"Using distributed storage","text":"<p>Because each Labtech task is responsible for caching its result to persistent storage, you must use a storage backend that can be accessed from any node in the cluster. For example, you could use an NFS share or a cloud object storage provider (e.g. Amazon S3 or Azure Blob Storage).</p> <p>To learn how to configure Labtech to use a non-local storage backend, see: How can I cache task results somewhere other than my filesystem?</p> <p>In the following example, we will run a LocalStack instance to emulate an Amazon S3 object storage bucket. For testing, you can run your own LocalStack S3 bucket named <code>labtech-dev-bucket</code> with Docker Compose by creating the following <code>docker-compose.yml</code> and running <code>docker compose up localstack</code>:</p> <pre><code># docker-compose.yml\nservices:\n  localstack:\n    image: localstack/localstack:4.3\n    ports:\n      - \"127.0.0.1:4566:4566\"            # LocalStack Gateway\n      - \"127.0.0.1:4510-4559:4510-4559\"  # external services port range\n    volumes:\n      - \"./.localstack:/var/lib/localstack\"\n    post_start:\n      - command: awslocal s3api create-bucket --bucket labtech-dev-bucket\n</code></pre>"},{"location":"distributed/#code-example","title":"Code example","text":"<p>The following code demonstrates how to configure a Lab to run tasks across a Ray cluster:</p> <pre><code># Ray defaults to de-duplicating similar log messages. To show all log\n# messages from tasks, the RAY_DEDUP_LOGS environment variable must be\n# set to zero **before** importing ray and labtech.runners.ray. See:\n# https://docs.ray.io/en/latest/ray-observability/user-guides/\n#     configure-logging.html#log-deduplication\nimport os\nos.environ['RAY_DEDUP_LOGS'] = '0'\n\nimport labtech\nimport ray\nfrom labtech.storage import FsspecStorage\nfrom labtech.runners.ray import RayRunnerBackend\nfrom s3fs import S3FileSystem\n\n\n# Connect to a Ray cluster:\n# * If no cluster is running locally, `ray.init()` will start one.\n# * If you've started a local cluster with `ray start --head --port 6379`,\n#   `ray.init()` will connect to it.\n#   See: https://docs.ray.io/en/latest/ray-core/starting-ray.html\n# * You can specify the address of a remote cluster,\n#   e.g. `ray.init(address='ray://123.45.67.89:10001')`\nray.init()\n\n\n# Define a custom Storage backend for our localstack S3 bucket\n# using s3fs (which implements the fsspec interface)\nclass S3fsStorage(FsspecStorage):\n\n    def fs_constructor(self):\n        return S3FileSystem(\n            # Use localstack endpoint:\n            endpoint_url='http://localhost:4566',\n            key='anything',\n            secret='anything',\n        )\n\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        labtech.logger.info(f'Running with seed {self.seed}')\n        return self.seed\n\nexperiments = [Experiment(seed=seed) for seed in range(10)]\n\n\n# Configure a Lab with remote storage and a Ray runner backend:\nlab = labtech.Lab(\n    # labtech-dev-bucket is the name of our localstack bucket:\n    storage=S3fsStorage('labtech-dev-bucket/lab_cache'),\n    runner_backend=RayRunnerBackend(),\n)\n\nresults = lab.run_tasks(experiments, bust_cache=True)\nprint(results)\n\n\n# Shutdown the connection to the Ray cluster:\nray.shutdown()\n</code></pre>"},{"location":"distributed/#ray-remote-function-options","title":"Ray remote function options","text":"<p>Ray allows you to configure a number of options that control how a task will be executed. These can be configured for Labtech tasks by defining a <code>runner_options()</code> method on a task type that returns a <code>ray.remote_options</code> section.</p> <p>For example, you can configure the minimum memory and CPU cores that must be available to a Ray worker that is executing a task:</p> <pre><code>    @task\n    class Experiment:\n        ...\n\n        def runner_options(self):\n            # Require 2 CPU cores and 2G of memory for each task of this type.\n            return {\n                'ray': {\n                    'remote_options': {\n                        'num_cpus': '2',\n                        'memory': (2 * (1024 ** 3)),\n                    },\n                }\n            }\n\n        def run(self):\n            ...\n</code></pre>"},{"location":"distributed/#syncing-python-environments-across-a-cluster","title":"Syncing Python environments across a cluster","text":"<p>One of the challenges of running tasks across a distributed cluster is ensuring that the Python execution environment is identical in each worker process running on each node. You should employ the following mechanisms provided by Ray to ensure that your tasks execute identically wherever they are run.</p>"},{"location":"distributed/#worker-initialisation","title":"Worker initialisation","text":"<p>You can use Ray's <code>worker_process_setup_hook</code> to execute one-off setup code before any tasks are run in a Ray worker process.</p> <p>For example, you can pass <code>worker_process_setup_hook</code> into <code>ray.init()</code> to configure mlflow in each worker:</p> <pre><code>def worker_setup():\n    # Initialise mlflow on each worker to use a centralised\n    # mlflow tracking server:\n    mlflow.set_tracking_uri('http://my-mlflow-host:8080')\n    mlflow.set_experiment('example_ray_experiment')\n\n\nray.init(\n    runtime_env={\n        'worker_process_setup_hook': worker_setup,\n    },\n)\n</code></pre>"},{"location":"distributed/#file-and-package-dependencies","title":"File and package dependencies","text":"<p>There are two broad approaches to ensuring the necessary source code, data files, and Python packages are installed on each node in your Ray cluster, as discussed in Ray's documentation on Environment Dependencies:</p> <ol> <li>Pre-install all dependencies onto each node in your cluster. Ray's    cluster launcher has options to support this.</li> <li>Specify a runtime environment    for Ray to install on-demand whenever a node runs a task.<ul> <li>When using Labtech for experimentation, your code and    dependencies may be changing frequently, so the flexibility of a    runtime environment may be a better fit.</li> <li>You can specify a runtime environment for all tasks with    <code>ray.init(runtime_env={...})</code> or for specific tasks with the    <code>runtime_env</code> remote function option.</li> <li>You can use a runtime environment to:<ul> <li>Share a working directory of local source code and data files</li> <li>Share local Python modules that you may still be modifying</li> <li>List Python packages to be installed</li> </ul> </li> </ul> </li> </ol>"},{"location":"distributed/#fault-tolerance","title":"Fault tolerance","text":"<p>Because a variety of failures can occur in a distributed system, Ray has several mechanisms for fault tolerance.</p> <p>You should ensure that your tasks can be safely executed multiple times, as Ray may re-execute tasks under certain circumstances. You can control how many times Ray will re-execute tasks in some of these circumstances through remote function options:</p> <ul> <li>If your task's <code>run()</code> method raises an exception, Ray will not   re-execute the task unless you set   <code>retry_exceptions</code>   to <code>True</code> or a list of exception types.</li> <li>If the worker running a task dies   or a stored object is lost due to node failure   then Ray will re-execute the task up to   <code>max_retries</code>   (which you can disable by setting <code>max_retries=0</code>).<ul> <li>If a task is re-executed after a stored object is lost, it will still   re-run the task instead of loading it's result from the cache.</li> </ul> </li> <li>If Ray's memory monitor   terminates a task to avoid running out of memory, then the task will   be re-executed irrespective of the <code>max_retries</code> setting. You can   disable this behaviour by disabling Ray's built-in memory monitor.</li> </ul>"},{"location":"distributed/#other-considerations","title":"Other considerations","text":"<ul> <li>A Lab's <code>max_workers</code> must be set to <code>None</code> when using the   <code>RayRunnerBackend</code>. This is   because Ray concurrency is not limited by a maximum number of tasks   but by   specifying the resource requirements of each task.</li> <li>Log messages from tasks are not displayed directly under the   Labtech progress bars but are instead available in Ray's worker   logs, which are, by default, available from:<ul> <li>The standard output stream of your Python script running Labtech</li> <li>Under the cell that executes <code>ray.init()</code> in a Jupyter notebook</li> <li>The Ray Dashboard   that aggregates all worker logs</li> </ul> </li> <li>Labtech's task monitor is currently unable to report on the CPU   usage, memory usage, and other process metrics for tasks run on Ray   clusters.<ul> <li>Instead, you can refer to the resource utilisation of cluster   nodes from the Ray Dashboard   or enable the Ray Metrics View   (which requires configuring Prometheus and Grafana).</li> </ul> </li> </ul>"},{"location":"distributed/#api-reference","title":"API Reference","text":""},{"location":"distributed/#labtech.runners.ray.RayRunnerBackend","title":"<code>labtech.runners.ray.RayRunnerBackend</code>","text":"<p>               Bases: <code>RunnerBackend</code></p> <p>Runner Backend that runs each task on a Ray cluster.</p> <p>Ray's shared-memory object store is used to distribute context and results between nodes, and Ray will allocate tasks to cluster nodes where large memory dependencies are already loaded.</p> <p>Ray remote options may be provided for a task by defining a <code>runner_options()</code> on it's Task type that returns a dictionary of options under <code>ray.remote_options</code> (the implementation of which may be based on task parameter values):</p> <pre><code>@task\nclass Experiment:\n    ...\n\n    def runner_options(self):\n        # Require 2 CPU cores and 2G of memory for each task of this type.\n        return {\n            'ray': {\n                'remote_options': {\n                    'num_cpus': '2',\n                    'memory': (2 * (1024 ** 3)),\n                },\n            }\n        }\n\n    def run(self):\n        ...\n</code></pre> Source code in <code>labtech/runners/ray.py</code> <pre><code>class RayRunnerBackend(RunnerBackend):\n    \"\"\"Runner Backend that runs each task on a [Ray](https://www.ray.io/) cluster.\n\n    Ray's [shared-memory object store](https://docs.ray.io/en/latest/ray-core/objects.html)\n    is used to distribute context and results between nodes, and Ray\n    will allocate tasks to cluster nodes where large memory\n    dependencies are already loaded.\n\n    [Ray remote options](https://docs.ray.io/en/latest/ray-core/api/doc/ray.remote_function.RemoteFunction.options.html)\n    may be provided for a task by defining a `runner_options()` on\n    it's Task type that returns a dictionary of options under\n    `ray.remote_options` (the implementation of which may be based on task\n    parameter values):\n\n    ```python\n    @task\n    class Experiment:\n        ...\n\n        def runner_options(self):\n            # Require 2 CPU cores and 2G of memory for each task of this type.\n            return {\n                'ray': {\n                    'remote_options': {\n                        'num_cpus': '2',\n                        'memory': (2 * (1024 ** 3)),\n                    },\n                }\n            }\n\n        def run(self):\n            ...\n    ```\n\n    \"\"\"\n\n    def __init__(self, monitor_interval_seconds: float = 1, monitor_timeout_seconds: int = 5) -&gt; None:\n        \"\"\"\n        Args:\n            monitor_interval_seconds: Determines frequency of requests to\n                Ray for task states.\n            monitor_timeout_seconds: Maximum time to wait for a request to\n                Ray for task states.\n        \"\"\"\n        self.monitor_interval_seconds = monitor_interval_seconds\n        self.monitor_timeout_seconds = monitor_timeout_seconds\n\n    def build_runner(self, *, context: LabContext, storage: Storage, max_workers: int | None) -&gt; Runner:\n        if max_workers is not None:\n            raise RunnerError((\n                'Remove max_workers from your Lab configuration, as RayRunnerBackend only supports max_workers=None. '\n                'You can manage Ray concurrency by specifying required resources in the `runner_options` for a task: '\n                'https://ben-denham.github.io/labtech/distributed/#ray-remote-function-options'\n            ))\n\n        return RayRunner(\n            context=context,\n            storage=storage,\n            monitor_interval_seconds=self.monitor_interval_seconds,\n            monitor_timeout_seconds=self.monitor_timeout_seconds,\n        )\n</code></pre>"},{"location":"distributed/#labtech.runners.ray.RayRunnerBackend.__init__","title":"<code>__init__(monitor_interval_seconds: float = 1, monitor_timeout_seconds: int = 5) -&gt; None</code>","text":"<p>Parameters:</p> <ul> <li> <code>monitor_interval_seconds</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Determines frequency of requests to Ray for task states.</p> </li> <li> <code>monitor_timeout_seconds</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Maximum time to wait for a request to Ray for task states.</p> </li> </ul> Source code in <code>labtech/runners/ray.py</code> <pre><code>def __init__(self, monitor_interval_seconds: float = 1, monitor_timeout_seconds: int = 5) -&gt; None:\n    \"\"\"\n    Args:\n        monitor_interval_seconds: Determines frequency of requests to\n            Ray for task states.\n        monitor_timeout_seconds: Maximum time to wait for a request to\n            Ray for task states.\n    \"\"\"\n    self.monitor_interval_seconds = monitor_interval_seconds\n    self.monitor_timeout_seconds = monitor_timeout_seconds\n</code></pre>"},{"location":"runners/","title":"Task Runner Backends","text":""},{"location":"runners/#task-runner-backends","title":"Task Runner Backends","text":"<p>You can control how tasks are executed in parallel by specifying an instance of one of the following Runner Backend classes for the <code>runner_backend</code> argument of your <code>Lab</code>:</p>"},{"location":"runners/#labtech.runners.ForkRunnerBackend","title":"<code>labtech.runners.ForkRunnerBackend</code>","text":"<p>               Bases: <code>RunnerBackend</code></p> <p>Runner Backend that runs each task in a forked subprocess.</p> <p>The context and dependency task results are shared in-memory between each subprocess.</p>"},{"location":"runners/#labtech.runners.SpawnRunnerBackend","title":"<code>labtech.runners.SpawnRunnerBackend</code>","text":"<p>               Bases: <code>RunnerBackend</code></p> <p>Runner Backend that runs each task in a spawned subprocess.</p> <p>The required context and dependency task results are copied/duplicated into the memory of each subprocess.</p>"},{"location":"runners/#labtech.runners.ThreadRunnerBackend","title":"<code>labtech.runners.ThreadRunnerBackend</code>","text":"<p>               Bases: <code>RunnerBackend</code></p> <p>Runner Backend that runs tasks asynchronously in separate threads.</p> <p>Memory use is reduced by sharing the same in-memory context and dependency task results across threads.</p>"},{"location":"runners/#labtech.runners.SerialRunnerBackend","title":"<code>labtech.runners.SerialRunnerBackend</code>","text":"<p>               Bases: <code>RunnerBackend</code></p> <p>Runner Backend that runs each task serially in the main process and thread.</p>"},{"location":"runners/#distributed-task-runner-backends","title":"Distributed Task Runner Backends","text":"<p>Labtech offers support for running tasks across multiple machines. See: Multi-Machine Clusters</p>"},{"location":"runners/#custom-task-runner-backends","title":"Custom Task Runner Backends","text":"<p>You can define your own Runner Backend to execute tasks with a different form of parallelism or distributed computing platform by defining an implementation of the <code>RunnerBackend</code> abstract base class:</p>"},{"location":"runners/#labtech.types.RunnerBackend","title":"<code>labtech.types.RunnerBackend</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Factory class to construct Runner objects.</p> Source code in <code>labtech/types.py</code> <pre><code>class RunnerBackend(ABC):\n    \"\"\"Factory class to construct [Runner][labtech.types.Runner] objects.\"\"\"\n\n    @abstractmethod\n    def build_runner(self, *, context: LabContext, storage: Storage, max_workers: int | None) -&gt; Runner:\n        \"\"\"Return a Runner prepared with the given configuration.\n\n        Args:\n            context: Additional variables made available to tasks that aren't\n                considered when saving to/loading from the cache.\n            storage: Where task results should be cached to.\n            max_workers: The maximum number of parallel worker processes for\n                running tasks.\n        \"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.RunnerBackend.build_runner","title":"<code>build_runner(*, context: LabContext, storage: Storage, max_workers: int | None) -&gt; Runner</code>  <code>abstractmethod</code>","text":"<p>Return a Runner prepared with the given configuration.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>LabContext</code>)           \u2013            <p>Additional variables made available to tasks that aren't considered when saving to/loading from the cache.</p> </li> <li> <code>storage</code>               (<code>Storage</code>)           \u2013            <p>Where task results should be cached to.</p> </li> <li> <code>max_workers</code>               (<code>int | None</code>)           \u2013            <p>The maximum number of parallel worker processes for running tasks.</p> </li> </ul> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef build_runner(self, *, context: LabContext, storage: Storage, max_workers: int | None) -&gt; Runner:\n    \"\"\"Return a Runner prepared with the given configuration.\n\n    Args:\n        context: Additional variables made available to tasks that aren't\n            considered when saving to/loading from the cache.\n        storage: Where task results should be cached to.\n        max_workers: The maximum number of parallel worker processes for\n            running tasks.\n    \"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner","title":"<code>labtech.types.Runner</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Manages the execution of Tasks, typically by delegating to a parallel processing framework.</p> Source code in <code>labtech/types.py</code> <pre><code>class Runner(ABC):\n    \"\"\"Manages the execution of [Tasks][labtech.types.Task], typically\n    by delegating to a parallel processing framework.\"\"\"\n\n    @abstractmethod\n    def submit_task(self, task: Task, task_name: str, use_cache: bool) -&gt; None:\n        \"\"\"Submit the given task object to be run and have its result cached.\n\n        It is up to the Runner to decide when to start running the\n        task (i.e. when resources become available).\n\n        The implementation of this method should run the task by\n        effectively calling:\n\n        ```\n        for dependency_task in get_direct_dependencies(task, all_identities=True):\n            # Where results_map is expected to contain the TaskResult for\n            # each dependency_task.\n            dependency_task._set_results_map(results_map)\n\n        current_process = multiprocessing.current_process()\n        orig_process_name = current_process.name\n        try:\n            # If the thread name or similar is set instead of the process\n            # name, then the Runner should update the handler of the global\n            # labtech.utils.logger to include that instead of the process name.\n            current_process.name = task_name\n            return labtech.runners.base.run_or_load_task(\n                task=task,\n                use_cache=use_cache,\n                filtered_context=task.filter_context(self.context),\n                storage=self.storage,\n            )\n        finally:\n            current_process.name = orig_process_name\n        ```\n\n        Args:\n            task: The task to execute.\n            task_name: Name to use when referring to the task in logs.\n            use_cache: If True, the task's result should be fetched from the\n                cache if it is available (fetching should still be done in a\n                delegated process).\n\n        \"\"\"\n\n    @abstractmethod\n    def wait(self, *, timeout_seconds: float | None) -&gt; Iterator[tuple[Task, ResultMeta | BaseException]]:\n        \"\"\"Wait up to timeout_seconds or until at least one of the\n        submitted tasks is done, then return an iterator of tasks in a\n        done state and a list of tasks in all other states.\n\n        Each task is returned as a pair where the first value is the\n        task itself, and the second value is either:\n\n        * For a successfully completed task: Metadata of the result.\n        * For a task that fails with any BaseException descendant: The exception\n          that was raised.\n\n        Cancelled tasks are never returned.\n\n        \"\"\"\n\n    @abstractmethod\n    def cancel(self) -&gt; None:\n        \"\"\"Cancel all submitted tasks that have not yet been started.\"\"\"\n\n    @abstractmethod\n    def stop(self) -&gt; None:\n        \"\"\"Stop all currently running tasks.\"\"\"\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Clean up any resources used by the Runner after all tasks\n        are finished, cancelled, or stopped.\"\"\"\n\n    @abstractmethod\n    def pending_task_count(self) -&gt; int:\n        \"\"\"Returns the number of tasks that have been submitted but\n        not yet cancelled or returned from a call to wait().\"\"\"\n\n    @abstractmethod\n    def get_result(self, task: Task) -&gt; TaskResult:\n        \"\"\"Returns the in-memory result for a task that was\n        successfully run by this Runner. Raises a KeyError for a\n        result with no in-memory result.\"\"\"\n\n    @abstractmethod\n    def remove_results(self, tasks: Sequence[Task]) -&gt; None:\n        \"\"\"Removes the in-memory results for tasks that were\n        sucessfully run by this Runner. Ignores tasks that have no\n        in-memory result.\"\"\"\n\n    @abstractmethod\n    def get_task_infos(self) -&gt; list[TaskMonitorInfo]:\n        \"\"\"Returns a snapshot of monitoring information about each\n        task that is currently running.\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.submit_task","title":"<code>submit_task(task: Task, task_name: str, use_cache: bool) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Submit the given task object to be run and have its result cached.</p> <p>It is up to the Runner to decide when to start running the task (i.e. when resources become available).</p> <p>The implementation of this method should run the task by effectively calling:</p> <pre><code>for dependency_task in get_direct_dependencies(task, all_identities=True):\n    # Where results_map is expected to contain the TaskResult for\n    # each dependency_task.\n    dependency_task._set_results_map(results_map)\n\ncurrent_process = multiprocessing.current_process()\norig_process_name = current_process.name\ntry:\n    # If the thread name or similar is set instead of the process\n    # name, then the Runner should update the handler of the global\n    # labtech.utils.logger to include that instead of the process name.\n    current_process.name = task_name\n    return labtech.runners.base.run_or_load_task(\n        task=task,\n        use_cache=use_cache,\n        filtered_context=task.filter_context(self.context),\n        storage=self.storage,\n    )\nfinally:\n    current_process.name = orig_process_name\n</code></pre> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>Task</code>)           \u2013            <p>The task to execute.</p> </li> <li> <code>task_name</code>               (<code>str</code>)           \u2013            <p>Name to use when referring to the task in logs.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>If True, the task's result should be fetched from the cache if it is available (fetching should still be done in a delegated process).</p> </li> </ul> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef submit_task(self, task: Task, task_name: str, use_cache: bool) -&gt; None:\n    \"\"\"Submit the given task object to be run and have its result cached.\n\n    It is up to the Runner to decide when to start running the\n    task (i.e. when resources become available).\n\n    The implementation of this method should run the task by\n    effectively calling:\n\n    ```\n    for dependency_task in get_direct_dependencies(task, all_identities=True):\n        # Where results_map is expected to contain the TaskResult for\n        # each dependency_task.\n        dependency_task._set_results_map(results_map)\n\n    current_process = multiprocessing.current_process()\n    orig_process_name = current_process.name\n    try:\n        # If the thread name or similar is set instead of the process\n        # name, then the Runner should update the handler of the global\n        # labtech.utils.logger to include that instead of the process name.\n        current_process.name = task_name\n        return labtech.runners.base.run_or_load_task(\n            task=task,\n            use_cache=use_cache,\n            filtered_context=task.filter_context(self.context),\n            storage=self.storage,\n        )\n    finally:\n        current_process.name = orig_process_name\n    ```\n\n    Args:\n        task: The task to execute.\n        task_name: Name to use when referring to the task in logs.\n        use_cache: If True, the task's result should be fetched from the\n            cache if it is available (fetching should still be done in a\n            delegated process).\n\n    \"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.wait","title":"<code>wait(*, timeout_seconds: float | None) -&gt; Iterator[tuple[Task, ResultMeta | BaseException]]</code>  <code>abstractmethod</code>","text":"<p>Wait up to timeout_seconds or until at least one of the submitted tasks is done, then return an iterator of tasks in a done state and a list of tasks in all other states.</p> <p>Each task is returned as a pair where the first value is the task itself, and the second value is either:</p> <ul> <li>For a successfully completed task: Metadata of the result.</li> <li>For a task that fails with any BaseException descendant: The exception   that was raised.</li> </ul> <p>Cancelled tasks are never returned.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef wait(self, *, timeout_seconds: float | None) -&gt; Iterator[tuple[Task, ResultMeta | BaseException]]:\n    \"\"\"Wait up to timeout_seconds or until at least one of the\n    submitted tasks is done, then return an iterator of tasks in a\n    done state and a list of tasks in all other states.\n\n    Each task is returned as a pair where the first value is the\n    task itself, and the second value is either:\n\n    * For a successfully completed task: Metadata of the result.\n    * For a task that fails with any BaseException descendant: The exception\n      that was raised.\n\n    Cancelled tasks are never returned.\n\n    \"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.cancel","title":"<code>cancel() -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Cancel all submitted tasks that have not yet been started.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef cancel(self) -&gt; None:\n    \"\"\"Cancel all submitted tasks that have not yet been started.\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.stop","title":"<code>stop() -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Stop all currently running tasks.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef stop(self) -&gt; None:\n    \"\"\"Stop all currently running tasks.\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.close","title":"<code>close() -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Clean up any resources used by the Runner after all tasks are finished, cancelled, or stopped.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Clean up any resources used by the Runner after all tasks\n    are finished, cancelled, or stopped.\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.pending_task_count","title":"<code>pending_task_count() -&gt; int</code>  <code>abstractmethod</code>","text":"<p>Returns the number of tasks that have been submitted but not yet cancelled or returned from a call to wait().</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef pending_task_count(self) -&gt; int:\n    \"\"\"Returns the number of tasks that have been submitted but\n    not yet cancelled or returned from a call to wait().\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.get_result","title":"<code>get_result(task: Task) -&gt; TaskResult</code>  <code>abstractmethod</code>","text":"<p>Returns the in-memory result for a task that was successfully run by this Runner. Raises a KeyError for a result with no in-memory result.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef get_result(self, task: Task) -&gt; TaskResult:\n    \"\"\"Returns the in-memory result for a task that was\n    successfully run by this Runner. Raises a KeyError for a\n    result with no in-memory result.\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.remove_results","title":"<code>remove_results(tasks: Sequence[Task]) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Removes the in-memory results for tasks that were sucessfully run by this Runner. Ignores tasks that have no in-memory result.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef remove_results(self, tasks: Sequence[Task]) -&gt; None:\n    \"\"\"Removes the in-memory results for tasks that were\n    sucessfully run by this Runner. Ignores tasks that have no\n    in-memory result.\"\"\"\n</code></pre>"},{"location":"runners/#labtech.types.Runner.get_task_infos","title":"<code>get_task_infos() -&gt; list[TaskMonitorInfo]</code>  <code>abstractmethod</code>","text":"<p>Returns a snapshot of monitoring information about each task that is currently running.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef get_task_infos(self) -&gt; list[TaskMonitorInfo]:\n    \"\"\"Returns a snapshot of monitoring information about each\n    task that is currently running.\"\"\"\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#labtech-tutorial","title":"Labtech Tutorial","text":"<p>The following tutorial presents a complete example of using labtech to easily add parallelism and caching to machine learning experiments.</p> <p>You can also run this tutorial as an interactive notebook.</p> <p>Before we begin, let's install <code>labtech</code> along with some other dependencies we will use in this tutorial:</p> <pre><code>%pip install labtech mlflow scikit-learn\n</code></pre> <p>Let's also clear any caches that were created by previous runs of this tutorial:</p> <pre><code>!rm -rf storage/tutorial/\n!mkdir -p storage/tutorial/\n</code></pre>"},{"location":"tutorial/#running-a-single-experiment-as-a-labtech-task","title":"Running a single experiment as a labtech task","text":"<p>To get started, we'll take the following simple machine learning experiment code and convert it to be run with labtech.</p> <pre><code>from sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\n\ndigits_X, digits_y = datasets.load_digits(return_X_y=True)\ndigits_X = StandardScaler().fit_transform(digits_X)\n\nclf = RandomForestClassifier(\n    n_estimators=5,\n    random_state=42,\n)\nclf.fit(digits_X, digits_y)\nprob_y = clf.predict_proba(digits_X)\n\nprint(f'{log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>Throughout this tutorial, we will use labtech to improve and extend this experiment, which currently:</p> <ol> <li>Loads and scales the <code>digits</code> dataset.<ul> <li>This is a benchmark dataset where the goal is to train a   classifier that can correctly assign a number between 0 and 9 to   the image of a hand-written digit.</li> <li>We will want to extend our experimentation to also include other   datasets.</li> </ul> </li> <li>Trains a random forest classifier on the digits dataset.<ul> <li>The classifier is configured with <code>n_estimators=5</code> (i.e. a forest   of 5 trees) and a fixed <code>random_state</code> (to ensure we get the same   result every time we run the code).</li> <li>We will want to extend our experimentation to test other   <code>n_estimators</code> values and classifiers other than a random forest.</li> </ul> </li> <li>Evaluates the classifier by calculating the logistic loss of    probabilities predicted by the classifier for the dataset.<ul> <li>Standard evaluation practice would be to calculate loss for a   separate test dataset, but we will use a single dataset for both   training and testing to simplify this tutorial.</li> </ul> </li> </ol> <p>Let's set up this same experiment to be run with labtech, providing us with a foundation that we can extend throughout this tutorial.</p> <p>First, we'll define a labtech task type that will load the dataset, train the classifier, and return the probabilities predicted for the dataset. Defining a task type for our experiment is as simple as defining a class decorated with <code>@labtech.task</code> that defines a <code>run()</code> method that performs the experiment and returns its result (the predicted probabilities):</p> <pre><code>import labtech\n\n@labtech.task\nclass ClassifierExperiment:\n\n    def run(self):\n        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n        digits_X = StandardScaler().fit_transform(digits_X)\n\n        clf = RandomForestClassifier(\n            n_estimators=5,\n            random_state=42,\n        )\n        clf.fit(digits_X, digits_y)\n        prob_y = clf.predict_proba(digits_X)\n        return prob_y\n</code></pre> <p>Next, we create a labtech lab that can be used to execute the experiment. We'll configure the lab to cache results in a folder called <code>storage/tutorial/classification_lab_1</code> and to display notebook-friendly progress bars:</p> <pre><code>lab = labtech.Lab(storage='storage/tutorial/classification_lab_1')\n</code></pre> <p>Finally, we create a task instance of <code>ClassifierExperiment</code> and call <code>lab.run_task()</code> to run it. The output will be the predicted probabilities returned by the task's <code>run()</code> method, so we can calculate the loss from them as before:</p> <pre><code>classifier_experiment = ClassifierExperiment()\nprob_y = lab.run_task(classifier_experiment)\nprint(f'{log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>An immediate benefit of running an experiment this way with labtech is that the result will be cached to disk for future use. Any future calls to run the same experiment (even after restarting Python) will load the result from the cache:</p> <pre><code>prob_y = lab.run_task(classifier_experiment)\nprint(f'{log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>Defining the task to return the prediction probabilities instead of just the loss metric gives us flexibility to change the evaluation in the future (e.g. from <code>log_loss</code> to another metric) while still being able to re-use the same cached result.</p> <p>We can also ask our lab to return <code>task</code> objects for all previously cached results for a given task type by calling <code>lab.cached_tasks()</code>. A given task could then be passed to <code>lab.run_task()</code> to load it's result (or we could pass a list of tasks to <code>lab.run_tasks()</code>, as we will see in the next section of this tutorial).</p> <pre><code>lab.cached_tasks([\n    ClassifierExperiment,\n])\n</code></pre> <p>Whenever you make a change that will impact the behaviour of a task (i.e. most changes to the <code>run()</code> method or the code it depends on), it is very important that you add or update the <code>code_version</code> in <code>@task</code> (e.g. <code>@task(code_version='v2')</code>). Labtech will re-run tasks if there are no cached results with a <code>code_version</code> matching your current code. If you don't update the <code>code_version</code> or otherwise clear your cache, then the returned cached results may no longer reflect the actual results of your current code.</p> <p>You may like to save storage space by clearing up old cached results with <code>lab.uncache_tasks()</code>:</p> <pre><code>lab.uncache_tasks([\n    classifier_experiment,\n])\n</code></pre>"},{"location":"tutorial/#parameterising-tasks-and-running-many-tasks-in-parallel","title":"Parameterising tasks, and running many tasks in parallel","text":"<p>Let's extend our experimentation to compare the results for classifiers configured with different <code>n_estimators</code> values.</p> <p>To do so, we'll add an <code>n_estimators</code> parameter to our <code>ClassifierExperiment</code> task type and reference it within the <code>run()</code> method as <code>self.n_estimators</code>. Task parameters are declared in exactly the same way as dataclass fields:</p> <pre><code>import labtech\n\n@labtech.task\nclass ClassifierExperiment:\n    n_estimators: int\n\n    def run(self):\n        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n        digits_X = StandardScaler().fit_transform(digits_X)\n\n        clf = RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            random_state=42,\n        )\n        clf.fit(digits_X, digits_y)\n        prob_y = clf.predict_proba(digits_X)\n        return prob_y\n</code></pre> <p>Now we'll use a list comprehension to construct a list of <code>ClassifierExperiment</code> tasks with different <code>n_estimators</code> values:</p> <pre><code>classifier_experiments = [\n    ClassifierExperiment(\n        n_estimators=n_estimators,\n    )\n    for n_estimators in range(1, 11)\n]\n</code></pre> <p>We can run a list of tasks with <code>lab.run_tasks()</code>, which has the added benefit of leveraging Python's multiprocessing capabilities to run the tasks in parallel - running as many tasks simultaneously as possible with the CPU of the machine running the tasks. Also, because we've changed the definition of our <code>ClassifierExperiment</code> class, we'll keep caches for the new definition separate by constructing a new lab that uses a different storage directory:</p> <pre><code>lab = labtech.Lab(storage='storage/tutorial/classification_lab_2')\nresults = lab.run_tasks(classifier_experiments)\n</code></pre> <p><code>lab.run_tasks()</code> returns a dictionary mapping each input task to the result it returned, which we can loop over to print loss metrics for each experiment:</p> <pre><code>for experiment, prob_y in results.items():\n    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')\n</code></pre>"},{"location":"tutorial/#maximising-concurrency-and-caching-with-dependent-tasks","title":"Maximising concurrency and caching with dependent tasks","text":"<p>Labtech's true power lies in its ability to manage complex networks of dependent tasks - automatically running as many tasks as possible in parallel (even different types of tasks) and re-using cached results wherever possible.</p> <p>To demonstrate this, let's extend our experimentation with a new post-processing step that will take the probabilities returned by one of our previous <code>ClassifierExperiment</code> tasks and assign a probability of <code>1</code> to the most likely class for each record (and conversely assign a probability of <code>0</code> to all other classes).</p> <p>To achieve this, we will define a new <code>MinMaxProbabilityExperiment</code> task type that accepts a <code>ClassifierExperiment</code> as a parameter. Labtech will consider any task in a parameter to be a dependency of the task. Dependency tasks will be run before any of their dependent tasks, allowing us to access the result from the <code>.result</code> attribute of the task parameter (i.e. <code>self.classifier_experiment.result</code>):</p> <pre><code>import numpy as np\n\n\n@labtech.task\nclass MinMaxProbabilityExperiment:\n    classifier_experiment: ClassifierExperiment\n\n    def run(self):\n        prob_y = self.classifier_experiment.result\n        # Replace the maximum probability in each row with 1,\n        # and replace all other probabilities with 0.\n        min_max_prob_y = np.zeros(prob_y.shape)\n        min_max_prob_y[np.arange(len(prob_y)), prob_y.argmax(axis=1)] = 1\n        return min_max_prob_y\n</code></pre> <p>We can then construct and run a list of <code>MinMaxProbabilityExperiment</code> tasks that depend on our previous <code>ClassifierExperiment</code> tasks in <code>classifier_experiments</code>. Labtech will ensure each of the <code>classifier_experiments</code> has been run before it's dependent <code>MinMaxProbabilityExperiment</code> is run, re-using results depended on by multiple tasks and loading previously cached results wherever possible:</p> <pre><code>min_max_prob_experiments = [\n    MinMaxProbabilityExperiment(\n        classifier_experiment=classifier_experiment,\n    )\n    for classifier_experiment in classifier_experiments\n]\n\nresults = lab.run_tasks(min_max_prob_experiments)\nfor experiment, prob_y in results.items():\n    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>By simply specifying task dependencies, you can construct any task structure that can be expressed as a directed acyclic graph (or DAG) and let labtech handle running tasks concurrently, sharing results between dependent tasks, and using caches wherever possible.</p>"},{"location":"tutorial/#parameterising-tasks-with-complex-objects","title":"Parameterising tasks with complex objects","text":"<p>Now let's extend our experimentation to compare different classifier models. We'd like to make the classifier itself a parameter to the task, but task parameters can only be json-serializable values or dependency tasks. Therefore, we will use dependency tasks to construct and return classifier objects to our experiment tasks. We achieve this in the following code by:</p> <ol> <li>Defining <code>RFClassifierTask</code> and <code>LRClassifierTask</code> task types.<ul> <li><code>RFClassifierTask</code> returns a random forest classifier   parameterised by an <code>n_estimators</code> value.</li> <li><code>LRClassifierTask</code> returns a logistic regression classifier.</li> <li>Because constructing a classifier object is inexpensive, we don't   need to cache them, so we set <code>cache=None</code> in the <code>@labtech.task</code>   decorator for these task types.</li> <li>For type hinting purposes, we will identify these task types   with the <code>ClassifierTask</code> Protocol,   which will match any task type that returns an sklearn   classifier.</li> </ul> </li> <li>Redefining <code>ClassifierExperiment</code> to be parameterised by a    <code>ClassifierTask</code>.<ul> <li>The classifier object to be trained and applied is retrieved   from the <code>ClassifierTask</code> result with   <code>self.classifier_task.result</code>.</li> <li>Because one <code>ClassifierTask</code> result may be shared by many   <code>ClassifierExperiment</code> tasks, the <code>run()</code> method first creates   its own copy of the classifier with <code>clone()</code>.</li> </ul> </li> </ol> <pre><code>from typing import Protocol\n\nfrom sklearn.base import clone, ClassifierMixin\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass ClassifierTask(Protocol):\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(cache=None)\nclass RFClassifierTask:\n    n_estimators: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            random_state=42,\n        )\n\n\n@labtech.task(cache=None)\nclass LRClassifierTask:\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=42,\n        )\n\n\n@labtech.task\nclass ClassifierExperiment:\n    classifier_task: ClassifierTask\n\n    def run(self):\n        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n        digits_X = StandardScaler().fit_transform(digits_X)\n\n        clf = clone(self.classifier_task.result)\n        clf.fit(digits_X, digits_y)\n        prob_y = clf.predict_proba(digits_X)\n        return prob_y\n</code></pre> <p>Now we can generate and run a set of <code>RFClassifierTask</code> tasks for various <code>n_estimators</code> values, and construct a <code>ClassifierExperiment</code> for each of these <code>RFClassifierTask</code> tasks as well as an <code>LRClassifierTask</code> task:</p> <pre><code>rf_classifier_tasks = [\n    RFClassifierTask(\n        n_estimators=n_estimators,\n    )\n    for n_estimators in range(1, 11)\n]\nclassifier_experiments = [\n    ClassifierExperiment(\n        classifier_task=classifier_task,\n    )\n    for classifier_task in [\n        LRClassifierTask(),\n        *rf_classifier_tasks,\n    ]\n]\n\nlab = labtech.Lab(storage='storage/tutorial/classification_lab_3')\n\nresults = lab.run_tasks(classifier_experiments)\nfor experiment, prob_y in results.items():\n    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')\n</code></pre>"},{"location":"tutorial/#providing-large-objects-as-context","title":"Providing large objects as context","text":"<p>Sometimes we want to pass large, unchanging objects to our tasks, but don't want to be forced to load them in a dependent task. For example, it would be convenient to load a collection of datasets (on which to run our experiments) outside of any task, allowing us to inspect these datasets before and after the tasks have been run:</p> <pre><code>iris_X, iris_y = datasets.load_iris(return_X_y=True)\niris_X = StandardScaler().fit_transform(iris_X)\n\nDATASETS = {\n    'digits': {'X': digits_X, 'y': digits_y},\n    'iris': {'X': iris_X, 'y': iris_y},\n}\n</code></pre> <p>To achieve this, a labtech lab can be provided with a context that is made available to all tasks. In the following code, we:</p> <ol> <li>Pass a <code>context</code> to the <code>labtech.Lab()</code> constructor, with a    <code>'DATASETS'</code> key for the set of <code>DATASETS</code> defined above.</li> <li>Redefine <code>ClassifierExperiment</code> to accept a <code>dataset_key</code> parameter    and use it to look up a dataset inside the <code>'DATASETS'</code> key of the    context, which is made available by labtech as <code>self.context</code>.</li> <li>Alter the task generation and evaluation code to handle multiple    datasets.</li> </ol> <pre><code>@labtech.task\nclass ClassifierExperiment:\n    classifier_task: ClassifierTask\n    dataset_key: str\n\n    def run(self):\n        dataset = self.context['DATASETS'][self.dataset_key]\n        X, y = dataset['X'], dataset['y']\n\n        clf = clone(self.classifier_task.result)\n        clf.fit(X, y)\n        prob_y = clf.predict_proba(X)\n        return prob_y\n\n\nclassifier_experiments = [\n    ClassifierExperiment(\n        classifier_task=classifier_task,\n        dataset_key=dataset_key,\n    )\n    # By including multiple for clauses, we will produce a ClassifierExperiment\n    # for every combination of dataset_key and classifier_task\n    for dataset_key in DATASETS.keys()\n    for classifier_task in [LRClassifierTask(), *rf_classifier_tasks]\n]\n\nlab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_4',\n    context={\n        'DATASETS': DATASETS,\n    },\n)\n\nresults = lab.run_tasks(classifier_experiments)\nfor experiment, prob_y in results.items():\n    dataset_y = DATASETS[experiment.dataset_key][\"y\"]\n    print(f'{experiment}: {log_loss(dataset_y, prob_y) = :.3}')\n</code></pre> <p>The lab context can also be useful for passing parameters to a task that won't affect its result and therefore don't need to be part of the task's formal parameters. For example: log levels and task-internal parallelism settings.</p> <p>It is important that you do NOT make changes to context values that impact task results after you have started caching experiment results - otherwise your cached results may not reflect your latest context values.</p>"},{"location":"tutorial/#bringing-it-all-together-and-aggregating-results","title":"Bringing it all together and aggregating results","text":"<p>The following code brings all the steps from this tutorial together in one place, with some additional improvements:</p> <ul> <li>The \"experiment-like\" <code>ClassifierExperiment</code> and   <code>MinMaxProbabilityExperiment</code> task types are now identified by a   common <code>ExperimentTask</code> Protocol (which requires each of those   classes to provide a <code>dataset_key</code> attribute or property that is   used by the new <code>ExperimentEvaluationTask</code>).</li> <li>A new, final, <code>ExperimentEvaluationTask</code> task that depends on all   <code>ExperimentTask</code> tasks is used to compute the loss metric for all   experiments.<ul> <li>A final task like this is useful once we have a large number of   experiments as it allows us to cache the final evaluation of all   tasks, meaning that we only need to load experiment results and   re-calculate metrics when experiment parameters have changed or   new experiments have been added.</li> </ul> </li> <li>We enable labtech's integration with <code>mlflow</code>   by the following additions (see How can I use labtech with mlfow?   for details):<ol> <li>We add <code>mlflow_run=True</code> to the <code>@labtech.task</code> decorator of    <code>ClassifierExperiment</code> and <code>MinMaxProbabilityExperiment</code>,    indicating that each task of these types should be recorded as    a \"run\" in mflow.</li> <li>We name the over-arching mlflow \"experiment\" with    <code>mlflow.set_experiment('example_labtech_experiment')</code> before    the tasks are run.</li> </ol> </li> </ul> <pre><code>from typing import Protocol\n\nimport labtech\nfrom sklearn.base import clone, ClassifierMixin\n\n\n# === Prepare Datasets ===\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndigits_X, digits_y = datasets.load_digits(return_X_y=True)\ndigits_X = StandardScaler().fit_transform(digits_X)\n\niris_X, iris_y = datasets.load_iris(return_X_y=True)\niris_X = StandardScaler().fit_transform(iris_X)\n\nDATASETS = {\n    'digits': {'X': digits_X, 'y': digits_y},\n    'iris': {'X': iris_X, 'y': iris_y},\n}\n\n\n# === Classifier Tasks ===\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass ClassifierTask(Protocol):\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(cache=None)\nclass RFClassifierTask:\n    n_estimators: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            random_state=42,\n        )\n\n\n@labtech.task(cache=None)\nclass LRClassifierTask:\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=42,\n        )\n\n\n# === Experiment Tasks ===\n\nclass ExperimentTask(Protocol):\n    dataset_key: str\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(mlflow_run=True)\nclass ClassifierExperiment(ExperimentTask):\n    classifier_task: ClassifierTask\n    dataset_key: str\n\n    def run(self) -&gt; np.ndarray:\n        dataset = self.context['DATASETS'][self.dataset_key]\n        X, y = dataset['X'], dataset['y']\n\n        clf = clone(self.classifier_task.result)\n        clf.fit(X, y)\n        prob_y = clf.predict_proba(X)\n        return prob_y\n\n\n@labtech.task(mlflow_run=True)\nclass MinMaxProbabilityExperiment(ExperimentTask):\n    experiment: ExperimentTask\n\n    @property\n    def dataset_key(self):\n        return self.experiment.dataset_key\n\n    def run(self) -&gt; np.ndarray:\n        prob_y = self.experiment.result\n        # Replace the maximum probability in each row with 1,\n        # and replace all other probabilities with 0.\n        min_max_prob_y = np.zeros(prob_y.shape)\n        min_max_prob_y[np.arange(len(prob_y)), prob_y.argmax(axis=1)] = 1\n        return min_max_prob_y\n\n\n# === Results Aggregation ===\n\nfrom sklearn.metrics import log_loss\n\n\n@labtech.task\nclass ExperimentEvaluationTask:\n    experiments: list[ExperimentTask]\n\n    def run(self):\n        return {\n            experiment: {'log_loss': log_loss(\n                self.context['DATASETS'][experiment.dataset_key]['y'],\n                experiment.result,\n            )}\n            for experiment in self.experiments\n        }\n\n\n# === Task Construction ===\n\nrf_classifier_tasks = [\n    RFClassifierTask(\n        n_estimators=n_estimators,\n    )\n    for n_estimators in range(1, 11)\n]\n\nclassifier_experiments = [\n    ClassifierExperiment(\n        classifier_task=classifier_task,\n        dataset_key=dataset_key,\n    )\n    for dataset_key in DATASETS.keys()\n    for classifier_task in [LRClassifierTask(), *rf_classifier_tasks]\n]\n\nmin_max_prob_experiments = [\n    MinMaxProbabilityExperiment(\n        experiment=classifier_experiment,\n    )\n    for classifier_experiment in classifier_experiments\n]\n\nevaluation_task = ExperimentEvaluationTask(\n    experiments=[\n        *classifier_experiments,\n        *min_max_prob_experiments,\n    ]\n)\n\n\n# === Task Execution ===\n\nimport mlflow\n\nmlflow.set_experiment('example_labtech_experiment')\nlab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_final',\n    context={\n        'DATASETS': DATASETS,\n    },\n)\n\nevaluation_result = lab.run_task(evaluation_task)\nfor experiment, result in evaluation_result.items():\n    print(f'{experiment}: log_loss = {result[\"log_loss\"]:.3}')\n</code></pre>"},{"location":"tutorial/#visualising-tasks-and-dependencies","title":"Visualising tasks and dependencies","text":"<p>Finally, we can use Labtech to generate a diagram of a list of tasks that shows all of the task types, parameters, and dependencies:</p> <pre><code>from labtech.diagram import display_task_diagram\n\ndisplay_task_diagram([\n    evaluation_task,\n], direction='BT')\n</code></pre> <pre><code>classDiagram\n    direction BT\n\n    class ExperimentEvaluationTask\n    ExperimentEvaluationTask : list[ExperimentTask] experiments\n    ExperimentEvaluationTask : run()\n\n    class ClassifierExperiment\n    ClassifierExperiment : ClassifierTask classifier_task\n    ClassifierExperiment : str dataset_key\n    ClassifierExperiment : run() ndarray\n\n    class MinMaxProbabilityExperiment\n    MinMaxProbabilityExperiment : ExperimentTask experiment\n    MinMaxProbabilityExperiment : run() ndarray\n\n    class LRClassifierTask\n    LRClassifierTask : run() ClassifierMixin\n\n    class RFClassifierTask\n    RFClassifierTask : int n_estimators\n    RFClassifierTask : run() ClassifierMixin\n\n\n    ExperimentEvaluationTask &lt;-- \"many\" ClassifierExperiment: experiments\n    ExperimentEvaluationTask &lt;-- \"many\" MinMaxProbabilityExperiment: experiments\n\n    ClassifierExperiment &lt;-- LRClassifierTask: classifier_task\n    ClassifierExperiment &lt;-- RFClassifierTask: classifier_task\n\n    MinMaxProbabilityExperiment &lt;-- ClassifierExperiment: experiment</code></pre> <p>Such diagrams can help you visualise how your experiments are running, and may be useful to include in project documentation.</p>"},{"location":"tutorial/#next-steps","title":"Next steps","text":"<p>Congratulations on completing the labtech tutorial! You're now ready to manage complex experiment workflows with ease!</p> <p>To learn more about labtech, you can dive into the following resources:</p> <ul> <li>Cookbook of common patterns (as an interactive notebook)</li> <li>API reference for Labs and Tasks</li> <li>More options for cache formats and storage providers</li> <li>Diagramming tools</li> <li>More examples</li> </ul>"}]}