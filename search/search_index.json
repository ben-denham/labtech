{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"README","text":"labtech <p> GitHub - Documentation </p> <p>Labtech makes it easy to define multi-step experiment pipelines and run them with maximal parallelism and result caching:</p> <ul> <li>Defining tasks is simple; write a class with a single <code>run()</code>   method and parameters as dataclass-style attributes.</li> <li>Flexible experiment configuration; simply create task objects   for all of your parameter permutations.</li> <li>Handles pipelines of tasks; any task parameter that is itself a   task will be executed first and make its result available to its   dependent task(s).</li> <li>Implicit parallelism; Labtech resolves task dependencies and   runs tasks in sub-processes with as much parallelism as possible.</li> <li>Implicit caching and loading of task results; configurable and   extensible options for how and where task results are cached.</li> <li>Integration with mlflow; Automatically   log task runs to mlflow with all of their parameters.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install labtech\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from time import sleep\n\nimport labtech\n\n# Decorate your task class with @labtech.task:\n@labtech.task\nclass Experiment:\n    # Each Experiment task instance will take `base` and `power` parameters:\n    base: int\n    power: int\n\n    def run(self) -&gt; int:\n        # Define the task's run() method to return the result of the experiment:\n        labtech.logger.info(f'Raising {self.base} to the power of {self.power}')\n        sleep(1)\n        return self.base ** self.power\n\ndef main():\n    # Configure Experiment parameter permutations\n    experiments = [\n        Experiment(\n            base=base,\n            power=power,\n        )\n        for base in range(5)\n        for power in range(5)\n    ]\n\n    # Configure a Lab to run the experiments:\n    lab = labtech.Lab(\n        # Specify a directory to cache results in (running the experiments a second\n        # time will just load results from the cache!):\n        storage='demo_lab',\n        # Control the degree of parallelism:\n        max_workers=5,\n    )\n\n    # Run the experiments!\n    results = lab.run_tasks(experiments)\n    print([results[experiment] for experiment in experiments])\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Labtech can also produce graphical progress bars in Jupyter when the <code>Lab</code> is initialized with <code>notebook=True</code>:</p> <p></p> <p>Tasks parameters can be any of the following types:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>, <code>dict</code>, <code>Enum</code></li> <li>Task types: A task parameter is a \"nested task\" that will be   executed before its parent so that it may make use of the nested   result.</li> </ul> <p>Here's an example of defining a single long-running task to produce a result for a large number of dependent tasks:</p> <pre><code>from time import sleep\n\nimport labtech\n\n@labtech.task\nclass SlowTask:\n    base: int\n\n    def run(self) -&gt; int:\n        sleep(5)\n        return self.base ** 2\n\n@labtech.task\nclass DependentTask:\n    slow_task: SlowTask\n    multiplier: int\n\n    def run(self) -&gt; int:\n        return self.multiplier * self.slow_task.result\n\ndef main():\n    some_slow_task = SlowTask(base=42)\n    dependent_tasks = [\n        DependentTask(\n            slow_task=some_slow_task,\n            multiplier=multiplier,\n        )\n        for multiplier in range(10)\n    ]\n\n    lab = labtech.Lab(storage='demo_lab')\n    results = lab.run_tasks(dependent_tasks)\n    print([results[task] for task in dependent_tasks])\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Labtech can even generate a Mermaid diagram to visualise your tasks:</p> <pre><code>from labtech.diagram import display_task_diagram\n\nsome_slow_task = SlowTask(base=42)\ndependent_tasks = [\n    DependentTask(\n        slow_task=some_slow_task,\n        multiplier=multiplier,\n    )\n    for multiplier in range(10)\n]\n\ndisplay_task_diagram(dependent_tasks)\n</code></pre> <pre><code>classDiagram\n    direction BT\n\n    class DependentTask\n    DependentTask : SlowTask slow_task\n    DependentTask : int multiplier\n    DependentTask : run() int\n\n    class SlowTask\n    SlowTask : int base\n    SlowTask : run() int\n\n\n    DependentTask &lt;-- SlowTask: slow_task</code></pre> <p>To learn more, dive into the following resources:</p> <ul> <li>The labtech tutorial (as an interactive notebook)</li> <li>Cookbook of common patterns (as an interactive notebook)</li> <li>API reference for Labs and Tasks</li> <li>More options for cache formats and storage providers</li> <li>Diagramming tools</li> <li>More examples</li> </ul>"},{"location":"#mypy-plugin","title":"Mypy Plugin","text":"<p>For mypy type-checking of classes decorated with <code>labtech.task</code>, simply enable the labtech mypy plugin in your <code>mypy.ini</code> file:</p> <pre><code>[mypy]\nplugins = labtech.mypy_plugin\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Install Poetry dependencies with <code>make deps</code></li> <li>Run linting, mypy, and tests with <code>make check</code></li> <li>Documentation:<ul> <li>Run local server: <code>make docs-serve</code></li> <li>Build docs: <code>make docs-build</code></li> <li>Deploy docs to GitHub Pages: <code>make docs-github</code></li> <li>Docstring style follows the Google style guide</li> </ul> </li> </ul>"},{"location":"#todo","title":"TODO","text":"<ul> <li>Add unit tests</li> </ul>"},{"location":"caching/","title":"Caches and Storage","text":""},{"location":"caching/#caches","title":"Caches","text":"<p>You can control how the results of a particular task type should be formatted for caching by specifying an instance of one of the following Cache classes for the <code>cache</code> argument of the <code>labtech.task</code> decorator:</p>"},{"location":"caching/#labtech.cache.PickleCache","title":"<code>labtech.cache.PickleCache</code>","text":"<p>             Bases: <code>BaseCache</code></p> <p>Default cache that stores results as pickled Python objects.</p> <p>NOTE: As pickle is not secure, you should only load pickle cache results that you trust.</p> Source code in <code>labtech/cache.py</code> <pre><code>class PickleCache(BaseCache):\n    \"\"\"Default cache that stores results as\n    [pickled](https://docs.python.org/3/library/pickle.html) Python\n    objects.\n\n    NOTE: As pickle is not secure, you should only load pickle cache\n    results that you trust.\n\n    \"\"\"\n\n    KEY_PREFIX = 'pickle__'\n    RESULT_FILENAME = 'data.pickle'\n\n    def __init__(self, *, serializer: Optional[Serializer] = None,\n                 pickle_protocol: int = pickle.HIGHEST_PROTOCOL):\n        super().__init__(serializer=serializer)\n        self.pickle_protocol = pickle_protocol\n\n    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n        data_file = storage.file_handle(task.cache_key, self.RESULT_FILENAME, mode='wb')\n        with data_file:\n            pickle.dump(result, data_file, protocol=self.pickle_protocol)\n\n    def load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n        data_file = storage.file_handle(task.cache_key, self.RESULT_FILENAME, mode='rb')\n        with data_file:\n            return pickle.load(data_file)\n</code></pre>"},{"location":"caching/#labtech.cache.NullCache","title":"<code>labtech.cache.NullCache</code>","text":"<p>             Bases: <code>Cache</code></p> <p>Cache that never stores results in the storage provider.</p> Source code in <code>labtech/cache.py</code> <pre><code>class NullCache(Cache):\n    \"\"\"Cache that never stores results in the storage provider.\"\"\"\n\n    def cache_key(self, task: Task) -&gt; str:\n        return 'null'\n\n    def is_cached(self, storage: Storage, task: Task) -&gt; bool:\n        return False\n\n    def save(self, storage: Storage, task: Task[ResultT], result: TaskResult[ResultT]):\n        pass\n\n    def load_task(self, storage: Storage, task_type: Type[TaskT], key: str) -&gt; TaskT:\n        raise TaskNotFound\n\n    def load_result_with_meta(self, storage: Storage, task: Task[ResultT]) -&gt; TaskResult[ResultT]:\n        raise CacheError('Loading a result from a NullCache is not supported.')\n\n    def load_cache_timestamp(self, storage: Storage, task: Task) -&gt; Any:\n        raise CacheError('Loading a cache_timestamp from a NullCache is not supported.')\n\n    def delete(self, storage: Storage, task: Task):\n        pass\n</code></pre>"},{"location":"caching/#custom-caches","title":"Custom Caches","text":"<p>You can define your own type of Cache with its own format or behaviour by inheriting from <code>BaseCache</code>:</p>"},{"location":"caching/#labtech.cache.BaseCache","title":"<code>labtech.cache.BaseCache</code>","text":"<p>             Bases: <code>Cache</code></p> <p>Base class for defining a Cache that will store results in a storage provider.</p> Source code in <code>labtech/cache.py</code> <pre><code>class BaseCache(Cache):\n    \"\"\"Base class for defining a Cache that will store results in a\n    storage provider.\"\"\"\n\n    KEY_PREFIX = ''\n    \"\"\"Prefix for all files created by this Cache type - should be\n    different for each Cache type to avoid conflicts.\"\"\"\n\n    METADATA_FILENAME = 'metadata.json'\n\n    def __init__(self, *, serializer: Optional[Serializer] = None):\n        self.serializer = serializer or Serializer()\n\n    def cache_key(self, task: Task) -&gt; str:\n        serialized_str = json.dumps(self.serializer.serialize_task(task)).encode('utf-8')\n        # Use sha1, as it is the same hash as git, produces short\n        # hashes, and security concerns with sha1 are not relevant to\n        # our use case.\n        hashed = hashlib.sha1(serialized_str).hexdigest()\n        return f'{self.KEY_PREFIX}{task.__class__.__qualname__}__{hashed}'\n\n    def is_cached(self, storage: Storage, task: Task) -&gt; bool:\n        return storage.exists(task.cache_key)\n\n    def save(self, storage: Storage, task: Task[ResultT], task_result: TaskResult[ResultT]):\n        start_timestamp = None\n        if task_result.meta.start is not None:\n            start_timestamp = task_result.meta.start.isoformat()\n\n        duration_seconds = None\n        if task_result.meta.duration is not None:\n            duration_seconds = task_result.meta.duration.total_seconds()\n\n        metadata = {\n            'labtech_version': labtech_version,\n            'cache': self.__class__.__qualname__,\n            'cache_key': task.cache_key,\n            'task': self.serializer.serialize_task(task),\n            'start_timestamp': start_timestamp,\n            'duration_seconds': duration_seconds,\n        }\n        metadata_file = storage.file_handle(task.cache_key, self.METADATA_FILENAME, mode='w')\n        with metadata_file:\n            json.dump(metadata, metadata_file, indent=2)\n        self.save_result(storage, task, task_result.value)\n\n    def load_metadata(self, storage: Storage, task_type: Type[Task], key: str) -&gt; dict[str, Any]:\n        if not key.startswith(f'{self.KEY_PREFIX}{task_type.__qualname__}'):\n            raise TaskNotFound\n        with storage.file_handle(key, self.METADATA_FILENAME, mode='r') as metadata_file:\n            metadata = json.load(metadata_file)\n        if metadata.get('cache') != self.__class__.__qualname__:\n            raise TaskNotFound\n        return metadata\n\n    def build_result_meta(self, metadata: dict[str, Any]) -&gt; ResultMeta:\n        start = None\n        if 'start_timestamp' in metadata:\n            start = datetime.fromisoformat(metadata['start_timestamp'])\n\n        duration = None\n        if 'duration_seconds' in metadata:\n            duration = timedelta(seconds=metadata['duration_seconds'])\n\n        return ResultMeta(\n            start=start,\n            duration=duration,\n        )\n\n    def load_task(self, storage: Storage, task_type: Type[TaskT], key: str) -&gt; TaskT:\n        metadata = self.load_metadata(storage, task_type, key)\n        result_meta = self.build_result_meta(metadata)\n        task = self.serializer.deserialize_task(metadata['task'], result_meta=result_meta)\n        if not isinstance(task, task_type):\n            raise TaskNotFound\n        return task\n\n    def load_result_with_meta(self, storage: Storage, task: Task[ResultT]) -&gt; TaskResult[ResultT]:\n        result = self.load_result(storage, task)\n        metadata = self.load_metadata(storage, type(task), task.cache_key)\n        return TaskResult(\n            value=result,\n            meta=self.build_result_meta(metadata),\n        )\n\n    def delete(self, storage: Storage, task: Task):\n        storage.delete(task.cache_key)\n\n    @abstractmethod\n    def load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n        \"\"\"Loads the result for the given task from the storage provider.\n\n        Args:\n            storage: Storage provider to load the result from\n            task: task instance to load the result for\n\n        \"\"\"\n\n    @abstractmethod\n    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n        \"\"\"Saves the given task result into the storage provider.\n\n        Args:\n            storage: Storage provider to save the result into\n            task: task instance the result belongs to\n            result: result to save\n\n        \"\"\"\n</code></pre>"},{"location":"caching/#labtech.cache.BaseCache.KEY_PREFIX","title":"<code>KEY_PREFIX = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prefix for all files created by this Cache type - should be different for each Cache type to avoid conflicts.</p>"},{"location":"caching/#labtech.cache.BaseCache.save_result","title":"<code>save_result(storage: Storage, task: Task[ResultT], result: ResultT)</code>  <code>abstractmethod</code>","text":"<p>Saves the given task result into the storage provider.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>             (<code>Storage</code>)         \u2013          <p>Storage provider to save the result into</p> </li> <li> <code>task</code>             (<code>Task[ResultT]</code>)         \u2013          <p>task instance the result belongs to</p> </li> <li> <code>result</code>             (<code>ResultT</code>)         \u2013          <p>result to save</p> </li> </ul> Source code in <code>labtech/cache.py</code> <pre><code>@abstractmethod\ndef save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n    \"\"\"Saves the given task result into the storage provider.\n\n    Args:\n        storage: Storage provider to save the result into\n        task: task instance the result belongs to\n        result: result to save\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#labtech.cache.BaseCache.load_result","title":"<code>load_result(storage: Storage, task: Task[ResultT]) -&gt; ResultT</code>  <code>abstractmethod</code>","text":"<p>Loads the result for the given task from the storage provider.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>             (<code>Storage</code>)         \u2013          <p>Storage provider to load the result from</p> </li> <li> <code>task</code>             (<code>Task[ResultT]</code>)         \u2013          <p>task instance to load the result for</p> </li> </ul> Source code in <code>labtech/cache.py</code> <pre><code>@abstractmethod\ndef load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n    \"\"\"Loads the result for the given task from the storage provider.\n\n    Args:\n        storage: Storage provider to load the result from\n        task: task instance to load the result for\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#storage","title":"Storage","text":"<p>You can set the storage location for caching task results by specifying an instance of one of the following Storage classes for the <code>storage</code> argument of your <code>Lab</code>:</p>"},{"location":"caching/#labtech.storage.LocalStorage","title":"<code>labtech.storage.LocalStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Storage provider that stores cached results in a local filesystem directory.</p> Source code in <code>labtech/storage.py</code> <pre><code>class LocalStorage(Storage):\n    \"\"\"Storage provider that stores cached results in a local filesystem\n    directory.\"\"\"\n\n    def __init__(self, storage_dir: Union[str, Path], *, with_gitignore: bool = True):\n        \"\"\"\n        Args:\n            storage_dir: Path to the directory where cached results will be\n                stored. The directory will be created if it does not already\n                exist.\n            with_gitignore: If `True`, a `.gitignore` file will be created\n                inside the storage directory to ignore the entire storage\n                directory. If an existing `.gitignore` file exists, it will be\n                replaced.\n        \"\"\"\n        if isinstance(storage_dir, str):\n            storage_dir = Path(storage_dir)\n        self._storage_path = storage_dir.resolve()\n        if not self._storage_path.exists():\n            self._storage_path.mkdir()\n\n        if with_gitignore:\n            gitignore_path = self._storage_path / '.gitignore'\n            with gitignore_path.open('w') as gitignore_file:\n                gitignore_file.write('*\\n')\n\n    def _key_path(self, key: str) -&gt; Path:\n        key_path = (self._storage_path / key).resolve()\n        if key_path.parent != self._storage_path:\n            raise StorageError((f\"Key '{key}' should only reference a directory directly \"\n                                f\"under the storage directory '{self._storage_path}'\"))\n        return key_path\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return sorted([key_path.name for key_path in self._storage_path.iterdir()])\n\n    def exists(self, key: str) -&gt; bool:\n        key_path = self._key_path(key)\n        return key_path.exists()\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        key_path = self._key_path(key)\n        try:\n            key_path.mkdir()\n        except FileExistsError:\n            pass\n        file_path = (key_path / filename).resolve()\n        if file_path.parent != key_path:\n            raise StorageError((f\"Filename '{filename}' should only reference a directory directly \"\n                                f\"under the storage key directory '{key_path}'\"))\n        return file_path.open(mode=mode)\n\n    def delete(self, key: str):\n        key_path = self._key_path(key)\n        if key_path.exists():\n            shutil.rmtree(key_path)\n</code></pre>"},{"location":"caching/#labtech.storage.LocalStorage.__init__","title":"<code>__init__(storage_dir: Union[str, Path], *, with_gitignore: bool = True)</code>","text":"<p>Parameters:</p> <ul> <li> <code>storage_dir</code>             (<code>Union[str, Path]</code>)         \u2013          <p>Path to the directory where cached results will be stored. The directory will be created if it does not already exist.</p> </li> <li> <code>with_gitignore</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If <code>True</code>, a <code>.gitignore</code> file will be created inside the storage directory to ignore the entire storage directory. If an existing <code>.gitignore</code> file exists, it will be replaced.</p> </li> </ul> Source code in <code>labtech/storage.py</code> <pre><code>def __init__(self, storage_dir: Union[str, Path], *, with_gitignore: bool = True):\n    \"\"\"\n    Args:\n        storage_dir: Path to the directory where cached results will be\n            stored. The directory will be created if it does not already\n            exist.\n        with_gitignore: If `True`, a `.gitignore` file will be created\n            inside the storage directory to ignore the entire storage\n            directory. If an existing `.gitignore` file exists, it will be\n            replaced.\n    \"\"\"\n    if isinstance(storage_dir, str):\n        storage_dir = Path(storage_dir)\n    self._storage_path = storage_dir.resolve()\n    if not self._storage_path.exists():\n        self._storage_path.mkdir()\n\n    if with_gitignore:\n        gitignore_path = self._storage_path / '.gitignore'\n        with gitignore_path.open('w') as gitignore_file:\n            gitignore_file.write('*\\n')\n</code></pre>"},{"location":"caching/#labtech.storage.NullStorage","title":"<code>labtech.storage.NullStorage</code>","text":"<p>             Bases: <code>Storage</code></p> <p>Storage provider that does not store cached results.</p> Source code in <code>labtech/storage.py</code> <pre><code>class NullStorage(Storage):\n    \"\"\"Storage provider that does not store cached results.\"\"\"\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return []\n\n    def exists(self, key: str) -&gt; bool:\n        return False\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        return open(os.devnull, mode=mode)\n\n    def delete(self, key: str):\n        pass\n</code></pre>"},{"location":"caching/#custom-storage","title":"Custom Storage","text":"<p>To store cached results with an alternative storage provider (such as a storage bucket in the cloud), you can define your own type of Storage by inheriting from <code>Storage</code>:</p>"},{"location":"caching/#labtech.storage.Storage","title":"<code>labtech.storage.Storage</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Storage provider for persisting cached task results.</p> Source code in <code>labtech/types.py</code> <pre><code>class Storage(ABC):\n    \"\"\"Storage provider for persisting cached task results.\"\"\"\n\n    @abstractmethod\n    def find_keys(self) -&gt; Sequence[str]:\n        \"\"\"Returns the keys of all currently cached task results.\"\"\"\n\n    @abstractmethod\n    def exists(self, key: str) -&gt; bool:\n        \"\"\"Returns `True` if the given task `key` is present in the storage\n        cache.\"\"\"\n\n    @abstractmethod\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        \"\"\"Opens and returns a File-like object for a single file within the\n        storage cache.\n\n        Args:\n            key: The task key of the cached result containing the file.\n            filename: The name of the file to open.\n            mode: The file mode to open the file with.\n\n        \"\"\"\n\n    @abstractmethod\n    def delete(self, key: str) -&gt; None:\n        \"\"\"Deletes the cached result for the task with the given `key`.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.find_keys","title":"<code>find_keys() -&gt; Sequence[str]</code>  <code>abstractmethod</code>","text":"<p>Returns the keys of all currently cached task results.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef find_keys(self) -&gt; Sequence[str]:\n    \"\"\"Returns the keys of all currently cached task results.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.exists","title":"<code>exists(key: str) -&gt; bool</code>  <code>abstractmethod</code>","text":"<p>Returns <code>True</code> if the given task <code>key</code> is present in the storage cache.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef exists(self, key: str) -&gt; bool:\n    \"\"\"Returns `True` if the given task `key` is present in the storage\n    cache.\"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.file_handle","title":"<code>file_handle(key: str, filename: str, *, mode: str = 'r') -&gt; IO</code>  <code>abstractmethod</code>","text":"<p>Opens and returns a File-like object for a single file within the storage cache.</p> <p>Parameters:</p> <ul> <li> <code>key</code>             (<code>str</code>)         \u2013          <p>The task key of the cached result containing the file.</p> </li> <li> <code>filename</code>             (<code>str</code>)         \u2013          <p>The name of the file to open.</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>'r'</code> )         \u2013          <p>The file mode to open the file with.</p> </li> </ul> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n    \"\"\"Opens and returns a File-like object for a single file within the\n    storage cache.\n\n    Args:\n        key: The task key of the cached result containing the file.\n        filename: The name of the file to open.\n        mode: The file mode to open the file with.\n\n    \"\"\"\n</code></pre>"},{"location":"caching/#labtech.storage.Storage.delete","title":"<code>delete(key: str) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Deletes the cached result for the task with the given <code>key</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>@abstractmethod\ndef delete(self, key: str) -&gt; None:\n    \"\"\"Deletes the cached result for the task with the given `key`.\"\"\"\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":""},{"location":"cookbook/#labtech-cookbook","title":"Labtech Cookbook","text":"<p>The following cookbook presents labtech patterns for common use cases.</p> <p>You can also run this cookbook as an interactive notebook.</p> <pre><code>%pip install labtech fsspec mlflow pandas scikit-learn setuptools\n</code></pre> <pre><code>!mkdir storage\n</code></pre> <pre><code>import labtech\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.base import clone, ClassifierMixin\nfrom sklearn.preprocessing import StandardScaler\n\n# Prepare a dataset for examples\ndigits_X, digits_y = datasets.load_digits(return_X_y=True)\ndigits_X = StandardScaler().fit_transform(digits_X)\n</code></pre>"},{"location":"cookbook/#how-can-i-print-log-messages-from-my-task","title":"How can I print log messages from my task?","text":"<p>Using <code>labtech.logger</code> (a standard Python logger object) is the recommended approach for logging from a task, but all output that is sent to <code>STDOUT</code> (e.g. calls to <code>print()</code>) or <code>STDERR</code> (e.g. uncaught exceptions) will also be captured and logged:</p> <pre><code>@labtech.task\nclass PrintingExperiment:\n    seed: int\n\n    def run(self):\n        labtech.logger.warning(f'Warning, the seed is: {self.seed}')\n        print(f'The seed is: {self.seed}')\n        return self.seed * self.seed\n\n\nexperiments = [\n    PrintingExperiment(\n        seed=seed\n    )\n    for seed in range(5)\n]\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#how-do-i-specify-a-complex-object-like-a-model-or-dataset-as-a-task-parameter","title":"How do I specify a complex object, like a model or dataset, as a task parameter?","text":"<p>Because labtech needs to be able to reconstitute <code>Task</code> objects from caches, task parameters can only be:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Any member of an <code>Enum</code> type.</li> <li>Task types: A task parameter is a \"nested task\" that will be executed   before its parent so that it may make use of the nested result.</li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>,   <code>dict</code>, <code>frozendict</code></li> <li>Note: Mutable <code>list</code> and <code>dict</code> collections will be converted to     immutable <code>tuple</code> and <code>frozendict</code>     collections.</li> </ul> <p>The are three primary patterns you can use to provide a more complex object as a parameter to a task:</p> <ul> <li>Constructing the object in a dependent task</li> <li>Passing the object in an <code>Enum</code> parameter</li> <li>Passing the object in the lab context</li> </ul>"},{"location":"cookbook/#constructing-objects-in-dependent-tasks","title":"Constructing objects in dependent tasks","text":"<p>If your object can be constructed from its own set of parameters, then you can use a dependent task as a \"factory\" to construct your object.</p> <p>For example, you could define a task type to construct a machine learning model (like <code>LRClassifierTask</code> below), and then make a task of that type a parameter for your primary experiment task:</p> <pre><code>from sklearn.linear_model import LogisticRegression\n\n\n# Constructing a classifier object is inexpensive, so we don't need to\n# cache the result\n@labtech.task(cache=None)\nclass LRClassifierTask:\n    random_state: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=self.random_state,\n        )\n\n\n@labtech.task\nclass ClassifierExperiment:\n    classifier_task: LRClassifierTask\n\n    def run(self) -&gt; np.ndarray:\n        # Because the classifier task result may be shared between experiments,\n        # we clone it before fitting.\n        clf = clone(self.classifier_task.result)\n        clf.fit(digits_X, digits_y)\n        return clf.predict_proba(digits_X)\n\n\nexperiment = ClassifierExperiment(\n    classifier_task=LRClassifierTask(random_state=42),\n)\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresults = lab.run_tasks([experiment])\n</code></pre> <p>We can extend this example with additional task types to cater for different types of classifiers with a Protocol that defines their common result type:</p> <pre><code>from typing import Protocol\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\n\nclass ClassifierTask(Protocol):\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(cache=None)\nclass LRClassifierTask:\n    random_state: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=self.random_state,\n        )\n\n\n@labtech.task(cache=None)\nclass NBClassifierTask:\n\n    def run(self) -&gt; ClassifierMixin:\n        return GaussianNB()\n\n\n@labtech.task\nclass ClassifierExperiment:\n    classifier_task: ClassifierTask\n\n    def run(self) -&gt; np.ndarray:\n        # Because the classifier task result may be shared between experiments,\n        # we clone it before fitting.\n        clf = clone(self.classifier_task.result)\n        clf.fit(digits_X, digits_y)\n        return clf.predict_proba(digits_X)\n\n\nclassifier_tasks = [\n    LRClassifierTask(random_state=42),\n    NBClassifierTask(),\n]\nexperiments = [\n    ClassifierExperiment(classifier_task=classifier_task)\n    for classifier_task in classifier_tasks\n]\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#passing-objects-in-enum-parameters","title":"Passing objects in <code>Enum</code> parameters","text":"<p>For simple object parameters that have a fixed set of known values, an <code>Enum</code> of possible values can be used to provide parameter values.</p> <p>The following example shows how an <code>Enum</code> of functions can be used for a parameter to specify the operation that an experiment performs:</p> <p>Note: Because parameter values will be pickled when they are copied to parallel task sub-processes, the type used in a parameter <code>Enum</code> must support equality between identical (but distinct) object instances.</p> <pre><code>from enum import Enum\nfrom datetime import datetime\n\n\n# The custom class we want to provide objects of as parameters.\nclass Dataset:\n\n    def __init__(self, key):\n        self.key = key\n        self.data = datasets.fetch_openml(key, parser='auto').data\n\n    def __eq__(self, other):\n        # Support equality check to allow pickling of Enum values\n        if type(self) != type(other):\n            return False\n        return self.key == other.key\n\n\nclass DatasetOption(Enum):\n    TIC_TAC_TOE=Dataset('tic-tac-toe')\n    EEG_EYE_STATE=Dataset('eeg-eye-state')\n\n\n@labtech.task\nclass DatasetExperiment:\n    dataset: DatasetOption\n\n    def run(self):\n        dataset = self.dataset.value\n        return dataset.data.shape\n\n\nexperiments = [\n    DatasetExperiment(\n        dataset=dataset\n    )\n    for dataset in DatasetOption\n]\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#passing-objects-in-the-lab-context","title":"Passing objects in the lab context","text":"<p>If an object cannot be conveniently defined in an <code>Enum</code> (such as types like Numpy arrays or Pandas DataFrames that cannot be directly specified as an <code>Enum</code> value, or large values that cannot all be loaded into memory every time the <code>Enum</code> is loaded), then the lab context can be used to pass the object to a task.</p> <p>Warning: Because values provided in the lab context are not cached, they should be kept constant between runs or should not affect task results (e.g. parallel worker counts, log levels). If changing context values cause task results to change, then cached results may no longer be valid.</p> <p>The following example demonstrates specifying a <code>dataset_key</code> parameter to a task that is used to look up a dataset from the lab context:</p> <pre><code>DATASETS = {\n    'zeros': np.zeros((50, 10)),\n    'ones': np.ones((50, 10)),\n}\n\n\n@labtech.task\nclass SumExperiment:\n    dataset_key: str\n\n    def run(self):\n        dataset = self.context['DATASETS'][self.dataset_key]\n        return np.sum(dataset)\n\n\nexperiments = [\n    SumExperiment(\n        dataset_key=dataset_key\n    )\n    for dataset_key in DATASETS.keys()\n]\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n    context={\n        'DATASETS': DATASETS,\n    },\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#how-can-i-control-multi-processing-myself-within-a-task","title":"How can I control multi-processing myself within a task?","text":"<p>By default, Labtech executes tasks in parallel on all available CPU cores. However, you can control multi-processing yourself by disabling task parallelism and performing your own parallelism within a task's <code>run()</code> method.</p> <p>The following example uses <code>max_parallel</code> to allow only one <code>CVExperiment</code> to be executed at a time, and then performs cross-validation within the task using a number of workers specified in the lab context as <code>within_task_workers</code>:</p> <pre><code>from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\n\n\n@labtech.task(max_parallel=1)\nclass CVExperiment:\n    cv_folds: int\n\n    def run(self):\n        clf = GaussianNB()\n        return cross_val_score(\n            clf,\n            digits_X,\n            digits_y,\n            cv=self.cv_folds,\n            n_jobs=self.context['within_task_workers'],\n        )\n\n\nexperiments = [\n    CVExperiment(\n        cv_folds=cv_folds\n    )\n    for cv_folds in [5, 10]\n]\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n    context={\n        'within_task_workers': 4,\n    },\n)\nresults = lab.run_tasks(experiments)\n</code></pre> <p>Note: Instead of limiting parallelism for a single task type by specifying <code>max_parallel</code> in the <code>@labtech.task</code> decorator, you can limit parallelism across all tasks with <code>max_workers</code> when constructing a <code>labtech.Lab</code>.</p> <p>Note: The <code>joblib</code> library used by <code>sklearn</code> does not behave correctly when run from within a task sub-process, but setting <code>max_parallel=1</code> or <code>max_workers=1</code> ensures tasks are run inside the main process.</p>"},{"location":"cookbook/#how-can-i-make-labtech-continue-executing-tasks-even-when-one-or-more-fail","title":"How can I make labtech continue executing tasks even when one or more fail?","text":"<p>Labtech's default behaviour is to stop executing any new tasks as soon as any individual task fails. However, when executing tasks over a long period of time (e.g. a large number of tasks, or even a few long running tasks), it is sometimes helpful to have labtech continue to execute tasks even if one or more fail.</p> <p>If you set <code>continue_on_failure=True</code> when creating your lab, exceptions raised during the execution of a task will be logged, but the execution of other tasks will continue:</p> <pre><code>lab = labtech.Lab(\n    storage=None,\n    continue_on_failure=True,\n)\n</code></pre>"},{"location":"cookbook/#what-happens-to-my-cached-results-if-i-change-or-move-the-definition-of-a-task","title":"What happens to my cached results if I change or move the definition of a task?","text":"<p>A task's cache will store all details necessary to reinstantiate the task object, including the qualified name of the task's class and all of the task's parameters. Because of this, it is best not to change the parameters and location of a task's definition once you are seriously relying on cached results.</p> <p>If you need to add a new parameter or behaviour to an existing task type for which you have previously cached results, consider defining a sub-class for that extension so that you can continue using caches for the base class:</p> <pre><code>@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\n\n@labtech.task\nclass ExtendedExperiment(Experiment):\n    multiplier: int\n\n    def run(self):\n        base_result = super().run()\n        return base_result * self.multiplier\n</code></pre>"},{"location":"cookbook/#how-can-i-find-what-results-i-have-cached","title":"How can I find what results I have cached?","text":"<p>You can use the <code>cached_task()</code> method of a <code>Lab</code> instance to retrieve all cached task instances for a list of task types. You can then \"run\" the tasks to load their cached results:</p> <pre><code>cached_cvexperiment_tasks = lab.cached_tasks([CVExperiment])\nresults = lab.run_tasks(cached_cvexperiment_tasks)\n</code></pre>"},{"location":"cookbook/#how-can-i-clear-cached-results","title":"How can I clear cached results?","text":"<p>You can clear the cache for a list of tasks using the <code>uncache_tasks()</code> method of a <code>Lab</code> instance:</p> <pre><code>lab.uncache_tasks(cached_cvexperiment_tasks)\n</code></pre> <p>You can also ignore all previously cached results when running a list of tasks by passing the <code>bust_cache</code> option to <code>run_tasks()</code>:</p> <pre><code>lab.run_tasks(cached_cvexperiment_tasks, bust_cache=True)\n</code></pre>"},{"location":"cookbook/#what-types-of-values-should-my-tasks-return-to-be-cached","title":"What types of values should my tasks return to be cached?","text":"<p>While you can define your task to return any Python object you like to be cached, one generally useful approach is to return a dictionary comprised of built-in (e.g. lists, strings, numbers) or otherwise standard data types (e.g. arrays, dataframes). This is for two reasons:</p> <ol> <li>If the returned dictionary needs to be extended to include    additional keys, it will often be straightforward to adapt code    that uses task results to safely continue using previously cached    results that do not contain those keys.</li> <li>Using custom objects (such as dataclasses) could cause    issues when loading cached objects if the definition of the    class ever changes.</li> </ol> <p>If you want to keep the typing benefits of a custom dataclass, you can consider using a <code>TypeDict</code>:</p> <pre><code>from typing import TypedDict, NotRequired\n\n\nclass MyTaskResult(TypedDict):\n    predictions: np.ndarray\n    # Key added in a later version of the task. Requires Python &gt;= 3.11.\n    model_weights: NotRequired[np.ndarray]\n\n\n@labtech.task\nclass ExampleTask:\n    seed: int\n\n    def run(self):\n        return MyTaskResult(\n            predictions=np.array([1, 2, 3]),\n            model_weights=np.array([self.seed, self.seed ** 2]),\n        )\n</code></pre>"},{"location":"cookbook/#how-can-i-cache-task-results-in-a-format-other-than-pickle","title":"How can I cache task results in a format other than pickle?","text":"<p>You can define your own cache type to support storing cached results in a format other than pickle. To do so, you must define a class that extends <code>labtech.cache.BaseCache</code> and defines <code>KEY_PREFIX</code>, <code>save_result()</code>, and <code>load_result()</code>. You can then configure any task type by passing an instance of your new cache type for the <code>cache</code> option of the <code>@labtech.task</code> decorator.</p> <p>The following example demonstrates defining and using a custom cache type to store Pandas DataFrames as parquet files:</p> <pre><code>from labtech.cache import BaseCache\nfrom labtech.types import Task, ResultT\nfrom labtech.storage import Storage\nfrom typing import Any\nimport pandas as pd\n\n\nclass ParquetCache(BaseCache):\n    \"\"\"Caches a Pandas DataFrame result as a parquet file.\"\"\"\n    KEY_PREFIX = 'parquet__'\n\n    def save_result(self, storage: Storage, task: Task[ResultT], result: ResultT):\n        if not isinstance(result, pd.DataFrame):\n            raise ValueError('ParquetCache can only cache DataFrames')\n        with storage.file_handle(task.cache_key, 'result.parquet', mode='wb') as data_file:\n            result.to_parquet(data_file)\n\n    def load_result(self, storage: Storage, task: Task[ResultT]) -&gt; ResultT:\n        with storage.file_handle(task.cache_key, 'result.parquet', mode='rb') as data_file:\n            return pd.read_parquet(data_file)\n\n\n@labtech.task(cache=ParquetCache())\nclass TabularTask:\n\n    def run(self):\n        return pd.DataFrame({\n            'x': [1, 2, 3],\n            'y': [1, 4, 9],\n        })\n\n\nlab = labtech.Lab(\n    storage='storage/parquet_example',\n    notebook=True,\n)\nlab.run_tasks([TabularTask()])\n</code></pre>"},{"location":"cookbook/#how-can-i-cache-task-results-somewhere-other-than-my-filesystem","title":"How can I cache task results somewhere other than my filesystem?","text":"<p>You can cache results in a location other than the local filesystem by defining your own storage type that extends <code>labtech.storage.Storage</code> and defines <code>find_keys()</code>, <code>exists()</code>, <code>file_handle()</code> and <code>delete()</code>. You can then pass an instance of your new storage backend for the <code>storage</code> option when constructing a <code>Lab</code> instance.</p> <p>The following example demonstrates constructing a storage backend to interface to the <code>LocalFileSystem</code> provided by the <code>fsspec</code> library. This example could be adapted for other <code>fsspec</code> implementations, such as cloud storage providers like Amazon S3 and Azure Blob Storage:</p> <pre><code>from labtech.storage import Storage\nfrom typing import IO, Sequence\nfrom pathlib import Path\nfrom fsspec.implementations.local import LocalFileSystem\n\n\nclass LocalFsspecStorage(Storage):\n    \"\"\"Store results in the local file filesystem.\"\"\"\n\n    def __init__(self, storage_dir):\n        self.storage_dir = Path(storage_dir).resolve()\n        fs = self._fs('w')\n        fs.mkdirs(self.storage_dir, exist_ok=True)\n\n    def _fs(self, mode):\n        return LocalFileSystem()\n\n    def _key_to_path(self, key):\n        return self.storage_dir / key\n\n    def find_keys(self) -&gt; Sequence[str]:\n        return [str(Path(entry).relative_to(self.storage_dir)) for entry in self._fs('r').ls(self.storage_dir)]\n\n    def exists(self, key: str) -&gt; bool:\n        return self._fs('r').exists(self._key_to_path(key))\n\n    def file_handle(self, key: str, filename: str, *, mode: str = 'r') -&gt; IO:\n        fs_mode = 'w' if 'w' in mode else 'r'\n        fs = self._fs(fs_mode)\n        key_path = self._key_to_path(key)\n        fs.mkdirs(key_path, exist_ok=True)\n        file_path = (key_path / filename).resolve()\n        if file_path.parent != key_path:\n            raise ValueError((f\"Filename '{filename}' should only reference a directory directly \"\n                              f\"under the storage key directory '{key_path}'\"))\n        return fs.open(file_path, mode)\n\n    def delete(self, key: str):\n        fs = self._fs('w')\n        path = self._key_to_path(key)\n        if fs.exists(path):\n            fs.rm(path, recursive=True)\n\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\n\nexperiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(100)\n]\nlab = labtech.Lab(\n    storage=LocalFsspecStorage('storage/fsspec_example'),\n    notebook=True,\n)\nresults = lab.run_tasks(experiments)\n</code></pre>"},{"location":"cookbook/#loading-lots-of-cached-results-is-slow-how-can-i-make-it-faster","title":"Loading lots of cached results is slow, how can I make it faster?","text":"<p>If you have a large number of tasks, you may find that the overhead of loading each individual task result from the cache is unacceptably slow when you need to frequently reload previous results for analysis.</p> <p>In such cases, you may find it helpful to create a final task that depends on all of your individual tasks and aggregates all of their results into a single cached result. Note that this final result cache will need to be rebuilt whenever any of its dependent tasks changes or new dependent tasks are added. Furthermore, this approach will require additional storage for the final cache in addition to the individual result caches.</p> <p>The following example demonstrates defining and using an <code>AggregationTask</code> to aggregate the results from many individual tasks to create an aggregated cache that can be loaded more efficiently:</p> <pre><code>from labtech.types import Task\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\n\n@labtech.task\nclass AggregationTask:\n    sub_tasks: list[Task]\n\n    def run(self):\n        return [\n            sub_task.result\n            for sub_task in self.sub_tasks\n        ]\n\n\nexperiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(1000)\n]\naggregation_task = AggregationTask(\n    sub_tasks=experiments,\n)\nlab = labtech.Lab(\n    storage='storage/aggregation_lab',\n    notebook=True,\n)\nresult = lab.run_task(aggregation_task)\n</code></pre>"},{"location":"cookbook/#how-can-i-see-when-a-task-was-run-and-how-long-it-took-to-execute","title":"How can I see when a task was run and how long it took to execute?","text":"<p>Once a task has been executed (or loaded from cache), you can see when it was originally executed and how long it took to execute from the task's <code>.result_meta</code> attribute:</p> <pre><code>print(f'The task was executed at: {aggregation_task.result_meta.start}')\nprint(f'The task execution took: {aggregation_task.result_meta.duration}')\n</code></pre>"},{"location":"cookbook/#how-can-i-access-the-results-of-intermediatedependency-tasks","title":"How can I access the results of intermediate/dependency tasks?","text":"<p>To conserve memory, labtech's default behaviour is to unload the results of intermediate/dependency tasks once their directly dependent tasks have finished executing.</p> <p>A simple approach to access the results of an intermediate task may simply be to include it's results as part of the result of the task that depends on it - that way you only need to look at the results of the final task(s).</p> <p>Another approach is to include all of the intermediate tasks for which you wish to access the results for in the call to <code>run_tasks()</code>:</p> <pre><code>experiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(10)\n]\naggregation_task = AggregationTask(\n    sub_tasks=experiments,\n)\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresults = lab.run_tasks([\n    aggregation_task,\n    # Include intermediate tasks to access their results\n    *experiments,\n])\nprint([\n    results[experiment]\n    for experiment in experiments\n])\n</code></pre> <p>You can also configure labtech to not remove intermediate results from memory by setting <code>keep_nested_results=True</code> when calling <code>run_tasks()</code>. Intermediate results can then be accessed from the <code>.result</code> attribute of all task objects. However, only results that needed to be executed or loaded from cache in order to produce the final result will be available, so you may need to set <code>bust_cache=True</code> to ensure all intermediate tasks are executed:</p> <pre><code>experiments = [\n    Experiment(\n        seed=seed\n    )\n    for seed in range(10)\n]\naggregation_task = AggregationTask(\n    sub_tasks=experiments,\n)\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresult = lab.run_task(\n    aggregation_task,\n    keep_nested_results=True,\n    bust_cache=True,\n)\nprint([\n    experiment.result\n    for experiment in experiments\n])\n</code></pre>"},{"location":"cookbook/#how-can-i-construct-a-multi-step-experiment-pipeline","title":"How can I construct a multi-step experiment pipeline?","text":"<p>Say you want to model a multi-step experiment pipeline, where <code>StepA</code> is run before <code>StepB</code>, which is run before <code>StepC</code>:</p> <pre><code>StepA -&gt; StepB -&gt; StepC\n</code></pre> <p>This is modeled in labtech by defining a task type for each step, and having each step depend on the result from the previous step:</p> <pre><code>@labtech.task\nclass StepA:\n    seed_a: int\n\n    def run(self):\n        return self.seed_a\n\n\n@labtech.task\nclass StepB:\n    task_a: StepA\n    seed_b: int\n\n    def run(self):\n        return self.task_a.result * self.seed_b\n\n\n@labtech.task\nclass StepC:\n    task_b: StepB\n    seed_c: int\n\n    def run(self):\n        return self.task_b.result * self.seed_c\n\n\ntask_a = StepA(\n    seed_a=2,\n)\ntask_b = StepB(\n    seed_b=3,\n    task_a=task_a,\n)\ntask_c = StepC(\n    seed_c=5,\n    task_b=task_b,\n)\n\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresult = lab.run_task(task_c)\nprint(result)\n</code></pre>"},{"location":"cookbook/#how-can-i-visualise-my-task-types-including-their-parameters-and-dependencies","title":"How can I visualise my task types, including their parameters and dependencies?","text":"<p><code>labtech.diagram.display_task_diagram()</code> can be used to display a Mermaid diagram of task types for a given list of tasks:</p> <pre><code>from labtech.diagram import display_task_diagram\n\ndisplay_task_diagram(\n    [task_c],\n    direction='RL',\n)\n</code></pre> <p><code>labtech.diagram.build_task_diagram()</code> can be similarly used to return the Mermaid syntax for the diagram.</p>"},{"location":"cookbook/#how-can-i-use-labtech-with-mlflow","title":"How can I use labtech with mlflow?","text":"<p>If you want to log a task type as an mlflow \"run\", simply add <code>mlflow_run=True</code> to the call to <code>@labtech.task()</code>, which will:</p> <ul> <li>Wrap each run of the task with <code>mlflow.start_run()</code></li> <li>Tag the run with <code>labtech_task_type</code> equal to the task class name</li> <li>Log all task parameters with <code>mlflow.log_param()</code></li> </ul> <p>The following example demonstrates using labtech with mlflow. Note that you can still make any configuration changes (such as <code>mlflow.set_experiment()</code>) before the tasks are run, and you can make additional tracking calls (such as <code>mlflow.log_metric()</code> or <code>mlflow.log_model()</code>) in the body of your task's <code>run()</code> method:</p> <pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\n@labtech.task(mlflow_run=True)\nclass MLRun:\n    penalty_norm: str | None\n\n    def run(self) -&gt; np.ndarray:\n        clf = LogisticRegression(penalty=self.penalty_norm)\n        clf.fit(digits_X, digits_y)\n\n        labels = clf.predict(digits_X)\n\n        train_accuracy = accuracy_score(digits_y, labels)\n        mlflow.log_metric('train_accuracy', train_accuracy)\n\n        mlflow.sklearn.log_model(\n            sk_model=clf,\n            artifact_path='digits_model',\n            input_example=digits_X,\n            registered_model_name='digits-model',\n        )\n\n        return labels\n\n\nruns = [\n    MLRun(\n        penalty_norm=penalty_norm,\n    )\n    for penalty_norm in [None, 'l2']\n]\n\nmlflow.set_experiment('example_labtech_experiment')\nlab = labtech.Lab(\n    storage=None,\n    notebook=True,\n)\nresults = lab.run_tasks(runs)\n</code></pre> <p>Note: While the mlflow documentation recommends wrapping only your tracking code with <code>mlflow.start_run()</code>, labtech wraps the entire call to the <code>run()</code> method of your task in order to track execution times in mlflow.</p>"},{"location":"cookbook/#why-do-i-see-the-following-error-an-attempt-has-been-made-to-start-a-new-process-before-the-current-process-has-finished","title":"Why do I see the following error: <code>An attempt has been made to start a new process before the current process has finished</code>?","text":"<p>When running labtech in a Python script on Windows, macOS, or any Python environment using the <code>spawn</code> multiprocessing start method, you will see the following error if you do not guard your experiment and lab creation and other non-definition code with <code>__name__ == '__main__'</code>:</p> <pre><code>RuntimeError:\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n</code></pre> <p>To avoid this error, it is recommended that you write all of your non-definition code for a Python script in a <code>main()</code> function, and then guard the call to <code>main()</code> with <code>__name__ == '__main__'</code>:</p> <pre><code>import labtech\n\n@labtech.task\nclass Experiment:\n    seed: int\n\n    def run(self):\n        return self.seed * self.seed\n\ndef main():\n    experiments = [\n        Experiment(\n            seed=seed\n        )\n        for seed in range(1000)\n    ]\n    lab = labtech.Lab(\n        storage='storage/guarded_lab',\n        notebook=True,\n    )\n    result = lab.run_tasks(experiments)\n    print(result)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>For details, see Safe importing of main module.</p>"},{"location":"core/","title":"Labs and Tasks","text":"<p>In this document you'll find API reference documentation for configuring Labs and tasks. For a tutorial-style explanation, see the README.</p> <p>Any task type decorated with <code>labtech.task</code> will provide the following attributes and methods:</p>"},{"location":"core/#labtech.Lab","title":"<code>labtech.Lab</code>","text":"<p>Primary interface for configuring, running, and getting results of tasks.</p> <p>A Lab can be used to run tasks with <code>run_tasks()</code>.</p> <p>Previously cached tasks can be retrieved with <code>cached_tasks()</code>, and can then have their results loaded with <code>run_tasks()</code> or be removed from the cache storage with <code>uncache_tasks()</code>.</p> Source code in <code>labtech/lab.py</code> <pre><code>class Lab:\n    \"\"\"Primary interface for configuring, running, and getting results of tasks.\n\n    A Lab can be used to run tasks with [`run_tasks()`][labtech.Lab.run_tasks].\n\n    Previously cached tasks can be retrieved with\n    [`cached_tasks()`][labtech.Lab.cached_tasks], and can then have\n    their results loaded with [`run_tasks()`][labtech.Lab.run_tasks]\n    or be removed from the cache storage with\n    [`uncache_tasks()`][labtech.Lab.uncache_tasks].\n\n    \"\"\"\n\n    def __init__(self, *,\n                 storage: Union[str, Path, None, Storage],\n                 continue_on_failure: bool = True,\n                 max_workers: Optional[int] = None,\n                 notebook: bool = False,\n                 context: Optional[dict[str, Any]] = None):\n        \"\"\"\n        Args:\n            storage: Where task results should be cached to. A string or\n                [`Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n                will be interpreted as the path to a local directory, `None`\n                will result in no caching. Any [Storage][labtech.types.Storage]\n                instance may also be specified.\n            continue_on_failure: If `True`, exceptions raised by tasks will be\n                logged, but execution of other tasks will continue.\n            max_workers: The maximum number of parallel worker processes for\n                running tasks. Uses the same default as\n                `concurrent.futures.ProcessPoolExecutor`: the number of\n                processors on the machine. When `max_workers=1`, all tasks will\n                be run in the main process, without multi-processing.\n            notebook: Should be set to `True` if run from a Jupyter notebook\n                for graphical progress bars.\n            context: A dictionary of additional variables to make available to\n                tasks. The context will not be cached, so the values should not\n                affect results (e.g. parallelism factors) or should be kept\n                constant between runs (e.g. datasets).\n        \"\"\"\n        if isinstance(storage, str) or isinstance(storage, Path):\n            storage = LocalStorage(storage)\n        elif storage is None:\n            storage = NullStorage()\n        self._storage = storage\n        self.continue_on_failure = continue_on_failure\n        self.max_workers = max_workers\n        self.notebook = notebook\n        if context is None:\n            context = {}\n        self.context = context\n\n    def run_tasks(self, tasks: Sequence[TaskT], *,\n                  bust_cache: bool = False,\n                  keep_nested_results: bool = False,\n                  disable_progress: bool = False) -&gt; Dict[TaskT, Any]:\n        \"\"\"Run the given tasks with as much process parallelism as possible.\n        Loads task results from the cache storage where possible and\n        caches results of executed tasks.\n\n        Any attribute of a task that is itself a task object is\n        considered a \"nested task\", and will be executed or loaded so\n        that it's result is made available to its parent task. If the\n        same task is nested inside multiple task objects, it will only\n        be executed/loaded once.\n\n        As well as returning the results, each task's result will be\n        assigned to a `result` attribute on the task itself (including\n        nested tasks when `keep_nested_results` is `True`).\n\n        Args:\n            tasks: The tasks to execute. Each should be an instance of a class\n                decorated with [`labtech.task`][labtech.task].\n            bust_cache: If `True`, no task results will be loaded from the\n                cache storage; all tasks will be re-executed.\n            keep_nested_results: If `False`, results of nested tasks that were\n                executed or loaded in order to complete the provided tasks will\n                be cleared from memory once they are no longer needed.\n            disable_progress: If `True`, do not display a tqdm progress bar\n                tracking task execution.\n\n        Returns:\n            A dictionary mapping each of the provided tasks to its\n                corresponding result.\n\n        \"\"\"\n        check_tasks(tasks)\n        runner = TaskRunner(self,\n                            bust_cache=bust_cache,\n                            keep_nested_results=keep_nested_results,\n                            disable_progress=disable_progress)\n        results = runner.run(tasks)\n        # Return results in the same order as tasks\n        return {task: results[task] for task in tasks}\n\n    def run_task(self, task: Task[ResultT], **kwargs) -&gt; ResultT:\n        \"\"\"Run a single task and return its result. Supports the same keyword\n        arguments as `run_tasks`.\n\n        NOTE: If you have many tasks to run, you should use\n        `run_tasks` instead to parallelise their execution.\n\n        Returns:\n            The result of the given task.\n\n        \"\"\"\n        results = self.run_tasks([task], **kwargs)\n        return results[task]\n\n    def cached_tasks(self, task_types: Sequence[Type[TaskT]]) -&gt; Sequence[TaskT]:\n        \"\"\"Returns all task instances present in the Lab's cache storage for\n        the given `task_types`, each of which should be a task class\n        decorated with [`labtech.task`][labtech.task].\n\n        Does not load task results from the cache storage, but they\n        can be loaded by calling\n        [`run_tasks()`][labtech.Lab.run_tasks] with the returned task\n        instances.\n\n        \"\"\"\n        check_task_types(task_types)\n        keys = self._storage.find_keys()\n        tasks = []\n        for key in keys:\n            for task_type in task_types:\n                try:\n                    task = task_type._lt.cache.load_task(self._storage, task_type, key)\n                except TaskNotFound:\n                    pass\n                else:\n                    tasks.append(task)\n                    break\n        return tasks\n\n    def is_cached(self, task: Task) -&gt; bool:\n        \"\"\"Checks if a result is present for given task in the Lab's cache\n        storage.\"\"\"\n        check_tasks([task])\n        return task._lt.cache.is_cached(self._storage, task)\n\n    def uncache_tasks(self, tasks: Sequence[Task]):\n        \"\"\"Removes cached results for the given tasks from the Lab's cache\n        storage.\"\"\"\n        check_tasks(tasks)\n        for task in tasks:\n            if self.is_cached(task):\n                task._lt.cache.delete(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.Lab.__init__","title":"<code>__init__(*, storage: Union[str, Path, None, Storage], continue_on_failure: bool = True, max_workers: Optional[int] = None, notebook: bool = False, context: Optional[dict[str, Any]] = None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>storage</code>             (<code>Union[str, Path, None, Storage]</code>)         \u2013          <p>Where task results should be cached to. A string or <code>Path</code> will be interpreted as the path to a local directory, <code>None</code> will result in no caching. Any Storage instance may also be specified.</p> </li> <li> <code>continue_on_failure</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If <code>True</code>, exceptions raised by tasks will be logged, but execution of other tasks will continue.</p> </li> <li> <code>max_workers</code>             (<code>Optional[int]</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of parallel worker processes for running tasks. Uses the same default as <code>concurrent.futures.ProcessPoolExecutor</code>: the number of processors on the machine. When <code>max_workers=1</code>, all tasks will be run in the main process, without multi-processing.</p> </li> <li> <code>notebook</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Should be set to <code>True</code> if run from a Jupyter notebook for graphical progress bars.</p> </li> <li> <code>context</code>             (<code>Optional[dict[str, Any]]</code>, default:                 <code>None</code> )         \u2013          <p>A dictionary of additional variables to make available to tasks. The context will not be cached, so the values should not affect results (e.g. parallelism factors) or should be kept constant between runs (e.g. datasets).</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def __init__(self, *,\n             storage: Union[str, Path, None, Storage],\n             continue_on_failure: bool = True,\n             max_workers: Optional[int] = None,\n             notebook: bool = False,\n             context: Optional[dict[str, Any]] = None):\n    \"\"\"\n    Args:\n        storage: Where task results should be cached to. A string or\n            [`Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n            will be interpreted as the path to a local directory, `None`\n            will result in no caching. Any [Storage][labtech.types.Storage]\n            instance may also be specified.\n        continue_on_failure: If `True`, exceptions raised by tasks will be\n            logged, but execution of other tasks will continue.\n        max_workers: The maximum number of parallel worker processes for\n            running tasks. Uses the same default as\n            `concurrent.futures.ProcessPoolExecutor`: the number of\n            processors on the machine. When `max_workers=1`, all tasks will\n            be run in the main process, without multi-processing.\n        notebook: Should be set to `True` if run from a Jupyter notebook\n            for graphical progress bars.\n        context: A dictionary of additional variables to make available to\n            tasks. The context will not be cached, so the values should not\n            affect results (e.g. parallelism factors) or should be kept\n            constant between runs (e.g. datasets).\n    \"\"\"\n    if isinstance(storage, str) or isinstance(storage, Path):\n        storage = LocalStorage(storage)\n    elif storage is None:\n        storage = NullStorage()\n    self._storage = storage\n    self.continue_on_failure = continue_on_failure\n    self.max_workers = max_workers\n    self.notebook = notebook\n    if context is None:\n        context = {}\n    self.context = context\n</code></pre>"},{"location":"core/#labtech.Lab.run_tasks","title":"<code>run_tasks(tasks: Sequence[TaskT], *, bust_cache: bool = False, keep_nested_results: bool = False, disable_progress: bool = False) -&gt; Dict[TaskT, Any]</code>","text":"<p>Run the given tasks with as much process parallelism as possible. Loads task results from the cache storage where possible and caches results of executed tasks.</p> <p>Any attribute of a task that is itself a task object is considered a \"nested task\", and will be executed or loaded so that it's result is made available to its parent task. If the same task is nested inside multiple task objects, it will only be executed/loaded once.</p> <p>As well as returning the results, each task's result will be assigned to a <code>result</code> attribute on the task itself (including nested tasks when <code>keep_nested_results</code> is <code>True</code>).</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>             (<code>Sequence[TaskT]</code>)         \u2013          <p>The tasks to execute. Each should be an instance of a class decorated with <code>labtech.task</code>.</p> </li> <li> <code>bust_cache</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>True</code>, no task results will be loaded from the cache storage; all tasks will be re-executed.</p> </li> <li> <code>keep_nested_results</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>False</code>, results of nested tasks that were executed or loaded in order to complete the provided tasks will be cleared from memory once they are no longer needed.</p> </li> <li> <code>disable_progress</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>True</code>, do not display a tqdm progress bar tracking task execution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[TaskT, Any]</code>         \u2013          <p>A dictionary mapping each of the provided tasks to its corresponding result.</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def run_tasks(self, tasks: Sequence[TaskT], *,\n              bust_cache: bool = False,\n              keep_nested_results: bool = False,\n              disable_progress: bool = False) -&gt; Dict[TaskT, Any]:\n    \"\"\"Run the given tasks with as much process parallelism as possible.\n    Loads task results from the cache storage where possible and\n    caches results of executed tasks.\n\n    Any attribute of a task that is itself a task object is\n    considered a \"nested task\", and will be executed or loaded so\n    that it's result is made available to its parent task. If the\n    same task is nested inside multiple task objects, it will only\n    be executed/loaded once.\n\n    As well as returning the results, each task's result will be\n    assigned to a `result` attribute on the task itself (including\n    nested tasks when `keep_nested_results` is `True`).\n\n    Args:\n        tasks: The tasks to execute. Each should be an instance of a class\n            decorated with [`labtech.task`][labtech.task].\n        bust_cache: If `True`, no task results will be loaded from the\n            cache storage; all tasks will be re-executed.\n        keep_nested_results: If `False`, results of nested tasks that were\n            executed or loaded in order to complete the provided tasks will\n            be cleared from memory once they are no longer needed.\n        disable_progress: If `True`, do not display a tqdm progress bar\n            tracking task execution.\n\n    Returns:\n        A dictionary mapping each of the provided tasks to its\n            corresponding result.\n\n    \"\"\"\n    check_tasks(tasks)\n    runner = TaskRunner(self,\n                        bust_cache=bust_cache,\n                        keep_nested_results=keep_nested_results,\n                        disable_progress=disable_progress)\n    results = runner.run(tasks)\n    # Return results in the same order as tasks\n    return {task: results[task] for task in tasks}\n</code></pre>"},{"location":"core/#labtech.Lab.run_task","title":"<code>run_task(task: Task[ResultT], **kwargs) -&gt; ResultT</code>","text":"<p>Run a single task and return its result. Supports the same keyword arguments as <code>run_tasks</code>.</p> <p>NOTE: If you have many tasks to run, you should use <code>run_tasks</code> instead to parallelise their execution.</p> <p>Returns:</p> <ul> <li> <code>ResultT</code>         \u2013          <p>The result of the given task.</p> </li> </ul> Source code in <code>labtech/lab.py</code> <pre><code>def run_task(self, task: Task[ResultT], **kwargs) -&gt; ResultT:\n    \"\"\"Run a single task and return its result. Supports the same keyword\n    arguments as `run_tasks`.\n\n    NOTE: If you have many tasks to run, you should use\n    `run_tasks` instead to parallelise their execution.\n\n    Returns:\n        The result of the given task.\n\n    \"\"\"\n    results = self.run_tasks([task], **kwargs)\n    return results[task]\n</code></pre>"},{"location":"core/#labtech.Lab.cached_tasks","title":"<code>cached_tasks(task_types: Sequence[Type[TaskT]]) -&gt; Sequence[TaskT]</code>","text":"<p>Returns all task instances present in the Lab's cache storage for the given <code>task_types</code>, each of which should be a task class decorated with <code>labtech.task</code>.</p> <p>Does not load task results from the cache storage, but they can be loaded by calling <code>run_tasks()</code> with the returned task instances.</p> Source code in <code>labtech/lab.py</code> <pre><code>def cached_tasks(self, task_types: Sequence[Type[TaskT]]) -&gt; Sequence[TaskT]:\n    \"\"\"Returns all task instances present in the Lab's cache storage for\n    the given `task_types`, each of which should be a task class\n    decorated with [`labtech.task`][labtech.task].\n\n    Does not load task results from the cache storage, but they\n    can be loaded by calling\n    [`run_tasks()`][labtech.Lab.run_tasks] with the returned task\n    instances.\n\n    \"\"\"\n    check_task_types(task_types)\n    keys = self._storage.find_keys()\n    tasks = []\n    for key in keys:\n        for task_type in task_types:\n            try:\n                task = task_type._lt.cache.load_task(self._storage, task_type, key)\n            except TaskNotFound:\n                pass\n            else:\n                tasks.append(task)\n                break\n    return tasks\n</code></pre>"},{"location":"core/#labtech.Lab.is_cached","title":"<code>is_cached(task: Task) -&gt; bool</code>","text":"<p>Checks if a result is present for given task in the Lab's cache storage.</p> Source code in <code>labtech/lab.py</code> <pre><code>def is_cached(self, task: Task) -&gt; bool:\n    \"\"\"Checks if a result is present for given task in the Lab's cache\n    storage.\"\"\"\n    check_tasks([task])\n    return task._lt.cache.is_cached(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.Lab.uncache_tasks","title":"<code>uncache_tasks(tasks: Sequence[Task])</code>","text":"<p>Removes cached results for the given tasks from the Lab's cache storage.</p> Source code in <code>labtech/lab.py</code> <pre><code>def uncache_tasks(self, tasks: Sequence[Task]):\n    \"\"\"Removes cached results for the given tasks from the Lab's cache\n    storage.\"\"\"\n    check_tasks(tasks)\n    for task in tasks:\n        if self.is_cached(task):\n            task._lt.cache.delete(self._storage, task)\n</code></pre>"},{"location":"core/#labtech.task","title":"<code>labtech.task(*args, cache: Union[CacheDefault, None, Cache] = CACHE_DEFAULT, max_parallel: Optional[int] = None, mlflow_run: bool = False)</code>","text":"<p>Class decorator for defining task type classes.</p> <p>Task types are frozen [<code>dataclasses</code>], and attribute definitions should capture all parameters of the task type. Parameter attributes can be any of the following types:</p> <ul> <li>Simple scalar types: <code>str</code>, <code>bool</code>, <code>float</code>, <code>int</code>, <code>None</code></li> <li>Any member of an <code>Enum</code> type. Referring to members of an <code>Enum</code> can be   used to parameterise a task with a value that does not have one of the   types above (e.g. a Pandas/Numpy dataset).</li> <li>Task types: A task parameter is a \"nested task\" that will be executed   before its parent so that it may make use of the nested result.</li> <li>Collections of any of these types: <code>list</code>, <code>tuple</code>,   <code>dict</code>, <code>frozendict</code></li> <li>Dictionaries may only contain string keys.</li> <li>Note: Mutable <code>list</code> and <code>dict</code> collections will be converted to     immutable <code>tuple</code> and <code>frozendict</code>     collections.</li> </ul> <p>The task type is expected to define a <code>run()</code> method that takes no arguments (other than <code>self</code>). The <code>run()</code> method should execute the task as parameterised by the task's attributes and return the result of the task.</p> <p>Example usage:</p> <pre><code>@labtech.task\nclass Experiment:\n    seed: int\n    multiplier: int\n\n    def run(self):\n        return self.seed * self.multiplier.value\n\nexperiment = Experiment(seed=1, multiplier=2)\n</code></pre> <p>You can also provide arguments to the decorator to control caching, parallelism, and mlflow tracking:</p> <pre><code>@labtech.task(cache=None, max_parallel=1, mlflow_run=True)\nclass Experiment:\n    ...\n\n    def run(self):\n        ...\n</code></pre> <p>If a <code>post_init(self)</code> method is defined, it will be called after the task object is initialised (analagously to the <code>__post_init__</code> method of a dataclass). Because task types are frozen dataclasses, attributes can only be assigned to the task with <code>object.__setattr__(self, attribute_name, attribute_value)</code>.</p> <p>Parameters:</p> <ul> <li> <code>cache</code>             (<code>Union[CacheDefault, None, Cache]</code>, default:                 <code>CACHE_DEFAULT</code> )         \u2013          <p>The Cache that controls how task results are formatted for caching. Can be set to an instance of any <code>Cache</code> class, or <code>None</code> to disable caching of this type of task. Defaults to a <code>PickleCache</code>.</p> </li> <li> <code>max_parallel</code>             (<code>Optional[int]</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of instances of this task type that are allowed to run simultaneously in separate sub-processes. Useful to set if running too many instances of this particular task simultaneously will exhaust system memory or processing resources. When <code>max_parallel=1</code>, all tasks will be run in the main process, without multi-processing.</p> </li> <li> <code>mlflow_run</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, the execution of each instance of this task type will be wrapped with <code>mlflow.start_run()</code>, tags the run with <code>labtech_task_type</code> equal to the task class name, and all parameters will be logged with <code>mlflow.log_param()</code>. You can make additional mlflow logging calls from the task's <code>run()</code> method.</p> </li> </ul> Source code in <code>labtech/tasks.py</code> <pre><code>def task(*args,\n         cache: Union[CacheDefault, None, Cache] = CACHE_DEFAULT,\n         max_parallel: Optional[int] = None,\n         mlflow_run: bool = False):\n    \"\"\"Class decorator for defining task type classes.\n\n    Task types are frozen [`dataclasses`], and attribute definitions\n    should capture all parameters of the task type. Parameter\n    attributes can be any of the following types:\n\n    * Simple scalar types: `str`, `bool`, `float`, `int`, `None`\n    * Any member of an `Enum` type. Referring to members of an `Enum` can be\n      used to parameterise a task with a value that does not have one of the\n      types above (e.g. a Pandas/Numpy dataset).\n    * Task types: A task parameter is a \"nested task\" that will be executed\n      before its parent so that it may make use of the nested result.\n    * Collections of any of these types: `list`, `tuple`,\n      `dict`, [`frozendict`](https://pypi.org/project/frozendict/)\n      * Dictionaries may only contain string keys.\n      * Note: Mutable `list` and `dict` collections will be converted to\n        immutable `tuple` and [`frozendict`](https://pypi.org/project/frozendict/)\n        collections.\n\n    The task type is expected to define a `run()` method that takes no\n    arguments (other than `self`). The `run()` method should execute\n    the task as parameterised by the task's attributes and return the\n    result of the task.\n\n    Example usage:\n\n    ```python\n    @labtech.task\n    class Experiment:\n        seed: int\n        multiplier: int\n\n        def run(self):\n            return self.seed * self.multiplier.value\n\n    experiment = Experiment(seed=1, multiplier=2)\n    ```\n\n    You can also provide arguments to the decorator to control caching,\n    parallelism, and [mlflow](https://mlflow.org/docs/latest/tracking.html#quickstart)\n    tracking:\n\n    ```python\n    @labtech.task(cache=None, max_parallel=1, mlflow_run=True)\n    class Experiment:\n        ...\n\n        def run(self):\n            ...\n    ```\n\n    If a `post_init(self)` method is defined, it will be called after\n    the task object is initialised (analagously to the `__post_init__`\n    method of a dataclass). Because task types are frozen dataclasses,\n    attributes can only be assigned to the task with\n    `object.__setattr__(self, attribute_name, attribute_value)`.\n\n    Args:\n        cache: The Cache that controls how task results are formatted for\n            caching. Can be set to an instance of any\n            [`Cache`](caching.md#caches) class, or `None` to disable caching\n            of this type of task. Defaults to a\n            [`PickleCache`][labtech.cache.PickleCache].\n        max_parallel: The maximum number of instances of this task type that\n            are allowed to run simultaneously in separate sub-processes. Useful\n            to set if running too many instances of this particular task\n            simultaneously will exhaust system memory or processing resources.\n            When `max_parallel=1`, all tasks will be run in the main process,\n            without multi-processing.\n        mlflow_run: If True, the execution of each instance of this task type\n            will be wrapped with `mlflow.start_run()`, tags the run with\n            `labtech_task_type` equal to the task class name, and all parameters\n            will be logged with `mlflow.log_param()`. You can make additional\n            mlflow logging calls from the task's `run()` method.\n\n    \"\"\"\n\n    def decorator(cls):\n        nonlocal cache\n\n        if not is_task_type(cls):\n            for reserved_attr in _RESERVED_ATTRS:\n                if hasattr(cls, reserved_attr):\n                    raise AttributeError(f\"Task type already defines reserved attribute '{reserved_attr}'.\")\n\n        post_init = getattr(cls, 'post_init', None)\n        cls.__post_init__ = _task_post_init\n\n        cls = dataclass(frozen=True, eq=True, order=True)(cls)\n\n        run_func = getattr(cls, 'run', None)\n        if not callable(run_func):\n            raise NotImplementedError(f\"Task type '{cls.__name__}' must define a 'run' method\")\n\n        if cache is CACHE_DEFAULT:\n            cache = PickleCache()\n        elif cache is None:\n            cache = NullCache()\n\n        cls._lt = TaskInfo(\n            cache=cast(Cache, cache),\n            orig_post_init=post_init,\n            max_parallel=max_parallel,\n            mlflow_run=mlflow_run,\n        )\n        cls.__getstate__ = _task__getstate__\n        cls.__setstate__ = _task__setstate__\n        cls._set_results_map = _task_set_results_map\n        cls._set_result_meta = _task_set_result_meta\n        cls.result = property(_task_result)\n        cls.set_context = _task_set_context\n        return cls\n\n    if len(args) &gt; 0 and isclass(args[0]):\n        return decorator(args[0], *args[1:])\n    else:\n        return decorator\n</code></pre>"},{"location":"core/#labtech.types.Task","title":"<code>labtech.types.Task</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Protocol</code>, <code>Generic[CovariantResultT]</code></p> <p>Interface provided by any class that is decorated by <code>labtech.task</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>@dataclass\nclass Task(Protocol, Generic[CovariantResultT]):\n    \"\"\"Interface provided by any class that is decorated by\n    [`labtech.task`][labtech.task].\"\"\"\n    _lt: TaskInfo\n    _is_task: Literal[True]\n    _results_map: Optional[ResultsMap]\n    cache_key: str\n    \"\"\"The key that uniquely identifies the location for this task within cache storage.\"\"\"\n    context: Optional[dict[str, Any]]\n    \"\"\"Context variables from the Lab that can be accessed when the task is running.\"\"\"\n    result_meta: Optional[ResultMeta]\n    \"\"\"Metadata about the execution of the task.\"\"\"\n\n    def _set_results_map(self, results_map: ResultsMap):\n        pass\n\n    def _set_result_meta(self, result_meta: ResultMeta):\n        pass\n\n    @property\n    def result(self) -&gt; CovariantResultT:\n        \"\"\"Returns the result executed/loaded for this task. If no result is\n        available in memory, accessing this property raises a `TaskError`.\"\"\"\n\n    def set_context(self, context: dict[str, Any]):\n        \"\"\"Set the context that is made available to the task while it is\n        running.\"\"\"\n\n    def run(self):\n        \"\"\"User-provided method that executes the task parameterised by the\n        attributes of the task.\n\n        Usually executed by [`Lab.run_tasks()`][labtech.Lab.run_tasks]\n        instead of being called directly.\n\n        \"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.cache_key","title":"<code>cache_key: str</code>  <code>instance-attribute</code>","text":"<p>The key that uniquely identifies the location for this task within cache storage.</p>"},{"location":"core/#labtech.types.Task.context","title":"<code>context: Optional[dict[str, Any]]</code>  <code>instance-attribute</code>","text":"<p>Context variables from the Lab that can be accessed when the task is running.</p>"},{"location":"core/#labtech.types.Task.result_meta","title":"<code>result_meta: Optional[ResultMeta]</code>  <code>instance-attribute</code>","text":"<p>Metadata about the execution of the task.</p>"},{"location":"core/#labtech.types.Task.result","title":"<code>result: CovariantResultT</code>  <code>property</code>","text":"<p>Returns the result executed/loaded for this task. If no result is available in memory, accessing this property raises a <code>TaskError</code>.</p>"},{"location":"core/#labtech.types.Task.set_context","title":"<code>set_context(context: dict[str, Any])</code>","text":"<p>Set the context that is made available to the task while it is running.</p> Source code in <code>labtech/types.py</code> <pre><code>def set_context(self, context: dict[str, Any]):\n    \"\"\"Set the context that is made available to the task while it is\n    running.\"\"\"\n</code></pre>"},{"location":"core/#labtech.types.Task.run","title":"<code>run()</code>","text":"<p>User-provided method that executes the task parameterised by the attributes of the task.</p> <p>Usually executed by <code>Lab.run_tasks()</code> instead of being called directly.</p> Source code in <code>labtech/types.py</code> <pre><code>def run(self):\n    \"\"\"User-provided method that executes the task parameterised by the\n    attributes of the task.\n\n    Usually executed by [`Lab.run_tasks()`][labtech.Lab.run_tasks]\n    instead of being called directly.\n\n    \"\"\"\n</code></pre>"},{"location":"core/#labtech.types.ResultT","title":"<code>labtech.types.ResultT = TypeVar('ResultT')</code>  <code>module-attribute</code>","text":"<p>Type variable for result returned by the <code>run</code> method of a <code>Task</code>.</p>"},{"location":"core/#labtech.types.ResultMeta","title":"<code>labtech.types.ResultMeta</code>  <code>dataclass</code>","text":"<p>Metadata about the execution of a task. If the task is loaded from cache, the metadata is also loaded from the cache.</p> Source code in <code>labtech/types.py</code> <pre><code>@dataclass(frozen=True)\nclass ResultMeta:\n    \"\"\"Metadata about the execution of a task. If the task is loaded from\n    cache, the metadata is also loaded from the cache.\"\"\"\n    start: Optional[datetime]\n    \"\"\"The timestamp when the task's execution began.\"\"\"\n    duration: Optional[timedelta]\n    \"\"\"The time that the task took to execute.\"\"\"\n</code></pre>"},{"location":"core/#labtech.types.ResultMeta.start","title":"<code>start: Optional[datetime]</code>  <code>instance-attribute</code>","text":"<p>The timestamp when the task's execution began.</p>"},{"location":"core/#labtech.types.ResultMeta.duration","title":"<code>duration: Optional[timedelta]</code>  <code>instance-attribute</code>","text":"<p>The time that the task took to execute.</p>"},{"location":"core/#labtech.is_task_type","title":"<code>labtech.is_task_type(cls)</code>","text":"<p>Returns <code>True</code> if the given <code>cls</code> is a class decorated with <code>labtech.task</code>.</p> Source code in <code>labtech/types.py</code> <pre><code>def is_task_type(cls):\n    \"\"\"Returns `True` if the given `cls` is a class decorated with\n    [`labtech.task`][labtech.task].\"\"\"\n    return isclass(cls) and isinstance(getattr(cls, '_lt', None), TaskInfo)\n</code></pre>"},{"location":"core/#labtech.is_task","title":"<code>labtech.is_task(obj)</code>","text":"<p>Returns <code>True</code> if the given <code>obj</code> is an instance of a task class.</p> Source code in <code>labtech/types.py</code> <pre><code>def is_task(obj):\n    \"\"\"Returns `True` if the given `obj` is an instance of a task class.\"\"\"\n    return is_task_type(type(obj)) and hasattr(obj, '_is_task')\n</code></pre>"},{"location":"core/#labtech.logger","title":"<code>labtech.logger = get_logger()</code>  <code>module-attribute</code>","text":"<p><code>logging.Logger</code> object that labtech logs events to during task execution.</p> <p>Can be used to customize logging and to write additional logs from task <code>run()</code> methods:</p> <pre><code>import logging\nfrom labtech import logger\n\n# Change verbosity of logging\nlogger.setLevel(logging.ERROR)\n\n# Logging methods to call from inside your task's run() method:\nlogger.info('Useful info from task: ...')\nlogger.warning('Warning from task: ...')\nlogger.error('Error from task: ...')\n</code></pre>"},{"location":"diagram/","title":"Diagramming","text":"<p>The <code>labtech.diagram</code> module provides the following functions for automatically generating a diagram to visualise your tasks, their parameters, and their dependency relationships.</p>"},{"location":"diagram/#labtech.diagram.build_task_diagram","title":"<code>labtech.diagram.build_task_diagram(tasks: Sequence[Task], *, direction: str = 'BT') -&gt; str</code>","text":"<p>Returns a Mermaid diagram representing the task types and dependencies of the given tasks.</p> <p>Each task type lists its parameters (with their return types) and its run method (with its return type).</p> <p>Arrows between task types point from a dependency task type to the task type that depends on it, and are labelled with the dependent task's parameter that references the dependency task type.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>             (<code>Sequence[Task]</code>)         \u2013          <p>A collection of tasks to diagram.</p> </li> <li> <code>direction</code>             (<code>str</code>, default:                 <code>'BT'</code> )         \u2013          <p>Direction that task types should be laid out, from dependent tasks to their dependencies. One of:</p> <ul> <li><code>'BT'</code> (bottom-to-top)</li> <li><code>'TB'</code> (top-to-bottom)</li> <li><code>'RL'</code> (right-to-left)</li> <li><code>'LR'</code> (left-to-right)</li> </ul> </li> </ul> Source code in <code>labtech/diagram.py</code> <pre><code>def build_task_diagram(tasks: Sequence[Task], *, direction: str = 'BT') -&gt; str:\n    \"\"\"Returns a [Mermaid diagram](https://mermaid.js.org/syntax/classDiagram.html)\n    representing the task types and dependencies of the given tasks.\n\n    Each task type lists its parameters (with their return types) and\n    its run method (with its return type).\n\n    Arrows between task types point from a dependency task type to the\n    task type that depends on it, and are labelled with the dependent\n    task's parameter that references the dependency task type.\n\n    Args:\n        tasks: A collection of tasks to diagram.\n        direction: Direction that task types should be laid out, from dependent\n            tasks to their dependencies. One of:\n\n            * `'BT'` (bottom-to-top)\n            * `'TB'` (top-to-bottom)\n            * `'RL'` (right-to-left)\n            * `'LR'` (left-to-right)\n\n    \"\"\"\n    return diagram_task_structure(\n        TaskStructure.build(tasks),\n        direction=direction,\n    )\n</code></pre>"},{"location":"diagram/#labtech.diagram.display_task_diagram","title":"<code>labtech.diagram.display_task_diagram(tasks: Sequence[Task], **kwargs) -&gt; None</code>","text":"<p>Displays a Mermaid diagram representing the task types and dependencies of the given tasks.</p> <p>If IPython is available (e.g. the code is being run from a Jupyter notebook), the diagram will be displayed as a Markdown <code>mermaid</code> code block, which will be rendered as a Mermaid diagram from JupyterLab 4.1 and Notebook 7.1.</p> <p>Because Markdown may render arbitrary HTML, you should only diagram tasks that you trust.</p> <p>Accepts the same arguments as build_task_diagram.</p> Source code in <code>labtech/diagram.py</code> <pre><code>def display_task_diagram(tasks: Sequence[Task], **kwargs) -&gt; None:\n    \"\"\"Displays a [Mermaid diagram](https://mermaid.js.org/syntax/classDiagram.html)\n    representing the task types and dependencies of the given tasks.\n\n    If IPython is available (e.g. the code is being run from a Jupyter\n    notebook), the diagram will be displayed as a Markdown `mermaid`\n    code block, which will be rendered as a Mermaid diagram from\n    [JupyterLab 4.1 and Notebook 7.1](https://blog.jupyter.org/jupyterlab-4-1-and-notebook-7-1-are-here-20bfc3c10217).\n\n    Because Markdown may render arbitrary HTML, you should only\n    diagram tasks that you trust.\n\n    Accepts the same arguments as\n    [build_task_diagram][labtech.diagram.build_task_diagram].\n\n    \"\"\"\n    diagram = build_task_diagram(tasks, **kwargs)\n\n    ipython = False\n    try:\n        from IPython.display import display, Markdown\n    except ImportError:\n        pass\n    else:\n        if hasattr(builtins, '__IPYTHON__'):\n            ipython = True\n\n    if ipython:\n        display(Markdown(f'```mermaid\\n{diagram}\\n```'))\n    else:\n        print(diagram)\n</code></pre>"},{"location":"ray-research/","title":"Notes","text":""},{"location":"ray-research/#httpsdocsrayioenlatestray-coretaskshtml","title":"https://docs.ray.io/en/latest/ray-core/tasks.html","text":"<ul> <li>Each remote function can specify resource requirements (number of   CPUs or GPUs, memory, available memory):   https://docs.ray.io/en/latest/ray-core/scheduling/resources.html#resource-requirements</li> <li>A result/object ref passed to a task will be treated like a   dependency - Ray will get that output to the node it needs to be on:   https://docs.ray.io/en/latest/ray-core/tasks.html#passing-object-refs-to-ray-tasks</li> <li>Use ray.wait() to wait for the first completed future:   https://docs.ray.io/en/latest/ray-core/tasks.html#passing-object-refs-to-ray-tasks</li> <li>Tasks can be cancelled:   https://docs.ray.io/en/latest/ray-core/tasks.html#cancelling-tasks</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coretasksnested-taskshtml","title":"https://docs.ray.io/en/latest/ray-core/tasks/nested-tasks.html","text":"<ul> <li>A task can call another task - potentially a nice paradigm than   explicitly creating a DAG? Harder for us to implement for labtech,   as it requires running a task and then \"shelving\" it while its   dependencies run.</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coreobjectshtml","title":"https://docs.ray.io/en/latest/ray-core/objects.html","text":"<ul> <li>Use an object to represent the context? Or parts of the context?</li> <li>Probably use them to store object results to manage sharing results?   Maybe make that optional vs just loading from Storage?</li> <li>\"Remote objects are cached in Ray\u2019s distributed shared-memory object   store, and there is one object store per node in the cluster. In the   cluster setting, a remote object can live on one or many nodes,   independent of who holds the object ref(s).\"</li> <li>\"If the object is a numpy array or a collection of numpy arrays, the   get call is zero-copy and returns arrays backed by shared object   store memory. Otherwise, we deserialize the object data into a   Python object.\"</li> <li>\"If the current node\u2019s object store does not contain the object, the   object is downloaded.\"</li> <li>\"Objects are tracked via distributed reference counting, and their   data is automatically freed once all references to the object are   deleted.\"</li> <li>\"You can also pass objects to tasks via closure-capture.\"</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-corehandling-dependencieshtml","title":"https://docs.ray.io/en/latest/ray-core/handling-dependencies.html","text":"<ul> <li>\"For production usage or non-changing environments, we recommend   installing your dependencies into a container image and specifying   the image using the Cluster Launcher. For dynamic environments (e.g.   for development and experimentation), we recommend using runtime   environments.\"</li> <li>\"You can build all your files and dependencies into a container   image and specify this in your your Cluster YAML Configuration.\" ...   \"You can push local files to the cluster using ray rsync_up\"</li> <li>Runtime Environments:</li> <li>Can be different for each task, or one for the entire \"job\"     (app) - probably just use the latter for simplicity?</li> <li><code>ray.init(runtime_env={...})</code></li> <li>Specifying a <code>working_dir</code> will ensure that local directory is     pushed to the cluster by <code>ray.init()</code>.</li> <li>You can specify pip or conda deps, but probably best to leave     these to the cluster server setup - otherwise you'll be     re-downloading every time.</li> <li>You can specify Python modules that your tasks depend on with     <code>py_modules</code>.<ul> <li>This can be a directory of python files. \"if the local directory   contains a .gitignore file, the files and paths specified there   are not uploaded to the cluster.\"</li> <li>\"Note: This feature is currently limited to modules that are   packages with a single directory containing an init.py file.   For single-file modules, you may use working_dir.\"</li> <li>Also has an <code>excludes</code> option.</li> </ul> </li> <li>\"If runtime_env cannot be set up (e.g., network issues, download     failures, etc.), Ray will fail to schedule tasks/actors that     require the runtime_env. If you call ray.get, it will raise     RuntimeEnvSetupError with the error message in detail.\"</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coreschedulingindexhtml","title":"https://docs.ray.io/en/latest/ray-core/scheduling/index.html","text":"<ul> <li>You can choose a different scheduling strategy.</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coreschedulingacceleratorshtml","title":"https://docs.ray.io/en/latest/ray-core/scheduling/accelerators.html","text":"<ul> <li>Ray can handle GPU allocation</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coreschedulingray-oom-preventionhtml","title":"https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html","text":"<ul> <li>Ray automatically kills tasks to prevent out-of-memory</li> <li>Tasks killed by the memory monitor will be retried infinitely with   exponential backoff up to 60 seconds.</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-corefault-tolerancehtml","title":"https://docs.ray.io/en/latest/ray-core/fault-tolerance.html","text":"<ul> <li>Generally, don't call <code>.put()</code> inside a task it seems.</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-corefault_tolerancetaskshtml","title":"https://docs.ray.io/en/latest/ray-core/fault_tolerance/tasks.html","text":"<ul> <li>TODO</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-corefault_toleranceobjectshtml","title":"https://docs.ray.io/en/latest/ray-core/fault_tolerance/objects.html","text":"<ul> <li>TODO</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-observabilitygetting-startedhtml","title":"https://docs.ray.io/en/latest/ray-observability/getting-started.html","text":"<ul> <li>Ray Dashboard can be run to view running tasks</li> <li>It is automatically run on the head node on a given port.</li> <li>We should make sure tasks are meaningfully named to help our users     view this.</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-observabilityuser-guidescli-sdkhtml","title":"https://docs.ray.io/en/latest/ray-observability/user-guides/cli-sdk.html","text":"<ul> <li>There is also a CLI to get task status</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-observabilityuser-guidesconfigure-logginghtml","title":"https://docs.ray.io/en/latest/ray-observability/user-guides/configure-logging.html","text":"<ul> <li>Log files go into a tmp directory by default</li> <li>\"By default, Worker stdout and stderr for Tasks and Actors stream to   the Ray Driver (the entrypoint script that calls ray.init). It helps   users aggregate the logs for the distributed Ray application in a   single place.\"</li> <li>How will we make this work with our own logging? Plug on top of     this logging and manage it's output ourselves?</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coreray-daghtml","title":"https://docs.ray.io/en/latest/ray-core/ray-dag.html","text":"<ul> <li>There is an API to build a DAG that can be executed, but probably   not helpful for us.</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coremiscellaneoushtmlrunning-large-ray-clusters","title":"https://docs.ray.io/en/latest/ray-core/miscellaneous.html#running-large-ray-clusters","text":"<ul> <li>Tips for running Ray clusters with 1k+ nodes!</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coreuser-spawn-processeshtml","title":"https://docs.ray.io/en/latest/ray-core/user-spawn-processes.html","text":"<ul> <li>Notes for avoiding zombies when your task contains subprocesses and   is killed.</li> <li>\"RAY_kill_child_processes_on_worker_exit (default true): Only works   on Linux. If true, the worker kills all direct child processes on   exit. This won\u2019t work if the worker crashed. This is NOT recursive,   in that grandchild processes are not killed by this mechanism.\"</li> <li>\"RAY_kill_child_processes_on_worker_exit_with_raylet_subreaper   (default false): Only works on Linux greater than or equal to 3.4.   If true, Raylet recursively kills any child processes and grandchild   processes that were spawned by the worker after the worker exits.   This works even if the worker crashed. The killing happens within 10   seconds after the worker death.\"</li> <li>\"On non-Linux platforms, user-spawned process is not controlled by   Ray. The user is responsible for managing the lifetime of the child   processes. If the parent Ray worker process dies, the child   processes will continue to run.\"</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-corepatternsindexhtml","title":"https://docs.ray.io/en/latest/ray-core/patterns/index.html","text":"<ul> <li>TODO</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-coretips-for-first-timehtml","title":"https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html","text":"<ul> <li>TODO</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestray-corestarting-rayhtml","title":"https://docs.ray.io/en/latest/ray-core/starting-ray.html","text":"<ul> <li>TODO</li> </ul>"},{"location":"ray-research/#httpsdocsrayioenlatestclustergetting-startedhtmlcluster-index","title":"https://docs.ray.io/en/latest/cluster/getting-started.html#cluster-index","text":"<ul> <li>TODO</li> </ul>"},{"location":"ray-research/#questions","title":"Questions","text":"<ul> <li>Can I dynamically create a remote function?</li> <li>How will this work with mlflow?</li> <li>Have an option to not use distributed storage, where we load and   save all results locally, and distribute them as shared objects?</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":""},{"location":"tutorial/#labtech-tutorial","title":"Labtech Tutorial","text":"<p>The following tutorial presents a complete example of using labtech to easily add parallelism and caching to machine learning experiments.</p> <p>You can also run this tutorial as an interactive notebook.</p> <p>Before we begin, let's install <code>labtech</code> along with some other dependencies we will use in this tutorial:</p> <pre><code>%pip install labtech mlflow scikit-learn\n</code></pre> <p>Let's also clear any caches that were created by previous runs of this tutorial:</p> <pre><code>!rm -rf storage/tutorial/\n!mkdir -p storage/tutorial/\n</code></pre>"},{"location":"tutorial/#running-a-single-experiment-as-a-labtech-task","title":"Running a single experiment as a labtech task","text":"<p>To get started, we'll take the following simple machine learning experiment code and convert it to be run with labtech.</p> <pre><code>from sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\n\ndigits_X, digits_y = datasets.load_digits(return_X_y=True)\ndigits_X = StandardScaler().fit_transform(digits_X)\n\nclf = RandomForestClassifier(\n    n_estimators=5,\n    random_state=42,\n)\nclf.fit(digits_X, digits_y)\nprob_y = clf.predict_proba(digits_X)\n\nprint(f'{log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>Throughout this tutorial, we will use labtech to improve and extend this experiment, which currently:</p> <ol> <li>Loads and scales the <code>digits</code> dataset.<ul> <li>This is a benchmark dataset where the goal is to train a   classifier that can correctly assign a number between 0 and 9 to   the image of a hand-written digit.</li> <li>We will want to extend our experimentation to also include other   datasets.</li> </ul> </li> <li>Trains a random forest classifier on the digits dataset.<ul> <li>The classifier is configured with <code>n_estimators=5</code> (i.e. a forest   of 5 trees) and a fixed <code>random_state</code> (to ensure we get the same   result every time we run the code).</li> <li>We will want to extend our experimentation to test other   <code>n_estimators</code> values and classifiers other than a random forest.</li> </ul> </li> <li>Evaluates the classifier by calculating the logistic loss of    probabilities predicted by the classifier for the dataset.<ul> <li>Standard evaluation practice would be to calculate loss for a   separate test dataset, but we will use a single dataset for both   training and testing to simplify this tutorial.</li> </ul> </li> </ol> <p>Let's set up this same experiment to be run with labtech, providing us with a foundation that we can extend throughout this tutorial.</p> <p>First, we'll define a labtech task type that will load the dataset, train the classifier, and return the probabilities predicted for the dataset. Defining a task type for our experiment is as simple as defining a class decorated with <code>@labtech.task</code> that defines a <code>run()</code> method that performs the experiment and returns its result (the predicted probabilities):</p> <pre><code>import labtech\n\n@labtech.task\nclass ClassifierExperiment:\n\n    def run(self):\n        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n        digits_X = StandardScaler().fit_transform(digits_X)\n\n        clf = RandomForestClassifier(\n            n_estimators=5,\n            random_state=42,\n        )\n        clf.fit(digits_X, digits_y)\n        prob_y = clf.predict_proba(digits_X)\n        return prob_y\n</code></pre> <p>Next, we create a labtech lab that can be used to execute the experiment. We'll configure the lab to cache results in a folder called <code>storage/tutorial/classification_lab_1</code> and to display notebook-friendly progress bars:</p> <pre><code>lab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_1',\n    notebook=True,\n)\n</code></pre> <p>Finally, we create a task instance of <code>ClassifierExperiment</code> and call <code>lab.run_task()</code> to run it. The output will be the predicted probabilities returned by the task's <code>run()</code> method, so we can calculate the loss from them as before:</p> <pre><code>classifier_experiment = ClassifierExperiment()\nprob_y = lab.run_task(classifier_experiment)\nprint(f'{log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>An immediate benefit of running an experiment this way with labtech is that the result will be cached to disk for future use. Any future calls to run the same experiment (even after restarting Python) will load the result from the cache:</p> <pre><code>prob_y = lab.run_task(classifier_experiment)\nprint(f'{log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>Defining the task to return the prediction probabilities instead of just the loss metric gives us flexibility to change the evaluation in the future (e.g. from <code>log_loss</code> to another metric) while still being able to re-use the same cached result.</p> <p>We can also ask our lab to return <code>task</code> objects for all previously cached results for a given task type by calling <code>lab.cached_tasks()</code>. A given task could then be passed to <code>lab.run_task()</code> to load it's result (or we could pass a list of tasks to <code>lab.run_tasks()</code>, as we will see in the next section of this tutorial).</p> <pre><code>lab.cached_tasks([\n    ClassifierExperiment,\n])\n</code></pre> <p>It is very important that you clear any cached results whenever you make a change that will impact the behaviour of a task - otherwise your cached results may no longer reflect the actual result of the current code.</p> <p>You can clear the cached results for a list of tasks with <code>lab.uncache_tasks()</code>:</p> <pre><code>lab.uncache_tasks([\n    classifier_experiment,\n])\n</code></pre>"},{"location":"tutorial/#parameterising-tasks-and-running-many-tasks-in-parallel","title":"Parameterising tasks, and running many tasks in parallel","text":"<p>Let's extend our experimentation to compare the results for classifiers configured with different <code>n_estimators</code> values.</p> <p>To do so, we'll add an <code>n_estimators</code> parameter to our <code>ClassifierExperiment</code> task type and reference it within the <code>run()</code> method as <code>self.n_estimators</code>. Task parameters are declared in exactly the same way as dataclass fields:</p> <pre><code>import labtech\n\n@labtech.task\nclass ClassifierExperiment:\n    n_estimators: int\n\n    def run(self):\n        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n        digits_X = StandardScaler().fit_transform(digits_X)\n\n        clf = RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            random_state=42,\n        )\n        clf.fit(digits_X, digits_y)\n        prob_y = clf.predict_proba(digits_X)\n        return prob_y\n</code></pre> <p>Now we'll use a list comprehension to construct a list of <code>ClassifierExperiment</code> tasks with different <code>n_estimators</code> values:</p> <pre><code>classifier_experiments = [\n    ClassifierExperiment(\n        n_estimators=n_estimators,\n    )\n    for n_estimators in range(1, 11)\n]\n</code></pre> <p>We can run a list of tasks with <code>lab.run_tasks()</code>, which has the added benefit of leveraging Python's multiprocessing capabilities to run the tasks in parallel - running as many tasks simultaneously as possible with the CPU of the machine running the tasks. Also, because we've changed the definition of our <code>ClassifierExperiment</code> class, we'll keep caches for the new definition separate by constructing a new lab that uses a different storage directory:</p> <pre><code>lab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_2',\n    notebook=True,\n)\nresults = lab.run_tasks(classifier_experiments)\n</code></pre> <p><code>lab.run_tasks()</code> returns a dictionary mapping each input task to the result it returned, which we can loop over to print loss metrics for each experiment:</p> <pre><code>for experiment, prob_y in results.items():\n    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')\n</code></pre>"},{"location":"tutorial/#maximising-concurrency-and-caching-with-dependent-tasks","title":"Maximising concurrency and caching with dependent tasks","text":"<p>Labtech's true power lies in its ability to manage complex networks of dependent tasks - automatically running as many tasks as possible in parallel (even different types of tasks) and re-using cached results wherever possible.</p> <p>To demonstrate this, let's extend our experimentation with a new post-processing step that will take the probabilities returned by one of our previous <code>ClassifierExperiment</code> tasks and assign a probability of <code>1</code> to the most likely class for each record (and conversely assign a probability of <code>0</code> to all other classes).</p> <p>To achieve this, we will define a new <code>MinMaxProbabilityExperiment</code> task type that accepts a <code>ClassifierExperiment</code> as a parameter. Labtech will consider any task in a parameter to be a dependency of the task. Dependency tasks will be run before any of their dependent tasks, allowing us to access the result from the <code>.result</code> attribute of the task parameter (i.e. <code>self.classifier_experiment.result</code>):</p> <pre><code>import numpy as np\n\n\n@labtech.task\nclass MinMaxProbabilityExperiment:\n    classifier_experiment: ClassifierExperiment\n\n    def run(self):\n        prob_y = self.classifier_experiment.result\n        # Replace the maximum probability in each row with 1,\n        # and replace all other probabilities with 0.\n        min_max_prob_y = np.zeros(prob_y.shape)\n        min_max_prob_y[np.arange(len(prob_y)), prob_y.argmax(axis=1)] = 1\n        return min_max_prob_y\n</code></pre> <p>We can then construct and run a list of <code>MinMaxProbabilityExperiment</code> tasks that depend on our previous <code>ClassifierExperiment</code> tasks in <code>classifier_experiments</code>. Labtech will ensure each of the <code>classifier_experiments</code> has been run before it's dependent <code>MinMaxProbabilityExperiment</code> is run, re-using results depended on by multiple tasks and loading previously cached results wherever possible:</p> <pre><code>min_max_prob_experiments = [\n    MinMaxProbabilityExperiment(\n        classifier_experiment=classifier_experiment,\n    )\n    for classifier_experiment in classifier_experiments\n]\n\nresults = lab.run_tasks(min_max_prob_experiments)\nfor experiment, prob_y in results.items():\n    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')\n</code></pre> <p>By simply specifying task dependencies, you can construct any task structure that can be expressed as a directed acyclic graph (or DAG) and let labtech handle running tasks concurrently, sharing results between dependent tasks, and using caches wherever possible.</p>"},{"location":"tutorial/#parameterising-tasks-with-complex-objects","title":"Parameterising tasks with complex objects","text":"<p>Now let's extend our experimentation to compare different classifier models. We'd like to make the classifier itself a parameter to the task, but task parameters can only be json-serializable values or dependency tasks. Therefore, we will use dependency tasks to construct and return classifier objects to our experiment tasks. We achieve this in the following code by:</p> <ol> <li>Defining <code>RFClassifierTask</code> and <code>LRClassifierTask</code> task types.<ul> <li><code>RFClassifierTask</code> returns a random forest classifier   parameterised by an <code>n_estimators</code> value.</li> <li><code>LRClassifierTask</code> returns a logistic regression classifier.</li> <li>Because constructing a classifier object is inexpensive, we don't   need to cache them, so we set <code>cache=None</code> in the <code>@labtech.task</code>   decorator for these task types.</li> <li>For type hinting purposes, we will identify these task types   with the <code>ClassifierTask</code> Protocol,   which will match any task type that returns an sklearn   classifier.</li> </ul> </li> <li>Redefining <code>ClassifierExperiment</code> to be parameterised by a    <code>ClassifierTask</code>.<ul> <li>The classifier object to be trained and applied is retrieved   from the <code>ClassifierTask</code> result with   <code>self.classifier_task.result</code>.</li> <li>Because one <code>ClassifierTask</code> result may be shared by many   <code>ClassifierExperiment</code> tasks, the <code>run()</code> method first creates   its own copy of the classifier with <code>clone()</code>.</li> </ul> </li> </ol> <pre><code>from typing import Protocol\n\nfrom sklearn.base import clone, ClassifierMixin\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass ClassifierTask(Protocol):\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(cache=None)\nclass RFClassifierTask:\n    n_estimators: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            random_state=42,\n        )\n\n\n@labtech.task(cache=None)\nclass LRClassifierTask:\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=42,\n        )\n\n\n@labtech.task\nclass ClassifierExperiment:\n    classifier_task: ClassifierTask\n\n    def run(self):\n        digits_X, digits_y = datasets.load_digits(return_X_y=True)\n        digits_X = StandardScaler().fit_transform(digits_X)\n\n        clf = clone(self.classifier_task.result)\n        clf.fit(digits_X, digits_y)\n        prob_y = clf.predict_proba(digits_X)\n        return prob_y\n</code></pre> <p>Now we can generate and run a set of <code>RFClassifierTask</code> tasks for various <code>n_estimators</code> values, and construct a <code>ClassifierExperiment</code> for each of these <code>RFClassifierTask</code> tasks as well as an <code>LRClassifierTask</code> task:</p> <pre><code>rf_classifier_tasks = [\n    RFClassifierTask(\n        n_estimators=n_estimators,\n    )\n    for n_estimators in range(1, 11)\n]\nclassifier_experiments = [\n    ClassifierExperiment(\n        classifier_task=classifier_task,\n    )\n    for classifier_task in [\n        LRClassifierTask(),\n        *rf_classifier_tasks,\n    ]\n]\n\nlab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_3',\n    notebook=True,\n)\n\nresults = lab.run_tasks(classifier_experiments)\nfor experiment, prob_y in results.items():\n    print(f'{experiment}: {log_loss(digits_y, prob_y) = :.3}')\n</code></pre>"},{"location":"tutorial/#providing-large-objects-as-context","title":"Providing large objects as context","text":"<p>Sometimes we want to pass large, unchanging objects to our tasks, but don't want to be forced to load them in a dependent task. For example, it would be convenient to load a collection of datasets (on which to run our experiments) outside of any task, allowing us to inspect these datasets before and after the tasks have been run:</p> <pre><code>iris_X, iris_y = datasets.load_iris(return_X_y=True)\niris_X = StandardScaler().fit_transform(iris_X)\n\nDATASETS = {\n    'digits': {'X': digits_X, 'y': digits_y},\n    'iris': {'X': iris_X, 'y': iris_y},\n}\n</code></pre> <p>To achieve this, a labtech lab can be provided with a context that is made available to all tasks. In the following code, we:</p> <ol> <li>Pass a <code>context</code> to the <code>labtech.Lab()</code> constructor, with a    <code>'DATASETS'</code> key for the set of <code>DATASETS</code> defined above.</li> <li>Redefine <code>ClassifierExperiment</code> to accept a <code>dataset_key</code> parameter    and use it to look up a dataset inside the <code>'DATASETS'</code> key of the    context, which is made available by labtech as <code>self.context</code>.</li> <li>Alter the task generation and evaluation code to handle multiple    datasets.</li> </ol> <pre><code>@labtech.task\nclass ClassifierExperiment:\n    classifier_task: ClassifierTask\n    dataset_key: str\n\n    def run(self):\n        dataset = self.context['DATASETS'][self.dataset_key]\n        X, y = dataset['X'], dataset['y']\n\n        clf = clone(self.classifier_task.result)\n        clf.fit(X, y)\n        prob_y = clf.predict_proba(X)\n        return prob_y\n\n\nclassifier_experiments = [\n    ClassifierExperiment(\n        classifier_task=classifier_task,\n        dataset_key=dataset_key,\n    )\n    # By including multiple for clauses, we will produce a ClassifierExperiment\n    # for every combination of dataset_key and classifier_task\n    for dataset_key in DATASETS.keys()\n    for classifier_task in [LRClassifierTask(), *rf_classifier_tasks]\n]\n\nlab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_4',\n    notebook=True,\n    context={\n        'DATASETS': DATASETS,\n    },\n)\n\nresults = lab.run_tasks(classifier_experiments)\nfor experiment, prob_y in results.items():\n    dataset_y = DATASETS[experiment.dataset_key][\"y\"]\n    print(f'{experiment}: {log_loss(dataset_y, prob_y) = :.3}')\n</code></pre> <p>The lab context can also be useful for passing parameters to a task that won't affect its result and therefore don't need to be part of the task's formal parameters. For example: log levels and task-internal parallelism settings.</p> <p>It is important that you do NOT make changes to context values that impact task results after you have started caching experiment results - otherwise your cached results may not reflect your latest context values.</p>"},{"location":"tutorial/#bringing-it-all-together-and-aggregating-results","title":"Bringing it all together and aggregating results","text":"<p>The following code brings all the steps from this tutorial together in one place, with some additional improvements:</p> <ul> <li>The \"experiment-like\" <code>ClassifierExperiment</code> and   <code>MinMaxProbabilityExperiment</code> task types are now identified by a   common <code>ExperimentTask</code> Protocol (which requires each of those   classes to provide a <code>dataset_key</code> attribute or property that is   used by the new <code>ExperimentEvaluationTask</code>).</li> <li>A new, final, <code>ExperimentEvaluationTask</code> task that depends on all   <code>ExperimentTask</code> tasks is used to compute the loss metric for all   experiments.<ul> <li>A final task like this is useful once we have a large number of   experiments as it allows us to cache the final evaluation of all   tasks, meaning that we only need to load experiment results and   re-calculate metrics when experiment parameters have changed or   new experiments have been added.</li> </ul> </li> <li>We enable labtech's integration with <code>mlflow</code>   by the following additions (see How can I use labtech with mlfow?   for details):<ol> <li>We add <code>mlflow_run=True</code> to the <code>@labtech.task</code> decorator of    <code>ClassifierExperiment</code> and <code>MinMaxProbabilityExperiment</code>,    indicating that each task of these types should be recorded as    a \"run\" in mflow.</li> <li>We name the over-arching mlflow \"experiment\" with    <code>mlflow.set_experiment('example_labtech_experiment')</code> before    the tasks are run.</li> </ol> </li> </ul> <pre><code>from typing import Protocol\n\nimport labtech\nfrom sklearn.base import clone, ClassifierMixin\n\n\n# === Prepare Datasets ===\n\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndigits_X, digits_y = datasets.load_digits(return_X_y=True)\ndigits_X = StandardScaler().fit_transform(digits_X)\n\niris_X, iris_y = datasets.load_iris(return_X_y=True)\niris_X = StandardScaler().fit_transform(iris_X)\n\nDATASETS = {\n    'digits': {'X': digits_X, 'y': digits_y},\n    'iris': {'X': iris_X, 'y': iris_y},\n}\n\n\n# === Classifier Tasks ===\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass ClassifierTask(Protocol):\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(cache=None)\nclass RFClassifierTask:\n    n_estimators: int\n\n    def run(self) -&gt; ClassifierMixin:\n        return RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            random_state=42,\n        )\n\n\n@labtech.task(cache=None)\nclass LRClassifierTask:\n\n    def run(self) -&gt; ClassifierMixin:\n        return LogisticRegression(\n            random_state=42,\n        )\n\n\n# === Experiment Tasks ===\n\nclass ExperimentTask(Protocol):\n    dataset_key: str\n\n    def run(self) -&gt; ClassifierMixin:\n        pass\n\n\n@labtech.task(mlflow_run=True)\nclass ClassifierExperiment(ExperimentTask):\n    classifier_task: ClassifierTask\n    dataset_key: str\n\n    def run(self) -&gt; np.ndarray:\n        dataset = self.context['DATASETS'][self.dataset_key]\n        X, y = dataset['X'], dataset['y']\n\n        clf = clone(self.classifier_task.result)\n        clf.fit(X, y)\n        prob_y = clf.predict_proba(X)\n        return prob_y\n\n\n@labtech.task(mlflow_run=True)\nclass MinMaxProbabilityExperiment(ExperimentTask):\n    experiment: ExperimentTask\n\n    @property\n    def dataset_key(self):\n        return self.experiment.dataset_key\n\n    def run(self) -&gt; np.ndarray:\n        prob_y = self.experiment.result\n        # Replace the maximum probability in each row with 1,\n        # and replace all other probabilities with 0.\n        min_max_prob_y = np.zeros(prob_y.shape)\n        min_max_prob_y[np.arange(len(prob_y)), prob_y.argmax(axis=1)] = 1\n        return min_max_prob_y\n\n\n# === Results Aggregation ===\n\nfrom sklearn.metrics import log_loss\n\n\n@labtech.task\nclass ExperimentEvaluationTask:\n    experiments: list[ExperimentTask]\n\n    def run(self):\n        return {\n            experiment: {'log_loss': log_loss(\n                self.context['DATASETS'][experiment.dataset_key]['y'],\n                experiment.result,\n            )}\n            for experiment in self.experiments\n        }\n\n\n# === Task Construction ===\n\nrf_classifier_tasks = [\n    RFClassifierTask(\n        n_estimators=n_estimators,\n    )\n    for n_estimators in range(1, 11)\n]\n\nclassifier_experiments = [\n    ClassifierExperiment(\n        classifier_task=classifier_task,\n        dataset_key=dataset_key,\n    )\n    for dataset_key in DATASETS.keys()\n    for classifier_task in [LRClassifierTask(), *rf_classifier_tasks]\n]\n\nmin_max_prob_experiments = [\n    MinMaxProbabilityExperiment(\n        experiment=classifier_experiment,\n    )\n    for classifier_experiment in classifier_experiments\n]\n\nevaluation_task = ExperimentEvaluationTask(\n    experiments=[\n        *classifier_experiments,\n        *min_max_prob_experiments,\n    ]\n)\n\n\n# === Task Execution ===\n\nimport mlflow\n\nmlflow.set_experiment('example_labtech_experiment')\nlab = labtech.Lab(\n    storage='storage/tutorial/classification_lab_final',\n    notebook=True,\n    context={\n        'DATASETS': DATASETS,\n    },\n)\n\nevaluation_result = lab.run_task(evaluation_task)\nfor experiment, result in evaluation_result.items():\n    print(f'{experiment}: log_loss = {result[\"log_loss\"]:.3}')\n</code></pre>"},{"location":"tutorial/#visualising-tasks-and-dependencies","title":"Visualising tasks and dependencies","text":"<p>Finally, we can use Labtech to generate a diagram of a list of tasks that shows all of the task types, parameters, and dependencies:</p> <pre><code>from labtech.diagram import display_task_diagram\n\nlabtech.diagram.display_task_diagram([\n    evaluation_task,\n], direction='BT')\n</code></pre> <pre><code>classDiagram\n    direction BT\n\n    class ExperimentEvaluationTask\n    ExperimentEvaluationTask : list[ExperimentTask] experiments\n    ExperimentEvaluationTask : run()\n\n    class ClassifierExperiment\n    ClassifierExperiment : ClassifierTask classifier_task\n    ClassifierExperiment : str dataset_key\n    ClassifierExperiment : run() ndarray\n\n    class MinMaxProbabilityExperiment\n    MinMaxProbabilityExperiment : ExperimentTask experiment\n    MinMaxProbabilityExperiment : run() ndarray\n\n    class LRClassifierTask\n    LRClassifierTask : run() ClassifierMixin\n\n    class RFClassifierTask\n    RFClassifierTask : int n_estimators\n    RFClassifierTask : run() ClassifierMixin\n\n\n    ExperimentEvaluationTask &lt;-- \"many\" ClassifierExperiment: experiments\n    ExperimentEvaluationTask &lt;-- \"many\" MinMaxProbabilityExperiment: experiments\n\n    ClassifierExperiment &lt;-- LRClassifierTask: classifier_task\n    ClassifierExperiment &lt;-- RFClassifierTask: classifier_task\n\n    MinMaxProbabilityExperiment &lt;-- ClassifierExperiment: experiment</code></pre> <p>Such diagrams can help you visualise how your experiments are running, and may be useful to include in project documentation.</p>"},{"location":"tutorial/#next-steps","title":"Next steps","text":"<p>Congratulations on completing the labtech tutorial! You're now ready to manage complex experiment workflows with ease!</p> <p>To learn more about labtech, you can dive into the following resources:</p> <ul> <li>Cookbook of common patterns (as an interactive notebook)</li> <li>API reference for Labs and Tasks</li> <li>More options for cache formats and storage providers</li> <li>Diagramming tools</li> <li>More examples</li> </ul>"}]}